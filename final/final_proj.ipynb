{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 117287,
          "databundleVersionId": 14018857,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "nathaniel14133437_midterm_CS5173",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grillinr/evolutionary-computing/blob/main/final/final_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and seed for easier checking"
      ],
      "metadata": {
        "id": "uibqKYOH1C2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import argparse\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import accuracy_score, fbeta_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "SEED = 5173\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n",
        "print(device)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:17.559153Z",
          "iopub.execute_input": "2025-10-17T00:35:17.559461Z",
          "iopub.status.idle": "2025-10-17T00:35:17.568112Z",
          "shell.execute_reply.started": "2025-10-17T00:35:17.559437Z",
          "shell.execute_reply": "2025-10-17T00:35:17.567065Z"
        },
        "id": "rU8AhTMY1C2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d70f9b9d-7849-45ef-d8f0-744ae4218162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define helper functions"
      ],
      "metadata": {
        "id": "i8xd5F721C2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_with_scaler(data, scaler=None, fit=False):\n",
        "    data = data.dropna()\n",
        "    X = data.drop(columns=[\"id\", \"record\", \"type\"]).values.astype(np.float32)\n",
        "    y = data[\"type\"].astype(\"category\").cat.codes.values\n",
        "\n",
        "    if fit:\n",
        "        X = scaler.fit_transform(X)\n",
        "    else:\n",
        "        X = scaler.transform(X)\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "def evaluate(model, X, y, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        y_pred = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    y_true = y.cpu().numpy()\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"f_beta_macro\": fbeta_score(y_true, y_pred, average=\"macro\", beta=2, zero_division=0)\n",
        "    }\n",
        "\n",
        "\n",
        "def estimate_flops(model, input_shape):\n",
        "    \"\"\"\n",
        "    Estimate FLOPs for Linear and Conv2d layers only.\n",
        "    Args:\n",
        "        model (nn.Module): PyTorch model\n",
        "        input_shape (tuple): shape of one input sample, e.g., (1, 3, 224, 224) or (1, input_dim)\n",
        "    Returns:\n",
        "        total_flops (int)\n",
        "    \"\"\"\n",
        "    flops = 0\n",
        "\n",
        "    def count_layer(layer, x_in, x_out):\n",
        "        nonlocal flops\n",
        "        # Conv2d FLOPs = Kx * Ky * Cin * Cout * Hout * Wout\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            out_h, out_w = x_out.shape[2:]\n",
        "            kernel_ops = layer.kernel_size[0] * layer.kernel_size[1]\n",
        "            flops += kernel_ops * layer.in_channels * layer.out_channels * out_h * out_w\n",
        "        # Linear FLOPs = input_features * output_features\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            flops += layer.in_features * layer.out_features\n",
        "\n",
        "    hooks = []\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(layer.register_forward_hook(count_layer))\n",
        "\n",
        "    dummy = torch.randn(input_shape).to(next(model.parameters()).device)\n",
        "    with torch.no_grad():\n",
        "        model(dummy)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return flops"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:46:03.730208Z",
          "iopub.execute_input": "2025-10-17T00:46:03.730536Z",
          "iopub.status.idle": "2025-10-17T00:46:03.738421Z",
          "shell.execute_reply.started": "2025-10-17T00:46:03.730511Z",
          "shell.execute_reply": "2025-10-17T00:46:03.73737Z"
        },
        "id": "ZxxTIRWL1C2O"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model Architecture (DNN)"
      ],
      "metadata": {
        "id": "dTi3Q6241C2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size=32, hidden=(32, 16, 8), num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        input_dim = input_size\n",
        "\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(input_dim, h))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            input_dim = h\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:38.736024Z",
          "iopub.execute_input": "2025-10-17T00:35:38.736378Z",
          "iopub.status.idle": "2025-10-17T00:35:38.74276Z",
          "shell.execute_reply.started": "2025-10-17T00:35:38.736352Z",
          "shell.execute_reply": "2025-10-17T00:35:38.741836Z"
        },
        "id": "qzOM0laS1C2P"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training loop"
      ],
      "metadata": {
        "id": "8V1SVoBd1C2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "dataset = pd.read_csv(\"/content/train.csv\")\n",
        "train_dataset, val_dataset = train_test_split(dataset, train_size=0.7, random_state=SEED)\n",
        "scaler = StandardScaler()\n",
        "X_train, y_train = prepare_data_with_scaler(train_dataset, scaler, fit=True)\n",
        "X_val, y_val = prepare_data_with_scaler(val_dataset, scaler, fit=False)\n",
        "\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_val, y_val = X_val.to(device), y_val.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:46:12.869203Z",
          "iopub.execute_input": "2025-10-17T00:46:12.869487Z",
          "iopub.status.idle": "2025-10-17T00:46:18.983013Z",
          "shell.execute_reply.started": "2025-10-17T00:46:12.869467Z",
          "shell.execute_reply": "2025-10-17T00:46:18.982193Z"
        },
        "id": "Dv6o3n1t1C2Q"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "class Hyperparameters:\n",
        "    def __init__(self, lr, epochs, hidden, dropout_rate, patience):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.hidden = hidden\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.patience = patience\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Hyperparameters(lr={self.lr}, epochs={self.epochs}, hidden={self.hidden}, dropout_rate={self.dropout_rate}, patience={self.patience})\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:56.034543Z",
          "iopub.execute_input": "2025-10-17T00:35:56.034924Z",
          "iopub.status.idle": "2025-10-17T00:35:56.042659Z",
          "shell.execute_reply.started": "2025-10-17T00:35:56.034863Z",
          "shell.execute_reply": "2025-10-17T00:35:56.041417Z"
        },
        "id": "R3uFXDIH1C2Q"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params: Hyperparameters):\n",
        "  # Create model\n",
        "  model = DNN(hidden=params.hidden, dropout_rate=params.dropout_rate).to(device)\n",
        "\n",
        "  class_counts = train_dataset['type'].value_counts()\n",
        "  weights = 1.0 / class_counts.values\n",
        "  weights = torch.FloatTensor(weights).to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
        "\n",
        "  # Training loop with early stopping\n",
        "  best_val_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "  epochs_run = params.epochs\n",
        "  for epoch in range(1, params.epochs + 1):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(X_train)\n",
        "      loss = criterion(out, y_train)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss = loss.item()\n",
        "\n",
        "      train_metrics = evaluate(model, X_train, y_train, criterion)\n",
        "      val_metrics = evaluate(model, X_val, y_val, criterion)\n",
        "\n",
        "      if epoch % (params.epochs // 10) == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{params.epochs} | \"\n",
        "            f\"train_loss={train_loss:.4f} train_acc={train_metrics['accuracy']:.4f} \"\n",
        "            f\"train_f1={train_metrics['f_beta_macro']:.4f} \"\n",
        "            f\"val_loss={val_metrics['loss']:.4f} val_acc={val_metrics['accuracy']:.4f} \"\n",
        "            f\"val_f1={val_metrics['f_beta_macro']:.4f} \"\n",
        "        )\n",
        "\n",
        "      # Early stopping check\n",
        "      if val_metrics['loss'] < best_val_loss:\n",
        "          best_val_loss = val_metrics['loss']\n",
        "          patience_counter = 0\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "          if patience_counter >= params.patience:\n",
        "              print(f\"Early stopping at epoch {epoch}\")\n",
        "              epochs_run = epoch\n",
        "              break\n",
        "\n",
        "  # Return a tuple of fitness values for domination comparison\n",
        "  # Maximize accuracy, f1, and minimize loss, flops.\n",
        "  # For loss and flops, we take the negative value.\n",
        "  val_metrics[\"flops\"] = -estimate_flops(model, (1, 32))\n",
        "\n",
        "  return (val_metrics[\"accuracy\"], val_metrics[\"f_beta_macro\"], -val_metrics[\"loss\"], -estimate_flops(model, (1, 32)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:43:03.337535Z",
          "iopub.execute_input": "2025-10-17T00:43:03.337852Z",
          "iopub.status.idle": "2025-10-17T00:43:03.676338Z",
          "shell.execute_reply.started": "2025-10-17T00:43:03.337828Z",
          "shell.execute_reply": "2025-10-17T00:43:03.675024Z"
        },
        "id": "pEO7m2r11C2R"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "hyperparameters = Hyperparameters(lr=1e-3, epochs=500, hidden=(64, 32, 16, 8), dropout_rate=0.5, patience=100)\n",
        "# result = train(hyperparameters)"
      ],
      "metadata": {
        "id": "fUhJqaHLMxKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuroevolution"
      ],
      "metadata": {
        "id": "3_uGubzQTmqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_population(pop_size: int) -> List[Hyperparameters]:\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        lr = random.uniform(1e-5, 1e-1)\n",
        "        epochs = random.randint(10, 200)\n",
        "\n",
        "        # Generate variable-length hidden layer tuple\n",
        "        num_layers = random.randint(1, 5)\n",
        "        hidden = tuple(2 ** random.randint(3, 8) for _ in range(num_layers))\n",
        "\n",
        "        dropout_rate = random.uniform(0.0, 0.5)\n",
        "        patience = random.randint(5, 30)\n",
        "\n",
        "        population.append(Hyperparameters(lr, epochs, hidden, dropout_rate, patience))\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "J3EDsF-zX0b1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dominated(fitnesses: List[Tuple[float, ...]], idx: int) -> int:\n",
        "    \"\"\"Count how many points are dominated by fitnesses[idx]\"\"\"\n",
        "    point = fitnesses[idx]\n",
        "    dominated = 0\n",
        "    for other in fitnesses:\n",
        "        if other == point:\n",
        "            continue\n",
        "        # Check if point dominates other (all >= and at least one >)\n",
        "        if all(p >= o for p, o in zip(point, other)) and any(p > o for p, o in zip(point, other)):\n",
        "            dominated += 1\n",
        "    return dominated"
      ],
      "metadata": {
        "id": "JFWFExuUdf6H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Evolution Strategy to Optimize Hyperparameters"
      ],
      "metadata": {
        "id": "b80BDEmPX5Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evolution_strategy(mu: int, lambda_: int, tau: float, max_gens: int) -> List[Hyperparameters]:\n",
        "    population = init_population(mu)\n",
        "\n",
        "    for generation_number in range(1, max_gens + 1):\n",
        "        print(f\"Generation {generation_number} starting initial evaluation\")\n",
        "        fitnesses = []\n",
        "        for i, member in enumerate(population):\n",
        "            print(f\"Evaluating member {i}: {member}\")\n",
        "            fitnesses.append(train(member))\n",
        "\n",
        "        # fitnesses = [train(member) for member in population]\n",
        "\n",
        "        # Calculate proportion of dominated points for each individual\n",
        "        domination_counts = [count_dominated(fitnesses, i) for i in range(mu)]\n",
        "        domination_proportions = [count / mu for count in domination_counts]\n",
        "\n",
        "        offspring = []\n",
        "        for _ in range(lambda_):\n",
        "            # Select parent using tournament based on domination proportion\n",
        "            candidates = random.sample(range(mu), 2)\n",
        "            parent_idx = max(candidates, key=lambda i: domination_proportions[i])\n",
        "            parent = population[parent_idx]\n",
        "\n",
        "            # Mutate hyperparameters\n",
        "            lr = parent.lr * math.exp(tau * random.gauss(0.0, 1.0))\n",
        "            lr = max(1e-5, min(1e-1, lr))\n",
        "\n",
        "            epochs = int(parent.epochs + random.gauss(0.0, 10))\n",
        "            epochs = max(10, min(500, epochs))\n",
        "\n",
        "            # Mutate hidden layers\n",
        "            hidden = list(parent.hidden)\n",
        "            if random.random() < 0.3:\n",
        "                if len(hidden) > 1 and random.random() < 0.5:\n",
        "                    hidden.pop(random.randrange(len(hidden)))\n",
        "                elif len(hidden) < 5:\n",
        "                    hidden.insert(random.randrange(len(hidden) + 1), 2 ** random.randint(3, 8))\n",
        "            else:\n",
        "                idx = random.randrange(len(hidden))\n",
        "                hidden[idx] = max(8, min(256, int(hidden[idx] + random.gauss(0.0, 16))))\n",
        "\n",
        "            dropout_rate = parent.dropout_rate + random.gauss(0.0, 0.05)\n",
        "            dropout_rate = max(0.0, min(0.5, dropout_rate))\n",
        "\n",
        "            patience = int(parent.patience + random.gauss(0.0, 3))\n",
        "            patience = max(5, min(30, patience))\n",
        "\n",
        "            offspring.append(Hyperparameters(lr, epochs, tuple(hidden), dropout_rate, patience))\n",
        "\n",
        "        offspring_fitnesses = [train(member) for member in offspring]\n",
        "\n",
        "        # Calculate domination for offspring\n",
        "        offspring_domination_counts = [count_dominated(offspring_fitnesses, i) for i in range(lambda_)]\n",
        "        offspring_domination_proportions = [count / lambda_ for count in offspring_domination_counts]\n",
        "\n",
        "        # Logging\n",
        "        best_idx = max(range(mu), key=lambda i: domination_proportions[i])\n",
        "        print(f\"Gen {generation_number} Best: {fitnesses[best_idx]}\")\n",
        "\n",
        "        # Select best member from offspring based on domination\n",
        "        indexed = [(prop, i) for i, prop in enumerate(offspring_domination_proportions)]\n",
        "        indexed.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        population = [offspring[i] for _, i in indexed[:mu]]\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "-_jNJVH6TmSq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pop = [Hyperparameters(lr=0.075963869926427, epochs=110, hidden=(27, 14), dropout_rate=0.03570794398355265, patience=19),\n",
        "Hyperparameters(lr=0.058618450698914214, epochs=155, hidden=(49,), dropout_rate=0.24084559091639896, patience=22),\n",
        "Hyperparameters(lr=0.03591685145376863, epochs=154, hidden=(64,), dropout_rate=0.08112417680099193, patience=18),\n",
        "Hyperparameters(lr=0.05702871043570519, epochs=150, hidden=(44,), dropout_rate=0.20232812001415448, patience=14),\n",
        "Hyperparameters(lr=0.07068164533410258, epochs=128, hidden=(34, 22), dropout_rate=0.08438533268973301, patience=23),\n",
        "Hyperparameters(lr=0.046316062463119444, epochs=167, hidden=(59,), dropout_rate=0.284827195291693, patience=5),\n",
        "Hyperparameters(lr=0.07101403078911156, epochs=131, hidden=(93,), dropout_rate=0.12339982161939395, patience=28),\n",
        "Hyperparameters(lr=0.06895945931608811, epochs=111, hidden=(42,), dropout_rate=0.07567568953126667, patience=30),\n",
        "Hyperparameters(lr=0.05761319230521403, epochs=153, hidden=(58,), dropout_rate=0.2497238481170354, patience=25),\n",
        "Hyperparameters(lr=0.057957123134657396, epochs=170, hidden=(38,), dropout_rate=0.1719235760065474, patience=16),\n",
        "Hyperparameters(lr=0.083971988253907, epochs=113, hidden=(53,), dropout_rate=0.17523486601145874, patience=15),\n",
        "Hyperparameters(lr=0.059138679673085566, epochs=179, hidden=(16, 48), dropout_rate=0.1772226476422656, patience=24),\n",
        "Hyperparameters(lr=0.04955507050587288, epochs=163, hidden=(63,), dropout_rate=0.22816655253259505, patience=10),\n",
        "Hyperparameters(lr=0.0788010978205179, epochs=84, hidden=(66,), dropout_rate=0.10675248189853592, patience=6),\n",
        "Hyperparameters(lr=0.0674894565155647, epochs=153, hidden=(67,), dropout_rate=0.3972397614152903, patience=26),\n",
        "Hyperparameters(lr=0.07743634327329373, epochs=79, hidden=(32,), dropout_rate=0.08661383231865555, patience=11),\n",
        "Hyperparameters(lr=0.07491964419750978, epochs=86, hidden=(19, 51), dropout_rate=0.06580738738454688, patience=28),\n",
        "Hyperparameters(lr=0.05991332160473051, epochs=151, hidden=(86,), dropout_rate=0.5, patience=22),\n",
        "Hyperparameters(lr=0.08158206009841892, epochs=121, hidden=(34, 55), dropout_rate=0.14618937233485874, patience=23),\n",
        "Hyperparameters(lr=0.05985280898044459, epochs=158, hidden=(44, 64), dropout_rate=0.23961099900759616, patience=29),\n",
        "Hyperparameters(lr=0.06016770079064622, epochs=80, hidden=(47,), dropout_rate=0.05991545061780843, patience=27),\n",
        "Hyperparameters(lr=0.0913161579000739, epochs=67, hidden=(52,), dropout_rate=0.08037962279718079, patience=16),\n",
        "Hyperparameters(lr=0.07665496489149337, epochs=86, hidden=(50,), dropout_rate=0.17329577809248436, patience=19),\n",
        "Hyperparameters(lr=0.07202572148217318, epochs=76, hidden=(72,), dropout_rate=0.10964808947852242, patience=6),\n",
        "Hyperparameters(lr=0.05724451809603519, epochs=168, hidden=(45,), dropout_rate=0.3157578860496876, patience=22),\n",
        "Hyperparameters(lr=0.07599547323615016, epochs=117, hidden=(40, 68), dropout_rate=0.0954707860264807, patience=9),\n",
        "Hyperparameters(lr=0.005164234261332292, epochs=228, hidden=(27, 51), dropout_rate=0.19062624499614939, patience=21),\n",
        "Hyperparameters(lr=0.0806143752346012, epochs=72, hidden=(37,), dropout_rate=0.07246881052498631, patience=11),\n",
        "Hyperparameters(lr=0.0700278038274355, epochs=115, hidden=(49, 16), dropout_rate=0.12553395358251676, patience=28),\n",
        "Hyperparameters(lr=0.005563737434721887, epochs=215, hidden=(37, 67), dropout_rate=0.08719107927609329, patience=20),\n",
        "Hyperparameters(lr=0.07099818060523648, epochs=172, hidden=(54, 45), dropout_rate=0.24697557149746108, patience=26),\n",
        "Hyperparameters(lr=0.08364310330878642, epochs=117, hidden=(23, 8), dropout_rate=0.08701830180863232, patience=26),\n",
        "Hyperparameters(lr=0.006795591837295142, epochs=243, hidden=(45, 64), dropout_rate=0.11529955266844638, patience=9),\n",
        "Hyperparameters(lr=0.06757823290909204, epochs=86, hidden=(20, 77), dropout_rate=0.1562567220599842, patience=23),\n",
        "Hyperparameters(lr=0.004970784571363729, epochs=214, hidden=(25, 106), dropout_rate=0.12105582693635134, patience=27),\n",
        "Hyperparameters(lr=0.05166151161353328, epochs=162, hidden=(64, 16, 32), dropout_rate=0.11630147790279065, patience=16),\n",
        "Hyperparameters(lr=0.05144920483806195, epochs=135, hidden=(48, 42), dropout_rate=0.21032467384703898, patience=15),\n",
        "Hyperparameters(lr=0.09472557848363745, epochs=84, hidden=(15, 64), dropout_rate=0.10565307869933374, patience=24),\n",
        "Hyperparameters(lr=0.06480597641772938, epochs=160, hidden=(70, 64), dropout_rate=0.3834200015584367, patience=21),\n",
        "Hyperparameters(lr=0.06497358378321397, epochs=131, hidden=(35,), dropout_rate=0.24168574633918521, patience=27),\n",
        "Hyperparameters(lr=0.07031735502681499, epochs=110, hidden=(18, 16, 14), dropout_rate=0.03139186073483591, patience=18),\n",
        "Hyperparameters(lr=0.04825552853889172, epochs=158, hidden=(54,), dropout_rate=0.2567679607709206, patience=5),\n",
        "Hyperparameters(lr=0.08396273208128421, epochs=94, hidden=(27, 8), dropout_rate=0.03630625524661061, patience=5),\n",
        "Hyperparameters(lr=0.04923427010744697, epochs=143, hidden=(46,), dropout_rate=0.21398201853796622, patience=5),\n",
        "Hyperparameters(lr=0.08065991838707287, epochs=100, hidden=(69,), dropout_rate=0.20408689261716048, patience=9),\n",
        "Hyperparameters(lr=0.06940133543683716, epochs=106, hidden=(33,), dropout_rate=0.0, patience=16),\n",
        "Hyperparameters(lr=0.05110468877753123, epochs=163, hidden=(16, 128, 32), dropout_rate=0.15930852353383868, patience=28),\n",
        "Hyperparameters(lr=0.05796641943260917, epochs=149, hidden=(16, 8), dropout_rate=0.059206445698600244, patience=12),\n",
        "Hyperparameters(lr=0.07387344004774914, epochs=94, hidden=(44,), dropout_rate=0.23724236072521934, patience=25),\n",
        "Hyperparameters(lr=0.0723942800422728, epochs=73, hidden=(19, 76), dropout_rate=0.10380150468382528, patience=27),\n",
        "Hyperparameters(lr=0.08074521180562258, epochs=106, hidden=(27, 64), dropout_rate=0.06880357788570875, patience=9),\n",
        "Hyperparameters(lr=0.07967962200677163, epochs=96, hidden=(8,), dropout_rate=0.002862267873272907, patience=18),\n",
        "Hyperparameters(lr=0.06788321735193605, epochs=87, hidden=(19, 60), dropout_rate=0.12008559782307664, patience=27),\n",
        "Hyperparameters(lr=0.0573499384501241, epochs=151, hidden=(8, 16), dropout_rate=0.0, patience=9),\n",
        "Hyperparameters(lr=0.07284624614928831, epochs=58, hidden=(40,), dropout_rate=0.0, patience=10),\n",
        "Hyperparameters(lr=0.07426375969821043, epochs=134, hidden=(11, 40), dropout_rate=0.18820374965742573, patience=18),\n",
        "Hyperparameters(lr=0.08456981765995479, epochs=145, hidden=(8, 55), dropout_rate=0.20322808964712197, patience=14),\n",
        "Hyperparameters(lr=0.046309303045632536, epochs=150, hidden=(23, 42), dropout_rate=0.36420054930291346, patience=20),\n",
        "Hyperparameters(lr=0.06090058137177728, epochs=167, hidden=(39,), dropout_rate=0.5, patience=22),\n",
        "Hyperparameters(lr=0.03953000836662636, epochs=93, hidden=(51, 23), dropout_rate=0.39069553088377457, patience=26),\n",
        "Hyperparameters(lr=0.06397309147497401, epochs=87, hidden=(23,), dropout_rate=0.18028264362359903, patience=30),\n",
        "Hyperparameters(lr=0.0050951023845820535, epochs=200, hidden=(82,), dropout_rate=0.04318069839862673, patience=23),\n",
        "Hyperparameters(lr=0.06716254757644693, epochs=129, hidden=(8,), dropout_rate=0.07860953792386219, patience=29),\n",
        "Hyperparameters(lr=0.005235635434359045, epochs=227, hidden=(37,), dropout_rate=0.0, patience=26),\n",
        "Hyperparameters(lr=0.03246880988921997, epochs=117, hidden=(41, 32), dropout_rate=0.5, patience=5),\n",
        "Hyperparameters(lr=0.051401368141830536, epochs=131, hidden=(44, 128), dropout_rate=0.3099107758141693, patience=20),\n",
        "Hyperparameters(lr=0.05858648600352965, epochs=149, hidden=(32, 8), dropout_rate=0.1869890634647153, patience=29),\n",
        "Hyperparameters(lr=0.07056025138079243, epochs=75, hidden=(8, 64), dropout_rate=0.0, patience=21),\n",
        "Hyperparameters(lr=0.04975330493306714, epochs=150, hidden=(9, 16), dropout_rate=0.23100845378154644, patience=11),\n",
        "Hyperparameters(lr=0.06071023420529073, epochs=163, hidden=(20,), dropout_rate=0.5, patience=25),\n",
        "Hyperparameters(lr=0.05520622275614853, epochs=155, hidden=(19, 8), dropout_rate=0.2564934987446767, patience=11),\n",
        "Hyperparameters(lr=0.005991239454495941, epochs=246, hidden=(66, 32, 64), dropout_rate=0.05836735890213736, patience=12),\n",
        "Hyperparameters(lr=0.048952438006446816, epochs=177, hidden=(8, 63), dropout_rate=0.23926640558564682, patience=24),\n",
        "Hyperparameters(lr=0.08408432906889683, epochs=101, hidden=(20, 8), dropout_rate=0.09442549913575635, patience=15),\n",
        "Hyperparameters(lr=0.0749659844040317, epochs=80, hidden=(11,), dropout_rate=0.06576117258504678, patience=21),\n",
        "Hyperparameters(lr=0.09397984553756318, epochs=70, hidden=(53, 64), dropout_rate=0.022279241802995883, patience=17),\n",
        "Hyperparameters(lr=0.006508156426303994, epochs=256, hidden=(66, 61), dropout_rate=0.15262933637770693, patience=5),\n",
        "Hyperparameters(lr=0.060268049787930454, epochs=200, hidden=(27, 16), dropout_rate=0.03091522702311223, patience=22),\n",
        "Hyperparameters(lr=0.0541036134914209, epochs=153, hidden=(13, 35, 16), dropout_rate=0.174647212710617, patience=28),\n",
        "Hyperparameters(lr=0.0064289472837459165, epochs=222, hidden=(66, 49), dropout_rate=0.11260893711122744, patience=5),\n",
        "Hyperparameters(lr=0.058528004298667016, epochs=163, hidden=(46, 93), dropout_rate=0.5, patience=29),\n",
        "Hyperparameters(lr=0.06386016889023398, epochs=165, hidden=(16, 26, 8), dropout_rate=0.4340944047797902, patience=11),\n",
        "Hyperparameters(lr=0.004660239600601557, epochs=227, hidden=(42, 87), dropout_rate=0.0, patience=20),\n",
        "Hyperparameters(lr=0.07051373453806209, epochs=124, hidden=(34, 54), dropout_rate=0.0, patience=23),\n",
        "Hyperparameters(lr=0.07496598755578027, epochs=94, hidden=(32, 20), dropout_rate=0.34924173263005776, patience=28),\n",
        "Hyperparameters(lr=0.06875038962445905, epochs=113, hidden=(8, 99), dropout_rate=0.13910503576816632, patience=28),\n",
        "Hyperparameters(lr=0.0768118382630108, epochs=72, hidden=(64, 71), dropout_rate=0.13519642210781616, patience=5),\n",
        "Hyperparameters(lr=0.05646251441732532, epochs=170, hidden=(10, 256), dropout_rate=0.14381812448297193, patience=18),\n",
        "Hyperparameters(lr=0.07019020403074706, epochs=102, hidden=(70, 158), dropout_rate=0.10644749136598143, patience=20),\n",
        "Hyperparameters(lr=0.08493804256424775, epochs=87, hidden=(48, 128), dropout_rate=0.1647923390020374, patience=18),\n",
        "Hyperparameters(lr=0.07327040119395435, epochs=115, hidden=(54, 128), dropout_rate=0.10316116891003746, patience=17),\n",
        "Hyperparameters(lr=0.07269762862341113, epochs=109, hidden=(256, 49), dropout_rate=0.06311284549365469, patience=28),\n",
        "Hyperparameters(lr=0.07472873582379554, epochs=106, hidden=(128, 49), dropout_rate=0.12982785211497452, patience=30),\n",
        "Hyperparameters(lr=0.05412337655931938, epochs=131, hidden=(128, 44), dropout_rate=0.2836552443611482, patience=13),\n",
        "Hyperparameters(lr=0.0720624633486621, epochs=120, hidden=(49, 256), dropout_rate=0.1065832816752969, patience=30),\n",
        "Hyperparameters(lr=0.05140577697147596, epochs=143, hidden=(128, 44), dropout_rate=0.3627274999006924, patience=20),\n",
        "Hyperparameters(lr=0.1, epochs=104, hidden=(64, 24, 64), dropout_rate=0.2236299180500707, patience=21),\n",
        "Hyperparameters(lr=0.08548694896021478, epochs=128, hidden=(40, 64, 256), dropout_rate=0.09582517795823102, patience=14),\n",
        "Hyperparameters(lr=0.005815400886198472, epochs=225, hidden=(66, 64, 256), dropout_rate=0.14997780640643327, patience=16),\n",
        "Hyperparameters(lr=0.08092507001962815, epochs=103, hidden=(27, 20), dropout_rate=0.030842370447529834, patience=5)]"
      ],
      "metadata": {
        "id": "gp5zi8pxGDFG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pop = evolution_strategy(mu=100, lambda_=100, tau=0.05, max_gens=10)\n",
        "for member in final_pop:\n",
        "    print(member)"
      ],
      "metadata": {
        "id": "g_WgpIjQeQ4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test output"
      ],
      "metadata": {
        "id": "WGchc6511C2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stats for final generation\n",
        "final_pop_stats = [train(member) for member in final_pop]\n",
        "print(final_pop_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK7iYZfTXJ1b",
        "outputId": "ac9a33da-0bcf-4a1b-8e8d-ee9260c49b6d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/110 | train_loss=0.3107 train_acc=0.8037 train_f1=0.2941 val_loss=2.9974 val_acc=0.7258 val_f1=0.1669 \n",
            "Epoch 22/110 | train_loss=0.1797 train_acc=0.9320 train_f1=0.3646 val_loss=2.7483 val_acc=0.8419 val_f1=0.1882 \n",
            "Early stopping at epoch 28\n",
            "Epoch 15/155 | train_loss=0.2518 train_acc=0.8646 train_f1=0.3968 val_loss=3.2697 val_acc=0.7790 val_f1=0.1856 \n",
            "Early stopping at epoch 23\n",
            "Epoch 15/154 | train_loss=0.2198 train_acc=0.8958 train_f1=0.4296 val_loss=3.3257 val_acc=0.8129 val_f1=0.1831 \n",
            "Early stopping at epoch 19\n",
            "Epoch 15/150 | train_loss=0.2628 train_acc=0.8738 train_f1=0.3302 val_loss=3.0034 val_acc=0.8081 val_f1=0.1822 \n",
            "Epoch 30/150 | train_loss=0.1384 train_acc=0.9171 train_f1=0.4718 val_loss=3.5969 val_acc=0.8242 val_f1=0.1902 \n",
            "Early stopping at epoch 30\n",
            "Epoch 12/128 | train_loss=0.2942 train_acc=0.7519 train_f1=0.2826 val_loss=2.7488 val_acc=0.6790 val_f1=0.1579 \n",
            "Epoch 24/128 | train_loss=0.2085 train_acc=0.9057 train_f1=0.3488 val_loss=2.7371 val_acc=0.8210 val_f1=0.1847 \n",
            "Early stopping at epoch 33\n",
            "Early stopping at epoch 6\n",
            "Epoch 13/131 | train_loss=0.2714 train_acc=0.8320 train_f1=0.3651 val_loss=4.1463 val_acc=0.7484 val_f1=0.1712 \n",
            "Epoch 26/131 | train_loss=0.1338 train_acc=0.9391 train_f1=0.4836 val_loss=7.0009 val_acc=0.8306 val_f1=0.1863 \n",
            "Epoch 39/131 | train_loss=0.0876 train_acc=0.9596 train_f1=0.6345 val_loss=7.0845 val_acc=0.8452 val_f1=0.1890 \n",
            "Early stopping at epoch 43\n",
            "Epoch 11/111 | train_loss=0.2631 train_acc=0.8781 train_f1=0.3681 val_loss=3.5464 val_acc=0.8032 val_f1=0.1865 \n",
            "Epoch 22/111 | train_loss=0.1397 train_acc=0.9468 train_f1=0.4689 val_loss=3.6729 val_acc=0.8565 val_f1=0.1909 \n",
            "Epoch 33/111 | train_loss=0.0909 train_acc=0.9398 train_f1=0.4978 val_loss=5.4121 val_acc=0.8403 val_f1=0.1934 \n",
            "Epoch 44/111 | train_loss=0.0603 train_acc=0.9681 train_f1=0.6194 val_loss=6.1433 val_acc=0.8742 val_f1=0.1993 \n",
            "Early stopping at epoch 44\n",
            "Epoch 15/153 | train_loss=0.2608 train_acc=0.8866 train_f1=0.4157 val_loss=3.6346 val_acc=0.7871 val_f1=0.1833 \n",
            "Epoch 30/153 | train_loss=0.1379 train_acc=0.9284 train_f1=0.4268 val_loss=4.2250 val_acc=0.8468 val_f1=0.1893 \n",
            "Epoch 45/153 | train_loss=0.0882 train_acc=0.9596 train_f1=0.6181 val_loss=4.9039 val_acc=0.8645 val_f1=0.1923 \n",
            "Early stopping at epoch 49\n",
            "Epoch 17/170 | train_loss=0.2005 train_acc=0.9150 train_f1=0.3624 val_loss=2.8774 val_acc=0.8452 val_f1=0.1890 \n",
            "Early stopping at epoch 17\n",
            "Epoch 11/113 | train_loss=0.3114 train_acc=0.8703 train_f1=0.3434 val_loss=3.6996 val_acc=0.8081 val_f1=0.1819 \n",
            "Epoch 22/113 | train_loss=0.2423 train_acc=0.9213 train_f1=0.4455 val_loss=4.3512 val_acc=0.8258 val_f1=0.1854 \n",
            "Early stopping at epoch 28\n",
            "Epoch 17/179 | train_loss=0.3684 train_acc=0.7739 train_f1=0.2832 val_loss=2.6324 val_acc=0.6839 val_f1=0.1589 \n",
            "Epoch 34/179 | train_loss=0.2380 train_acc=0.9029 train_f1=0.3470 val_loss=2.5128 val_acc=0.8226 val_f1=0.1847 \n",
            "Early stopping at epoch 37\n",
            "Early stopping at epoch 11\n",
            "Early stopping at epoch 7\n",
            "Epoch 15/153 | train_loss=0.3311 train_acc=0.8335 train_f1=0.3451 val_loss=4.1569 val_acc=0.7758 val_f1=0.1763 \n",
            "Epoch 30/153 | train_loss=0.1810 train_acc=0.9142 train_f1=0.4536 val_loss=4.4138 val_acc=0.8274 val_f1=0.1856 \n",
            "Early stopping at epoch 44\n",
            "Epoch 7/79 | train_loss=0.3681 train_acc=0.7236 train_f1=0.2630 val_loss=4.4571 val_acc=0.6565 val_f1=0.1535 \n",
            "Epoch 14/79 | train_loss=0.2128 train_acc=0.8887 train_f1=0.3472 val_loss=3.6332 val_acc=0.8274 val_f1=0.1858 \n",
            "Epoch 21/79 | train_loss=0.1518 train_acc=0.9362 train_f1=0.4567 val_loss=3.8359 val_acc=0.8371 val_f1=0.1874 \n",
            "Early stopping at epoch 22\n",
            "Epoch 8/86 | train_loss=0.3973 train_acc=0.7477 train_f1=0.2708 val_loss=2.5053 val_acc=0.6871 val_f1=0.1590 \n",
            "Epoch 16/86 | train_loss=0.2107 train_acc=0.8653 train_f1=0.3774 val_loss=2.5134 val_acc=0.8016 val_f1=0.1866 \n",
            "Epoch 24/86 | train_loss=0.1611 train_acc=0.9192 train_f1=0.4254 val_loss=3.0422 val_acc=0.8371 val_f1=0.1874 \n",
            "Epoch 32/86 | train_loss=0.1304 train_acc=0.9518 train_f1=0.4947 val_loss=3.6076 val_acc=0.8500 val_f1=0.1951 \n",
            "Early stopping at epoch 37\n",
            "Epoch 15/151 | train_loss=0.2944 train_acc=0.7931 train_f1=0.3322 val_loss=3.4357 val_acc=0.7258 val_f1=0.1670 \n",
            "Epoch 30/151 | train_loss=0.2042 train_acc=0.9277 train_f1=0.4424 val_loss=4.3611 val_acc=0.8452 val_f1=0.1890 \n",
            "Early stopping at epoch 37\n",
            "Epoch 12/121 | train_loss=0.3175 train_acc=0.8108 train_f1=0.2996 val_loss=2.6792 val_acc=0.7500 val_f1=0.1715 \n",
            "Epoch 24/121 | train_loss=0.2213 train_acc=0.9213 train_f1=0.3599 val_loss=2.9663 val_acc=0.8532 val_f1=0.1903 \n",
            "Early stopping at epoch 26\n",
            "Epoch 15/158 | train_loss=0.3185 train_acc=0.8951 train_f1=0.3427 val_loss=2.9764 val_acc=0.8274 val_f1=0.1857 \n",
            "Epoch 30/158 | train_loss=0.1700 train_acc=0.9419 train_f1=0.3741 val_loss=3.5207 val_acc=0.8661 val_f1=0.1927 \n",
            "Epoch 45/158 | train_loss=0.1274 train_acc=0.9376 train_f1=0.4118 val_loss=4.5615 val_acc=0.8419 val_f1=0.1885 \n",
            "Early stopping at epoch 51\n",
            "Epoch 8/80 | train_loss=0.3842 train_acc=0.6910 train_f1=0.2513 val_loss=5.9351 val_acc=0.6274 val_f1=0.1479 \n",
            "Epoch 16/80 | train_loss=0.1916 train_acc=0.9121 train_f1=0.3937 val_loss=3.5445 val_acc=0.8452 val_f1=0.1891 \n",
            "Epoch 24/80 | train_loss=0.1301 train_acc=0.9362 train_f1=0.4479 val_loss=4.0590 val_acc=0.8419 val_f1=0.1884 \n",
            "Early stopping at epoch 28\n",
            "Epoch 6/67 | train_loss=0.4784 train_acc=0.6719 train_f1=0.2447 val_loss=8.4209 val_acc=0.5823 val_f1=0.1385 \n",
            "Epoch 12/67 | train_loss=0.2543 train_acc=0.8675 train_f1=0.3846 val_loss=2.9589 val_acc=0.8081 val_f1=0.1823 \n",
            "Epoch 18/67 | train_loss=0.1633 train_acc=0.9157 train_f1=0.4369 val_loss=4.3719 val_acc=0.8371 val_f1=0.1875 \n",
            "Epoch 24/67 | train_loss=0.1497 train_acc=0.9320 train_f1=0.4922 val_loss=4.4652 val_acc=0.8500 val_f1=0.1949 \n",
            "Early stopping at epoch 29\n",
            "Epoch 8/86 | train_loss=0.3925 train_acc=0.8257 train_f1=0.3051 val_loss=5.1620 val_acc=0.7516 val_f1=0.1718 \n",
            "Epoch 16/86 | train_loss=0.1991 train_acc=0.9334 train_f1=0.3916 val_loss=3.9957 val_acc=0.8581 val_f1=0.1912 \n",
            "Epoch 24/86 | train_loss=0.2060 train_acc=0.9511 train_f1=0.4850 val_loss=4.5819 val_acc=0.8629 val_f1=0.1974 \n",
            "Early stopping at epoch 30\n",
            "Epoch 7/76 | train_loss=0.5017 train_acc=0.7066 train_f1=0.2576 val_loss=8.4365 val_acc=0.6194 val_f1=0.1460 \n",
            "Early stopping at epoch 7\n",
            "Epoch 16/168 | train_loss=0.2490 train_acc=0.8498 train_f1=0.3937 val_loss=3.1270 val_acc=0.7742 val_f1=0.1760 \n",
            "Early stopping at epoch 23\n",
            "Epoch 11/117 | train_loss=0.4263 train_acc=0.5493 train_f1=0.2020 val_loss=2.5564 val_acc=0.4532 val_f1=0.1115 \n",
            "Early stopping at epoch 18\n",
            "Epoch 22/228 | train_loss=0.5512 train_acc=0.0971 train_f1=0.0620 val_loss=3.9266 val_acc=0.0210 val_f1=0.0058 \n",
            "Early stopping at epoch 22\n",
            "Epoch 7/72 | train_loss=0.4187 train_acc=0.6577 train_f1=0.2382 val_loss=5.6154 val_acc=0.5742 val_f1=0.1371 \n",
            "Early stopping at epoch 12\n",
            "Epoch 11/115 | train_loss=0.3164 train_acc=0.9164 train_f1=0.3476 val_loss=2.0971 val_acc=0.8419 val_f1=0.1882 \n",
            "Epoch 22/115 | train_loss=0.2341 train_acc=0.9305 train_f1=0.3637 val_loss=2.3398 val_acc=0.8516 val_f1=0.1900 \n",
            "Epoch 33/115 | train_loss=0.1678 train_acc=0.9454 train_f1=0.4027 val_loss=2.9192 val_acc=0.8645 val_f1=0.1923 \n",
            "Early stopping at epoch 38\n",
            "Epoch 21/215 | train_loss=0.4522 train_acc=0.6917 train_f1=0.2509 val_loss=3.5677 val_acc=0.6113 val_f1=0.1442 \n",
            "Early stopping at epoch 21\n",
            "Epoch 17/172 | train_loss=0.2700 train_acc=0.9178 train_f1=0.3564 val_loss=2.9347 val_acc=0.8403 val_f1=0.1878 \n",
            "Epoch 34/172 | train_loss=0.1743 train_acc=0.9391 train_f1=0.3706 val_loss=4.0900 val_acc=0.8532 val_f1=0.1903 \n",
            "Early stopping at epoch 35\n",
            "Epoch 11/117 | train_loss=0.4789 train_acc=0.3572 train_f1=0.1424 val_loss=2.6315 val_acc=0.2548 val_f1=0.0659 \n",
            "Epoch 22/117 | train_loss=0.2871 train_acc=0.8554 train_f1=0.3230 val_loss=3.0917 val_acc=0.7952 val_f1=0.1797 \n",
            "Early stopping at epoch 27\n",
            "Early stopping at epoch 10\n",
            "Epoch 8/86 | train_loss=0.3613 train_acc=0.8611 train_f1=0.3199 val_loss=2.4989 val_acc=0.7919 val_f1=0.1787 \n",
            "Epoch 16/86 | train_loss=0.2705 train_acc=0.8646 train_f1=0.3269 val_loss=2.6395 val_acc=0.8016 val_f1=0.1810 \n",
            "Epoch 24/86 | train_loss=0.1926 train_acc=0.9235 train_f1=0.3618 val_loss=2.7847 val_acc=0.8532 val_f1=0.1903 \n",
            "Epoch 32/86 | train_loss=0.1439 train_acc=0.9313 train_f1=0.3809 val_loss=3.0857 val_acc=0.8532 val_f1=0.1958 \n",
            "Early stopping at epoch 32\n",
            "Epoch 21/214 | train_loss=0.4562 train_acc=0.7349 train_f1=0.2666 val_loss=3.7330 val_acc=0.6516 val_f1=0.1522 \n",
            "Early stopping at epoch 28\n",
            "Epoch 16/162 | train_loss=0.3010 train_acc=0.8923 train_f1=0.3390 val_loss=2.5608 val_acc=0.8032 val_f1=0.1813 \n",
            "Early stopping at epoch 28\n",
            "Epoch 13/135 | train_loss=0.4350 train_acc=0.7739 train_f1=0.2797 val_loss=2.0255 val_acc=0.6806 val_f1=0.1581 \n",
            "Epoch 26/135 | train_loss=0.2302 train_acc=0.9291 train_f1=0.3651 val_loss=2.9986 val_acc=0.8403 val_f1=0.1877 \n",
            "Early stopping at epoch 27\n",
            "Epoch 8/84 | train_loss=0.4015 train_acc=0.8327 train_f1=0.3085 val_loss=2.2119 val_acc=0.7403 val_f1=0.1693 \n",
            "Epoch 16/84 | train_loss=0.2586 train_acc=0.8838 train_f1=0.3361 val_loss=2.4109 val_acc=0.8065 val_f1=0.1820 \n",
            "Epoch 24/84 | train_loss=0.2010 train_acc=0.8831 train_f1=0.3710 val_loss=3.5934 val_acc=0.7984 val_f1=0.1805 \n",
            "Epoch 32/84 | train_loss=0.2069 train_acc=0.9079 train_f1=0.3842 val_loss=4.0068 val_acc=0.8194 val_f1=0.1846 \n",
            "Early stopping at epoch 33\n",
            "Epoch 16/160 | train_loss=0.2716 train_acc=0.9008 train_f1=0.3471 val_loss=2.5304 val_acc=0.8419 val_f1=0.1882 \n",
            "Early stopping at epoch 29\n",
            "Epoch 13/131 | train_loss=0.2978 train_acc=0.8944 train_f1=0.3762 val_loss=2.8586 val_acc=0.8290 val_f1=0.1859 \n",
            "Epoch 26/131 | train_loss=0.1709 train_acc=0.9313 train_f1=0.4394 val_loss=3.3009 val_acc=0.8516 val_f1=0.1901 \n",
            "Epoch 39/131 | train_loss=0.1328 train_acc=0.9476 train_f1=0.4787 val_loss=4.0876 val_acc=0.8484 val_f1=0.1895 \n",
            "Early stopping at epoch 41\n",
            "Epoch 11/110 | train_loss=0.3577 train_acc=0.8327 train_f1=0.3082 val_loss=2.7418 val_acc=0.7548 val_f1=0.1722 \n",
            "Early stopping at epoch 19\n",
            "Early stopping at epoch 6\n",
            "Early stopping at epoch 6\n",
            "Early stopping at epoch 6\n",
            "Epoch 10/100 | train_loss=0.4608 train_acc=0.7746 train_f1=0.2921 val_loss=6.2353 val_acc=0.7145 val_f1=0.1647 \n",
            "Epoch 20/100 | train_loss=0.2329 train_acc=0.9015 train_f1=0.4239 val_loss=5.0143 val_acc=0.8032 val_f1=0.1813 \n",
            "Early stopping at epoch 23\n",
            "Epoch 10/106 | train_loss=0.3548 train_acc=0.7597 train_f1=0.2779 val_loss=5.2237 val_acc=0.6903 val_f1=0.1601 \n",
            "Early stopping at epoch 17\n",
            "Epoch 16/163 | train_loss=0.3053 train_acc=0.8781 train_f1=0.3337 val_loss=2.7604 val_acc=0.8065 val_f1=0.1820 \n",
            "Early stopping at epoch 30\n",
            "Early stopping at epoch 13\n",
            "Epoch 9/94 | train_loss=0.3781 train_acc=0.7619 train_f1=0.2799 val_loss=5.2492 val_acc=0.6968 val_f1=0.1613 \n",
            "Epoch 18/94 | train_loss=0.2173 train_acc=0.9164 train_f1=0.3767 val_loss=3.5392 val_acc=0.8435 val_f1=0.1886 \n",
            "Early stopping at epoch 26\n",
            "Epoch 7/73 | train_loss=0.6189 train_acc=0.2112 train_f1=0.0975 val_loss=4.6150 val_acc=0.1161 val_f1=0.0312 \n",
            "Epoch 14/73 | train_loss=0.2962 train_acc=0.8887 train_f1=0.3343 val_loss=2.4989 val_acc=0.8210 val_f1=0.1845 \n",
            "Epoch 21/73 | train_loss=0.2104 train_acc=0.9107 train_f1=0.4097 val_loss=2.4460 val_acc=0.8323 val_f1=0.1919 \n",
            "Epoch 28/73 | train_loss=0.1363 train_acc=0.9362 train_f1=0.4421 val_loss=2.8536 val_acc=0.8452 val_f1=0.1889 \n",
            "Epoch 35/73 | train_loss=0.1335 train_acc=0.9199 train_f1=0.4097 val_loss=3.5361 val_acc=0.8403 val_f1=0.1934 \n",
            "Early stopping at epoch 38\n",
            "Epoch 10/106 | train_loss=0.3205 train_acc=0.7902 train_f1=0.2916 val_loss=3.8745 val_acc=0.7290 val_f1=0.1675 \n",
            "Epoch 20/106 | train_loss=0.1531 train_acc=0.9242 train_f1=0.3999 val_loss=3.3702 val_acc=0.8419 val_f1=0.1882 \n",
            "Early stopping at epoch 20\n",
            "Epoch 9/96 | train_loss=0.4808 train_acc=0.6662 train_f1=0.2422 val_loss=5.8644 val_acc=0.5726 val_f1=0.1363 \n",
            "Epoch 18/96 | train_loss=0.2770 train_acc=0.8717 train_f1=0.3304 val_loss=3.1283 val_acc=0.8097 val_f1=0.1825 \n",
            "Early stopping at epoch 19\n",
            "Epoch 8/87 | train_loss=0.3637 train_acc=0.7718 train_f1=0.2807 val_loss=3.2686 val_acc=0.6887 val_f1=0.1597 \n",
            "Epoch 16/87 | train_loss=0.2507 train_acc=0.9157 train_f1=0.3556 val_loss=2.7448 val_acc=0.8355 val_f1=0.1872 \n",
            "Epoch 24/87 | train_loss=0.1457 train_acc=0.9490 train_f1=0.4181 val_loss=2.8141 val_acc=0.8694 val_f1=0.1931 \n",
            "Epoch 32/87 | train_loss=0.1077 train_acc=0.9624 train_f1=0.5072 val_loss=3.9854 val_acc=0.8677 val_f1=0.1983 \n",
            "Epoch 40/87 | train_loss=0.1089 train_acc=0.9603 train_f1=0.4998 val_loss=4.2054 val_acc=0.8677 val_f1=0.1930 \n",
            "Early stopping at epoch 42\n",
            "Early stopping at epoch 10\n",
            "Epoch 5/58 | train_loss=0.5239 train_acc=0.7923 train_f1=0.2883 val_loss=6.0292 val_acc=0.6968 val_f1=0.1610 \n",
            "Epoch 10/58 | train_loss=0.2832 train_acc=0.8788 train_f1=0.3326 val_loss=3.7488 val_acc=0.8016 val_f1=0.1810 \n",
            "Epoch 15/58 | train_loss=0.2081 train_acc=0.9079 train_f1=0.3911 val_loss=3.4838 val_acc=0.8194 val_f1=0.1842 \n",
            "Epoch 20/58 | train_loss=0.1569 train_acc=0.9405 train_f1=0.4175 val_loss=4.2837 val_acc=0.8581 val_f1=0.2021 \n",
            "Early stopping at epoch 22\n",
            "Epoch 13/134 | train_loss=0.3577 train_acc=0.8668 train_f1=0.3239 val_loss=2.1305 val_acc=0.7935 val_f1=0.1795 \n",
            "Epoch 26/134 | train_loss=0.2374 train_acc=0.9199 train_f1=0.3577 val_loss=2.7065 val_acc=0.8387 val_f1=0.1878 \n",
            "Epoch 39/134 | train_loss=0.1774 train_acc=0.9490 train_f1=0.4295 val_loss=2.8410 val_acc=0.8661 val_f1=0.1927 \n",
            "Early stopping at epoch 39\n",
            "Epoch 14/145 | train_loss=0.4439 train_acc=0.6690 train_f1=0.2415 val_loss=2.5040 val_acc=0.5790 val_f1=0.1378 \n",
            "Early stopping at epoch 15\n",
            "Epoch 15/150 | train_loss=0.3588 train_acc=0.7718 train_f1=0.2813 val_loss=2.3093 val_acc=0.7097 val_f1=0.1639 \n",
            "Epoch 30/150 | train_loss=0.2431 train_acc=0.8901 train_f1=0.3393 val_loss=2.7001 val_acc=0.8161 val_f1=0.1839 \n",
            "Early stopping at epoch 34\n",
            "Epoch 16/167 | train_loss=0.2940 train_acc=0.8639 train_f1=0.3338 val_loss=3.7814 val_acc=0.7871 val_f1=0.1783 \n",
            "Early stopping at epoch 23\n",
            "Epoch 9/93 | train_loss=0.4658 train_acc=0.4387 train_f1=0.1676 val_loss=4.3886 val_acc=0.3435 val_f1=0.0869 \n",
            "Epoch 18/93 | train_loss=0.3198 train_acc=0.8405 train_f1=0.3134 val_loss=2.5157 val_acc=0.7694 val_f1=0.1750 \n",
            "Epoch 27/93 | train_loss=0.2483 train_acc=0.8916 train_f1=0.3417 val_loss=2.6190 val_acc=0.8177 val_f1=0.1840 \n",
            "Early stopping at epoch 27\n",
            "Epoch 8/87 | train_loss=0.5023 train_acc=0.7952 train_f1=0.2889 val_loss=4.3387 val_acc=0.7355 val_f1=0.1686 \n",
            "Epoch 16/87 | train_loss=0.3002 train_acc=0.7987 train_f1=0.2935 val_loss=2.4951 val_acc=0.7387 val_f1=0.1694 \n",
            "Epoch 24/87 | train_loss=0.2126 train_acc=0.8887 train_f1=0.3385 val_loss=3.2305 val_acc=0.8113 val_f1=0.1829 \n",
            "Epoch 32/87 | train_loss=0.1755 train_acc=0.9135 train_f1=0.4001 val_loss=3.3004 val_acc=0.8339 val_f1=0.1924 \n",
            "Epoch 40/87 | train_loss=0.1282 train_acc=0.9525 train_f1=0.4833 val_loss=3.4504 val_acc=0.8597 val_f1=0.1915 \n",
            "Epoch 48/87 | train_loss=0.1231 train_acc=0.9398 train_f1=0.4681 val_loss=4.2021 val_acc=0.8419 val_f1=0.1882 \n",
            "Early stopping at epoch 48\n",
            "Epoch 20/200 | train_loss=0.3836 train_acc=0.6903 train_f1=0.2496 val_loss=3.0513 val_acc=0.6048 val_f1=0.1430 \n",
            "Early stopping at epoch 24\n",
            "Epoch 12/129 | train_loss=0.3884 train_acc=0.6612 train_f1=0.2403 val_loss=4.6723 val_acc=0.5774 val_f1=0.1376 \n",
            "Epoch 24/129 | train_loss=0.2441 train_acc=0.8356 train_f1=0.3127 val_loss=3.1525 val_acc=0.7758 val_f1=0.1819 \n",
            "Epoch 36/129 | train_loss=0.1858 train_acc=0.8661 train_f1=0.3481 val_loss=3.4181 val_acc=0.8016 val_f1=0.1812 \n",
            "Epoch 48/129 | train_loss=0.1567 train_acc=0.9135 train_f1=0.4404 val_loss=3.2644 val_acc=0.8387 val_f1=0.1933 \n",
            "Early stopping at epoch 48\n",
            "Epoch 22/227 | train_loss=0.4502 train_acc=0.4876 train_f1=0.1825 val_loss=2.8071 val_acc=0.3839 val_f1=0.0960 \n",
            "Early stopping at epoch 27\n",
            "Early stopping at epoch 6\n",
            "Epoch 13/131 | train_loss=0.3453 train_acc=0.7739 train_f1=0.2836 val_loss=3.5433 val_acc=0.6839 val_f1=0.1587 \n",
            "Epoch 26/131 | train_loss=0.1940 train_acc=0.9185 train_f1=0.3653 val_loss=3.4834 val_acc=0.8403 val_f1=0.1881 \n",
            "Epoch 39/131 | train_loss=0.1379 train_acc=0.9376 train_f1=0.5397 val_loss=4.2638 val_acc=0.8403 val_f1=0.1881 \n",
            "Early stopping at epoch 39\n",
            "Epoch 14/149 | train_loss=0.3597 train_acc=0.8143 train_f1=0.3002 val_loss=2.1539 val_acc=0.7581 val_f1=0.1727 \n",
            "Epoch 28/149 | train_loss=0.2237 train_acc=0.9213 train_f1=0.3591 val_loss=2.7775 val_acc=0.8419 val_f1=0.1882 \n",
            "Early stopping at epoch 30\n",
            "Epoch 7/75 | train_loss=0.4215 train_acc=0.7697 train_f1=0.2809 val_loss=4.3182 val_acc=0.6968 val_f1=0.1611 \n",
            "Epoch 14/75 | train_loss=0.2211 train_acc=0.8760 train_f1=0.3332 val_loss=2.4288 val_acc=0.8081 val_f1=0.1822 \n",
            "Epoch 21/75 | train_loss=0.1471 train_acc=0.9412 train_f1=0.3730 val_loss=3.7277 val_acc=0.8645 val_f1=0.1920 \n",
            "Epoch 28/75 | train_loss=0.1017 train_acc=0.9568 train_f1=0.4486 val_loss=4.6189 val_acc=0.8677 val_f1=0.1980 \n",
            "Early stopping at epoch 34\n",
            "Early stopping at epoch 12\n",
            "Epoch 16/163 | train_loss=0.3884 train_acc=0.8051 train_f1=0.3033 val_loss=3.7154 val_acc=0.7323 val_f1=0.1681 \n",
            "Early stopping at epoch 26\n",
            "Early stopping at epoch 12\n",
            "Early stopping at epoch 13\n",
            "Epoch 17/177 | train_loss=0.4005 train_acc=0.6570 train_f1=0.2383 val_loss=2.2485 val_acc=0.5742 val_f1=0.1369 \n",
            "Early stopping at epoch 25\n",
            "Epoch 10/101 | train_loss=0.3641 train_acc=0.8001 train_f1=0.2929 val_loss=2.9774 val_acc=0.7597 val_f1=0.1732 \n",
            "Early stopping at epoch 16\n",
            "Epoch 8/80 | train_loss=0.4704 train_acc=0.7399 train_f1=0.2666 val_loss=4.7462 val_acc=0.6726 val_f1=0.1560 \n",
            "Epoch 16/80 | train_loss=0.2943 train_acc=0.8214 train_f1=0.3498 val_loss=3.0492 val_acc=0.7419 val_f1=0.1697 \n",
            "Early stopping at epoch 22\n",
            "Epoch 7/70 | train_loss=0.7810 train_acc=0.2197 train_f1=0.1001 val_loss=3.9122 val_acc=0.1097 val_f1=0.0295 \n",
            "Epoch 14/70 | train_loss=0.3447 train_acc=0.8434 train_f1=0.3142 val_loss=2.5656 val_acc=0.7452 val_f1=0.1705 \n",
            "Epoch 21/70 | train_loss=0.2173 train_acc=0.8901 train_f1=0.3401 val_loss=2.5913 val_acc=0.8194 val_f1=0.1841 \n",
            "Early stopping at epoch 21\n",
            "Early stopping at epoch 6\n",
            "Epoch 20/200 | train_loss=0.1908 train_acc=0.9355 train_f1=0.3651 val_loss=2.5855 val_acc=0.8516 val_f1=0.1901 \n",
            "Early stopping at epoch 36\n",
            "Epoch 15/153 | train_loss=0.4180 train_acc=0.7009 train_f1=0.2559 val_loss=2.4795 val_acc=0.6177 val_f1=0.1457 \n",
            "Early stopping at epoch 29\n",
            "Early stopping at epoch 6\n",
            "Epoch 16/163 | train_loss=0.3387 train_acc=0.8894 train_f1=0.3381 val_loss=2.3042 val_acc=0.8161 val_f1=0.1836 \n",
            "Epoch 32/163 | train_loss=0.2948 train_acc=0.9313 train_f1=0.3650 val_loss=3.2406 val_acc=0.8387 val_f1=0.1878 \n",
            "Early stopping at epoch 45\n",
            "Early stopping at epoch 12\n",
            "Early stopping at epoch 21\n",
            "Epoch 12/124 | train_loss=0.2890 train_acc=0.8377 train_f1=0.3114 val_loss=2.6781 val_acc=0.7661 val_f1=0.1742 \n",
            "Epoch 24/124 | train_loss=0.1246 train_acc=0.9554 train_f1=0.3902 val_loss=2.7028 val_acc=0.8806 val_f1=0.1949 \n",
            "Early stopping at epoch 33\n",
            "Epoch 9/94 | train_loss=0.4421 train_acc=0.5656 train_f1=0.2083 val_loss=2.7576 val_acc=0.4968 val_f1=0.1207 \n",
            "Epoch 18/94 | train_loss=0.3043 train_acc=0.8972 train_f1=0.3454 val_loss=2.6611 val_acc=0.8323 val_f1=0.1864 \n",
            "Epoch 27/94 | train_loss=0.2369 train_acc=0.9235 train_f1=0.3593 val_loss=3.6011 val_acc=0.8484 val_f1=0.1895 \n",
            "Epoch 36/94 | train_loss=0.2281 train_acc=0.9383 train_f1=0.3705 val_loss=3.7098 val_acc=0.8532 val_f1=0.1904 \n",
            "Epoch 45/94 | train_loss=0.2186 train_acc=0.9504 train_f1=0.3796 val_loss=3.6482 val_acc=0.8629 val_f1=0.1920 \n",
            "Early stopping at epoch 45\n",
            "Epoch 11/113 | train_loss=0.3523 train_acc=0.8604 train_f1=0.3217 val_loss=2.1956 val_acc=0.7871 val_f1=0.1784 \n",
            "Epoch 22/113 | train_loss=0.2634 train_acc=0.8597 train_f1=0.3222 val_loss=2.3856 val_acc=0.7935 val_f1=0.1797 \n",
            "Epoch 33/113 | train_loss=0.2080 train_acc=0.9086 train_f1=0.3903 val_loss=2.7012 val_acc=0.8258 val_f1=0.1858 \n",
            "Early stopping at epoch 39\n",
            "Epoch 7/72 | train_loss=0.4296 train_acc=0.8327 train_f1=0.3076 val_loss=2.2935 val_acc=0.7694 val_f1=0.1747 \n",
            "Early stopping at epoch 12\n",
            "Epoch 17/170 | train_loss=0.3260 train_acc=0.8540 train_f1=0.3196 val_loss=2.4400 val_acc=0.7903 val_f1=0.1790 \n",
            "Epoch 34/170 | train_loss=0.1934 train_acc=0.8994 train_f1=0.4145 val_loss=2.5028 val_acc=0.8194 val_f1=0.1952 \n",
            "Early stopping at epoch 38\n",
            "Epoch 10/102 | train_loss=0.4296 train_acc=0.7605 train_f1=0.2764 val_loss=3.0244 val_acc=0.6774 val_f1=0.1577 \n",
            "Epoch 20/102 | train_loss=0.2095 train_acc=0.9199 train_f1=0.3588 val_loss=3.2939 val_acc=0.8371 val_f1=0.1875 \n",
            "Epoch 30/102 | train_loss=0.1278 train_acc=0.9355 train_f1=0.4650 val_loss=3.5729 val_acc=0.8210 val_f1=0.1895 \n",
            "Early stopping at epoch 33\n",
            "Epoch 8/87 | train_loss=0.4444 train_acc=0.8101 train_f1=0.2913 val_loss=2.6068 val_acc=0.6968 val_f1=0.1611 \n",
            "Epoch 16/87 | train_loss=0.3233 train_acc=0.9114 train_f1=0.3482 val_loss=2.2741 val_acc=0.8435 val_f1=0.1887 \n",
            "Epoch 24/87 | train_loss=0.2188 train_acc=0.9419 train_f1=0.3722 val_loss=3.2683 val_acc=0.8710 val_f1=0.1934 \n",
            "Epoch 32/87 | train_loss=0.2120 train_acc=0.9483 train_f1=0.3849 val_loss=5.0318 val_acc=0.8694 val_f1=0.1931 \n",
            "Early stopping at epoch 33\n",
            "Epoch 11/115 | train_loss=0.4837 train_acc=0.0659 train_f1=0.0522 val_loss=2.7449 val_acc=0.0000 val_f1=0.0000 \n",
            "Early stopping at epoch 21\n",
            "Epoch 10/109 | train_loss=1.3784 train_acc=0.0659 train_f1=0.0727 val_loss=4.4777 val_acc=0.0048 val_f1=0.0106 \n",
            "Epoch 20/109 | train_loss=0.5272 train_acc=0.1793 train_f1=0.0873 val_loss=2.5553 val_acc=0.1339 val_f1=0.0358 \n",
            "Epoch 30/109 | train_loss=0.3145 train_acc=0.9199 train_f1=0.3481 val_loss=3.3856 val_acc=0.8274 val_f1=0.1851 \n",
            "Early stopping at epoch 32\n",
            "Epoch 10/106 | train_loss=0.3962 train_acc=0.9029 train_f1=0.3385 val_loss=2.0525 val_acc=0.8371 val_f1=0.1874 \n",
            "Epoch 20/106 | train_loss=0.2298 train_acc=0.8930 train_f1=0.3395 val_loss=2.6046 val_acc=0.8032 val_f1=0.1812 \n",
            "Epoch 30/106 | train_loss=0.1878 train_acc=0.9157 train_f1=0.3714 val_loss=3.1943 val_acc=0.8258 val_f1=0.1853 \n",
            "Early stopping at epoch 39\n",
            "Epoch 13/131 | train_loss=0.3891 train_acc=0.7987 train_f1=0.2930 val_loss=2.8273 val_acc=0.7032 val_f1=0.1626 \n",
            "Epoch 26/131 | train_loss=0.2236 train_acc=0.9355 train_f1=0.3663 val_loss=2.9471 val_acc=0.8500 val_f1=0.1896 \n",
            "Early stopping at epoch 30\n",
            "Epoch 12/120 | train_loss=0.3970 train_acc=0.6853 train_f1=0.2491 val_loss=2.3739 val_acc=0.6000 val_f1=0.1423 \n",
            "Epoch 24/120 | train_loss=0.1908 train_acc=0.9383 train_f1=0.3714 val_loss=2.9508 val_acc=0.8500 val_f1=0.1897 \n",
            "Early stopping at epoch 34\n",
            "Epoch 14/143 | train_loss=0.4587 train_acc=0.9150 train_f1=0.3445 val_loss=2.7851 val_acc=0.8339 val_f1=0.1865 \n",
            "Epoch 28/143 | train_loss=0.2614 train_acc=0.9476 train_f1=0.3764 val_loss=3.1425 val_acc=0.8629 val_f1=0.1920 \n",
            "Early stopping at epoch 31\n",
            "Epoch 10/104 | train_loss=0.6086 train_acc=0.1481 train_f1=0.0779 val_loss=3.4185 val_acc=0.0661 val_f1=0.0180 \n",
            "Epoch 20/104 | train_loss=0.4864 train_acc=0.8561 train_f1=0.3183 val_loss=2.8453 val_acc=0.7823 val_f1=0.1774 \n",
            "Early stopping at epoch 25\n",
            "Epoch 12/128 | train_loss=0.5406 train_acc=0.9178 train_f1=0.3305 val_loss=3.3621 val_acc=0.8452 val_f1=0.1875 \n",
            "Early stopping at epoch 17\n",
            "Early stopping at epoch 17\n",
            "Early stopping at epoch 6\n",
            "[(0.85, 0.18943206326383896, -3.2353885173797607, -1312), (0.8483870967741935, 0.19477075497810123, -3.637916088104248, -1813), (0.8290322580645161, 0.1860296778863554, -3.2799551486968994, -2368), (0.8241935483870968, 0.19023575517257338, -3.5969295501708984, -1628), (0.8403225806451613, 0.188018765788524, -3.8092715740203857, -1946), (0.617741935483871, 0.14546145081655906, -6.4402337074279785, -2183), (0.8580645161290322, 0.19657043208680489, -7.0202460289001465, -3441), (0.8741935483870967, 0.19927892295435334, -6.14325475692749, -1554), (0.8516129032258064, 0.1900647948164147, -4.950568675994873, -2146), (0.8451612903225807, 0.18896501983411468, -2.8774282932281494, -1406), (0.8370967741935483, 0.1872970046914471, -4.563498497009277, -1961), (0.8435483870967742, 0.18867243867243869, -2.4470412731170654, -1520), (0.75, 0.17171344165435748, -5.272684097290039, -2331), (0.5403225806451613, 0.12999611951882034, -11.490469932556152, -2442), (0.8516129032258064, 0.1954912160732021, -4.854316234588623, -2479), (0.8483870967741935, 0.189413035649982, -4.098164081573486, -1184), (0.8403225806451613, 0.19893139928376646, -4.361464023590088, -1832), (0.8290322580645161, 0.19139442709584437, -4.663793087005615, -3182), (0.8596774193548387, 0.1915199425080848, -2.971405267715454, -3233), (0.8612903225806452, 0.19194823867721064, -4.780822277069092, -4544), (0.8661290322580645, 0.1924731182795699, -4.099215984344482, -1739), (0.8483870967741935, 0.19454839534147395, -5.055497646331787, -1924), (0.8709677419354839, 0.19900635913667403, -4.444774150848389, -1850), (0.6193548387096774, 0.1459521094640821, -8.436467170715332, -2664), (0.8548387096774194, 0.19071608492263403, -3.679948091506958, -1665), (0.8064516129032258, 0.18188432157148055, -3.1046929359436035, -4340), (0.020967741935483872, 0.005780346820809248, -3.9265670776367188, -2496), (0.8048387096774193, 0.18152055292833758, -3.418482542037964, -1369), (0.8548387096774194, 0.19051042415528396, -3.4950835704803467, -2432), (0.6112903225806452, 0.14421613394216132, -3.5677330493927, -3998), (0.8435483870967742, 0.18853640951694303, -4.285872459411621, -4383), (0.8483870967741935, 0.18927671824397266, -2.72048282623291, -960), (0.0016129032258064516, 0.0004470272686633884, -4.249207496643066, -4640), (0.853225806451613, 0.195815055488264, -3.085707664489746, -2565), (0.5709677419354838, 0.13625866050808316, -3.7077412605285645, -3980), (0.8596774193548387, 0.19145114942528735, -3.084190845489502, -3744), (0.8451612903225807, 0.18862491000719944, -2.9487950801849365, -3762), (0.8241935483870968, 0.18534639100471526, -3.518130302429199, -1760), (0.8370967741935483, 0.18743228602383533, -3.452918529510498, -7040), (0.8596774193548387, 0.19145114942528735, -3.9138009548187256, -1295), (0.85, 0.1897731364782139, -2.338364362716675, -1158), (0.6919354838709677, 0.1598360655737705, -5.1943840980529785, -1998), (0.35161290322580646, 0.0886899918633035, -3.61043119430542, -1120), (0.6903225806451613, 0.1595229221021245, -4.582722187042236, -1702), (0.85, 0.1898414985590778, -5.423722743988037, -2553), (0.8080645161290323, 0.18757230109614528, -2.9691431522369385, -1221), (0.8516129032258064, 0.18992805755395684, -3.8702847957611084, -6816), (0.785483870967742, 0.17799707602339182, -3.4020469188690186, -680), (0.7935483870967742, 0.1844696386610261, -4.113524436950684, -1628), (0.8225806451612904, 0.19015243443019503, -3.559427261352539, -2432), (0.8419354838709677, 0.18824377930039668, -3.370215892791748, -2912), (0.8145161290322581, 0.1832365747460087, -2.862274169921875, -296), (0.8661290322580645, 0.19813765048559712, -4.531801700592041, -2048), (0.4258064516129032, 0.10534716679968077, -4.286107540130615, -464), (0.8629032258064516, 0.19738693467336682, -4.183628082275391, -1480), (0.8661290322580645, 0.19268030139935416, -2.8409831523895264, -992), (0.6435483870967742, 0.1506797583081571, -2.707226276397705, -971), (0.832258064516129, 0.18682114409847936, -2.7349801063537598, -1912), (0.817741935483871, 0.18416273156556484, -3.8294997215270996, -1443), (0.817741935483871, 0.18396226415094338, -2.6190242767333984, -2920), (0.8419354838709677, 0.18817591925018023, -4.202145576477051, -851), (0.6741935483870968, 0.15673040869891264, -3.044142484664917, -3034), (0.8387096774193549, 0.19325355236540007, -3.264357089996338, -296), (0.5129032258064516, 0.12402496099843993, -2.796387195587158, -1369), (0.01129032258064516, 0.003120820329915292, -4.54872989654541, -2784), (0.8403225806451613, 0.188086642599278, -4.263779640197754, -7680), (0.8387096774193549, 0.18765788523998556, -2.8524160385131836, -1320), (0.8483870967741935, 0.19417154251448202, -5.8093767166137695, -1088), (0.4612903225806452, 0.11317768104471707, -4.1620659828186035, -512), (0.8032258064516129, 0.1812886785584274, -2.9702136516571045, -740), (0.0, 0.0, -3.3154780864715576, -800), (0.0, 0.0, -3.852811813354492, -6592), (0.8225806451612904, 0.18498367791077258, -2.3123905658721924, -1075), (0.8032258064516129, 0.18135469774217045, -2.340097665786743, -840), (0.7725806451612903, 0.1812575861747006, -2.81925892829895, -407), (0.8193548387096774, 0.18405797101449275, -2.591329336166382, -5408), (0.0, 0.0, -3.238466262817383, -6443), (0.8516129032258064, 0.19525453744172788, -4.169626235961914, -1376), (0.8209677419354838, 0.1845540246555475, -2.5571625232696533, -1511), (0.0, 0.0, -2.521364450454712, -5591), (0.8467741935483871, 0.1891891891891892, -3.6889452934265137, -6215), (0.0, 0.0, -2.953731060028076, -1176), (0.4645161290322581, 0.11374407582938388, -3.3346362113952637, -5433), (0.8919354838709678, 0.2132797650794566, -4.358551979064941, -3194), (0.8629032258064516, 0.1920315865039483, -3.6481761932373047, -1764), (0.8354838709677419, 0.18747737965979008, -2.907254934310913, -1543), (0.7983870967741935, 0.18045935107546482, -2.733092784881592, -6947), (0.8161290322580645, 0.18386627906976744, -3.550450325012207, -4160), (0.8516129032258064, 0.1901332373064458, -3.746764659881592, -14090), (0.864516129032258, 0.19225251076040173, -5.314945697784424, -8320), (0.85, 0.18950017979144193, -3.4302897453308105, -9280), (0.8354838709677419, 0.18646508279337654, -3.1148340702056885, -20981), (0.8629032258064516, 0.1920315865039483, -3.764603614807129, -10613), (0.8693548387096774, 0.1930515759312321, -2.8621394634246826, -9948), (0.8580645161290322, 0.1910919540229885, -3.700185775756836, -15392), (0.8580645161290322, 0.1912293314162473, -3.0988881587982178, -9948), (0.8032258064516129, 0.1811567842851946, -2.234790802001953, -5440), (0.6193548387096774, 0.15187078484950825, -6.973092555999756, -21504), (0.31451612903225806, 0.08011503697617092, -3.315765142440796, -24000), (0.6741935483870968, 0.156203288490284, -4.102511405944824, -1504)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graph ouput of multi-objective optimization\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "gx1V7AjRYU_9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = prepare_data(test_dataset, device)\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    predictions = probs.argmax(axis=1)\n",
        "\n",
        "y_true = y_test.cpu().numpy()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, predictions)\n",
        "f_beta = fbeta_score(y_true, predictions, average=\"macro\", beta=2)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F-Beta (macro): {f_beta:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wcGqY8qX1C2R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fddfd4f4-dd28-46c5-e807-e3c0ffd2bfcd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prepare_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3266670381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prepare_data' is not defined"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fe40ec1"
      },
      "source": [
        "## Extract Statistics\n",
        "\n",
        "### Subtask:\n",
        "Extract the 'val_accuracy', 'val_f1', 'val_loss', and 'flops' from each member's statistics in `final_pop_stats`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_pop_stats)"
      ],
      "metadata": {
        "id": "0QfsURnCKM1u",
        "outputId": "b56c310b-4c6b-4bdb-e91b-7cedf10cf579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0.85, 0.18943206326383896, -3.2353885173797607, -1312), (0.8483870967741935, 0.19477075497810123, -3.637916088104248, -1813), (0.8290322580645161, 0.1860296778863554, -3.2799551486968994, -2368), (0.8241935483870968, 0.19023575517257338, -3.5969295501708984, -1628), (0.8403225806451613, 0.188018765788524, -3.8092715740203857, -1946), (0.617741935483871, 0.14546145081655906, -6.4402337074279785, -2183), (0.8580645161290322, 0.19657043208680489, -7.0202460289001465, -3441), (0.8741935483870967, 0.19927892295435334, -6.14325475692749, -1554), (0.8516129032258064, 0.1900647948164147, -4.950568675994873, -2146), (0.8451612903225807, 0.18896501983411468, -2.8774282932281494, -1406), (0.8370967741935483, 0.1872970046914471, -4.563498497009277, -1961), (0.8435483870967742, 0.18867243867243869, -2.4470412731170654, -1520), (0.75, 0.17171344165435748, -5.272684097290039, -2331), (0.5403225806451613, 0.12999611951882034, -11.490469932556152, -2442), (0.8516129032258064, 0.1954912160732021, -4.854316234588623, -2479), (0.8483870967741935, 0.189413035649982, -4.098164081573486, -1184), (0.8403225806451613, 0.19893139928376646, -4.361464023590088, -1832), (0.8290322580645161, 0.19139442709584437, -4.663793087005615, -3182), (0.8596774193548387, 0.1915199425080848, -2.971405267715454, -3233), (0.8612903225806452, 0.19194823867721064, -4.780822277069092, -4544), (0.8661290322580645, 0.1924731182795699, -4.099215984344482, -1739), (0.8483870967741935, 0.19454839534147395, -5.055497646331787, -1924), (0.8709677419354839, 0.19900635913667403, -4.444774150848389, -1850), (0.6193548387096774, 0.1459521094640821, -8.436467170715332, -2664), (0.8548387096774194, 0.19071608492263403, -3.679948091506958, -1665), (0.8064516129032258, 0.18188432157148055, -3.1046929359436035, -4340), (0.020967741935483872, 0.005780346820809248, -3.9265670776367188, -2496), (0.8048387096774193, 0.18152055292833758, -3.418482542037964, -1369), (0.8548387096774194, 0.19051042415528396, -3.4950835704803467, -2432), (0.6112903225806452, 0.14421613394216132, -3.5677330493927, -3998), (0.8435483870967742, 0.18853640951694303, -4.285872459411621, -4383), (0.8483870967741935, 0.18927671824397266, -2.72048282623291, -960), (0.0016129032258064516, 0.0004470272686633884, -4.249207496643066, -4640), (0.853225806451613, 0.195815055488264, -3.085707664489746, -2565), (0.5709677419354838, 0.13625866050808316, -3.7077412605285645, -3980), (0.8596774193548387, 0.19145114942528735, -3.084190845489502, -3744), (0.8451612903225807, 0.18862491000719944, -2.9487950801849365, -3762), (0.8241935483870968, 0.18534639100471526, -3.518130302429199, -1760), (0.8370967741935483, 0.18743228602383533, -3.452918529510498, -7040), (0.8596774193548387, 0.19145114942528735, -3.9138009548187256, -1295), (0.85, 0.1897731364782139, -2.338364362716675, -1158), (0.6919354838709677, 0.1598360655737705, -5.1943840980529785, -1998), (0.35161290322580646, 0.0886899918633035, -3.61043119430542, -1120), (0.6903225806451613, 0.1595229221021245, -4.582722187042236, -1702), (0.85, 0.1898414985590778, -5.423722743988037, -2553), (0.8080645161290323, 0.18757230109614528, -2.9691431522369385, -1221), (0.8516129032258064, 0.18992805755395684, -3.8702847957611084, -6816), (0.785483870967742, 0.17799707602339182, -3.4020469188690186, -680), (0.7935483870967742, 0.1844696386610261, -4.113524436950684, -1628), (0.8225806451612904, 0.19015243443019503, -3.559427261352539, -2432), (0.8419354838709677, 0.18824377930039668, -3.370215892791748, -2912), (0.8145161290322581, 0.1832365747460087, -2.862274169921875, -296), (0.8661290322580645, 0.19813765048559712, -4.531801700592041, -2048), (0.4258064516129032, 0.10534716679968077, -4.286107540130615, -464), (0.8629032258064516, 0.19738693467336682, -4.183628082275391, -1480), (0.8661290322580645, 0.19268030139935416, -2.8409831523895264, -992), (0.6435483870967742, 0.1506797583081571, -2.707226276397705, -971), (0.832258064516129, 0.18682114409847936, -2.7349801063537598, -1912), (0.817741935483871, 0.18416273156556484, -3.8294997215270996, -1443), (0.817741935483871, 0.18396226415094338, -2.6190242767333984, -2920), (0.8419354838709677, 0.18817591925018023, -4.202145576477051, -851), (0.6741935483870968, 0.15673040869891264, -3.044142484664917, -3034), (0.8387096774193549, 0.19325355236540007, -3.264357089996338, -296), (0.5129032258064516, 0.12402496099843993, -2.796387195587158, -1369), (0.01129032258064516, 0.003120820329915292, -4.54872989654541, -2784), (0.8403225806451613, 0.188086642599278, -4.263779640197754, -7680), (0.8387096774193549, 0.18765788523998556, -2.8524160385131836, -1320), (0.8483870967741935, 0.19417154251448202, -5.8093767166137695, -1088), (0.4612903225806452, 0.11317768104471707, -4.1620659828186035, -512), (0.8032258064516129, 0.1812886785584274, -2.9702136516571045, -740), (0.0, 0.0, -3.3154780864715576, -800), (0.0, 0.0, -3.852811813354492, -6592), (0.8225806451612904, 0.18498367791077258, -2.3123905658721924, -1075), (0.8032258064516129, 0.18135469774217045, -2.340097665786743, -840), (0.7725806451612903, 0.1812575861747006, -2.81925892829895, -407), (0.8193548387096774, 0.18405797101449275, -2.591329336166382, -5408), (0.0, 0.0, -3.238466262817383, -6443), (0.8516129032258064, 0.19525453744172788, -4.169626235961914, -1376), (0.8209677419354838, 0.1845540246555475, -2.5571625232696533, -1511), (0.0, 0.0, -2.521364450454712, -5591), (0.8467741935483871, 0.1891891891891892, -3.6889452934265137, -6215), (0.0, 0.0, -2.953731060028076, -1176), (0.4645161290322581, 0.11374407582938388, -3.3346362113952637, -5433), (0.8919354838709678, 0.2132797650794566, -4.358551979064941, -3194), (0.8629032258064516, 0.1920315865039483, -3.6481761932373047, -1764), (0.8354838709677419, 0.18747737965979008, -2.907254934310913, -1543), (0.7983870967741935, 0.18045935107546482, -2.733092784881592, -6947), (0.8161290322580645, 0.18386627906976744, -3.550450325012207, -4160), (0.8516129032258064, 0.1901332373064458, -3.746764659881592, -14090), (0.864516129032258, 0.19225251076040173, -5.314945697784424, -8320), (0.85, 0.18950017979144193, -3.4302897453308105, -9280), (0.8354838709677419, 0.18646508279337654, -3.1148340702056885, -20981), (0.8629032258064516, 0.1920315865039483, -3.764603614807129, -10613), (0.8693548387096774, 0.1930515759312321, -2.8621394634246826, -9948), (0.8580645161290322, 0.1910919540229885, -3.700185775756836, -15392), (0.8580645161290322, 0.1912293314162473, -3.0988881587982178, -9948), (0.8032258064516129, 0.1811567842851946, -2.234790802001953, -5440), (0.6193548387096774, 0.15187078484950825, -6.973092555999756, -21504), (0.31451612903225806, 0.08011503697617092, -3.315765142440796, -24000), (0.6741935483870968, 0.156203288490284, -4.102511405944824, -1504)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2e9620aa",
        "outputId": "1b69185c-46bf-48c0-f714-e4e8dfab429f"
      },
      "source": [
        "accuracies = []\n",
        "f1_scores = []\n",
        "losses = []\n",
        "flops_values = []\n",
        "\n",
        "for member_stats in final_pop_stats:\n",
        "    #if member_stats['val_accuracy'] > 0.1:\n",
        "    accuracies.append(member_stats['val_accuracy'])\n",
        "    f1_scores.append(member_stats['val_f1'])\n",
        "    losses.append(member_stats['val_loss'])\n",
        "    flops_values.append(member_stats['flops'])\n",
        "\n",
        "print(\"Extracted accuracies:\", accuracies)\n",
        "print(\"Extracted F1 scores:\", f1_scores)\n",
        "print(\"Extracted losses:\", losses)\n",
        "print(\"Extracted FLOPs:\", flops_values)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2694269211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmember_stats\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_pop_stats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#if member_stats['val_accuracy'] > 0.1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072ca572"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Create a figure and a 3D axes object\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Create the 3D scatter plot\n",
        "scatter = ax.scatter(accuracies, f1_scores, losses, c=flops_values, cmap='viridis', s=100, alpha=0.8)\n",
        "\n",
        "# Label the axes\n",
        "ax.set_xlabel('Validation Accuracy')\n",
        "ax.set_ylabel('Validation F1 Score')\n",
        "ax.set_zlabel('Validation Loss')\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Hyperparameter Performance in 3D Space (FLOPs as Color)')\n",
        "\n",
        "# Add a color bar\n",
        "cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
        "cbar.set_label('FLOPs')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20098052"
      },
      "source": [
        "# Task\n",
        "Prepare fitness data from `final_pop_stats` for Pareto dominance checking, considering accuracy, F1-score, negative loss, and negative FLOPs as objectives. Identify the Pareto front by determining which solutions are non-dominated. Extract the validation accuracy, F1-score, and loss values for these Pareto front solutions. Then, modify the existing 3D scatter plot in cell `072ca572` to visualize these Pareto front solutions with a distinct color and line connections, updating the plot title and legends as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4269c555"
      },
      "source": [
        "## Prepare Fitness Data for Pareto Front\n",
        "\n",
        "### Subtask:\n",
        "Convert the `final_pop_stats` (which are dictionaries) into a list of tuples, where each tuple represents the fitness objectives in a 'greater is better' format. This will include accuracy, F1-score, negative loss, and negative FLOPs, as expected for Pareto dominance checking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637096aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires converting the dictionary-based fitness statistics into a list of tuples, applying the 'greater is better' principle by negating loss and FLOPs. This code block will perform the requested conversion and store the result in `fitness_data`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01c16501"
      },
      "source": [
        "fitness_data = []\n",
        "\n",
        "for member_stats in final_pop_stats:\n",
        "    accuracy = member_stats['val_accuracy']\n",
        "    f1_score = member_stats['val_f1']\n",
        "    loss = member_stats['val_loss']\n",
        "    flops = member_stats['flops']\n",
        "\n",
        "    # Negate loss and flops for 'greater is better' comparison\n",
        "    if accuracy > 0.1:\n",
        "      fitness_data.append((accuracy, f1_score, -loss, -flops))\n",
        "\n",
        "print(\"Prepared fitness data for Pareto front (accuracy, f1_score, -loss, -flops):\")\n",
        "for item in fitness_data:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "231017c0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the fitness data is prepared, the next step is to implement the Pareto dominance check function to identify the non-dominated solutions. This function will be crucial for determining the Pareto front.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3aaabd0"
      },
      "source": [
        "def is_dominated(p1, p2):\n",
        "    \"\"\"\n",
        "    Checks if point p1 is dominated by point p2.\n",
        "    A point p1 is dominated by p2 if p2 is better than or equal to p1 in all objectives,\n",
        "    and strictly better in at least one objective.\n",
        "    Assumes 'greater is better' for all objectives.\n",
        "    \"\"\"\n",
        "    # Check if p2 is strictly better in at least one objective\n",
        "    strictly_better = False\n",
        "    for i in range(len(p1)):\n",
        "        if p2[i] > p1[i]:\n",
        "            strictly_better = True\n",
        "        elif p2[i] < p1[i]: # p2 is worse than p1 in this objective, so p1 is not dominated by p2\n",
        "            return False\n",
        "    return strictly_better\n",
        "\n",
        "def find_pareto_front(points):\n",
        "    \"\"\"\n",
        "    Finds the Pareto front from a list of points (solutions).\n",
        "    \"\"\"\n",
        "    pareto_front = []\n",
        "    for i, p1 in enumerate(points):\n",
        "        is_p1_dominated = False\n",
        "        for j, p2 in enumerate(points):\n",
        "            if i == j:\n",
        "                continue\n",
        "            if is_dominated(p1, p2):\n",
        "                is_p1_dominated = True\n",
        "                break\n",
        "        if not is_p1_dominated:\n",
        "            pareto_front.append(p1)\n",
        "    return pareto_front\n",
        "\n",
        "# Find the Pareto front using the prepared fitness data\n",
        "pareto_solutions = find_pareto_front(fitness_data)\n",
        "\n",
        "print(\"Pareto Front Solutions:\")\n",
        "for solution in pareto_solutions:\n",
        "    print(solution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bbcbdca"
      },
      "source": [
        "**Reasoning**:\n",
        "Extract the accuracy, F1-score, and positive loss values from each Pareto front solution to prepare them for plotting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8076d550"
      },
      "source": [
        "pareto_accuracies = []\n",
        "pareto_f1_scores = []\n",
        "pareto_losses = []\n",
        "\n",
        "for solution in pareto_solutions:\n",
        "    # solution is (accuracy, f1_score, -loss, -flops)\n",
        "    pareto_accuracies.append(solution[0])\n",
        "    pareto_f1_scores.append(solution[1])\n",
        "    pareto_losses.append(-solution[2]) # Negate back to positive loss\n",
        "\n",
        "print(\"Pareto Front Accuracies:\", pareto_accuracies)\n",
        "print(\"Pareto Front F1 Scores:\", pareto_f1_scores)\n",
        "print(\"Pareto Front Losses:\", pareto_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17b6c827"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot all initial solutions as before\n",
        "scatter = ax.scatter(accuracies, f1_scores, losses, c=flops_values, cmap='viridis', s=100, alpha=0.6, label='All Solutions')\n",
        "\n",
        "# Create a triangulated surface for the Pareto front\n",
        "# plot_trisurf can handle unsorted points and triangulate them\n",
        "ax.plot_trisurf(pareto_accuracies, pareto_f1_scores, pareto_losses, color='orange', alpha=0.5, label='Pareto Front Surface')\n",
        "\n",
        "# Highlight the individual Pareto front solutions with distinct scatter points\n",
        "ax.scatter(pareto_accuracies, pareto_f1_scores, pareto_losses, color='black', s=200, marker='o', edgecolors='black', label='Pareto Front Solutions', zorder=10)\n",
        "\n",
        "# Label the axes\n",
        "ax.set_xlabel('Validation Accuracy')\n",
        "ax.set_ylabel('Validation F1 Score')\n",
        "ax.set_zlabel('Validation Loss')\n",
        "\n",
        "# Set the title of the plot\n",
        "ax.set_title('Hyperparameter Performance (FLOPs as Color) with Pareto Front Surface')\n",
        "\n",
        "# Add a color bar for the scatter plot (all solutions) and set its label to 'FLOPs'\n",
        "cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
        "cbar.set_label('FLOPs')\n",
        "\n",
        "# Add legend to distinguish all solutions from Pareto front solutions/surface\n",
        "# The label from plot_trisurf might not appear in legend directly, so we explicitly add 'Pareto Front Solutions'\n",
        "# For a true surface legend, a custom legend handle might be needed, but scatter for points is sufficient as requested.\n",
        "ax.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}