{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 117287,
          "databundleVersionId": 14018857,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "nathaniel14133437_midterm_CS5173",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grillinr/evolutionary-computing/blob/main/final/final_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and seed for easier checking"
      ],
      "metadata": {
        "id": "uibqKYOH1C2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import argparse\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import accuracy_score, fbeta_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "SEED = 5173\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n",
        "print(device)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:17.559153Z",
          "iopub.execute_input": "2025-10-17T00:35:17.559461Z",
          "iopub.status.idle": "2025-10-17T00:35:17.568112Z",
          "shell.execute_reply.started": "2025-10-17T00:35:17.559437Z",
          "shell.execute_reply": "2025-10-17T00:35:17.567065Z"
        },
        "id": "rU8AhTMY1C2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c65c3b3-754b-435f-f56b-95872cdb24bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define helper functions"
      ],
      "metadata": {
        "id": "i8xd5F721C2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_with_scaler(data, scaler=None, fit=False):\n",
        "    data = data.dropna()\n",
        "    X = data.drop(columns=[\"id\", \"record\", \"type\"]).values.astype(np.float32)\n",
        "    y = data[\"type\"].astype(\"category\").cat.codes.values\n",
        "\n",
        "    if fit:\n",
        "        X = scaler.fit_transform(X)\n",
        "    else:\n",
        "        X = scaler.transform(X)\n",
        "\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "def evaluate(model, X, y, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, y)\n",
        "        y_pred = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    y_true = y.cpu().numpy()\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"f_beta_macro\": fbeta_score(y_true, y_pred, average=\"macro\", beta=2, zero_division=0)\n",
        "    }\n",
        "\n",
        "\n",
        "def estimate_flops(model, input_shape):\n",
        "    \"\"\"\n",
        "    Estimate FLOPs for Linear and Conv2d layers only.\n",
        "    Args:\n",
        "        model (nn.Module): PyTorch model\n",
        "        input_shape (tuple): shape of one input sample, e.g., (1, 3, 224, 224) or (1, input_dim)\n",
        "    Returns:\n",
        "        total_flops (int)\n",
        "    \"\"\"\n",
        "    flops = 0\n",
        "\n",
        "    def count_layer(layer, x_in, x_out):\n",
        "        nonlocal flops\n",
        "        # Conv2d FLOPs = Kx * Ky * Cin * Cout * Hout * Wout\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            out_h, out_w = x_out.shape[2:]\n",
        "            kernel_ops = layer.kernel_size[0] * layer.kernel_size[1]\n",
        "            flops += kernel_ops * layer.in_channels * layer.out_channels * out_h * out_w\n",
        "        # Linear FLOPs = input_features * output_features\n",
        "        elif isinstance(layer, nn.Linear):\n",
        "            flops += layer.in_features * layer.out_features\n",
        "\n",
        "    hooks = []\n",
        "    for layer in model.modules():\n",
        "        if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "            hooks.append(layer.register_forward_hook(count_layer))\n",
        "\n",
        "    dummy = torch.randn(input_shape).to(next(model.parameters()).device)\n",
        "    with torch.no_grad():\n",
        "        model(dummy)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return flops"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:46:03.730208Z",
          "iopub.execute_input": "2025-10-17T00:46:03.730536Z",
          "iopub.status.idle": "2025-10-17T00:46:03.738421Z",
          "shell.execute_reply.started": "2025-10-17T00:46:03.730511Z",
          "shell.execute_reply": "2025-10-17T00:46:03.73737Z"
        },
        "id": "ZxxTIRWL1C2O"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model Architecture (DNN)"
      ],
      "metadata": {
        "id": "dTi3Q6241C2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size=32, hidden=(32, 16, 8), num_classes=5, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        input_dim = input_size\n",
        "\n",
        "        for h in hidden:\n",
        "            layers.append(nn.Linear(input_dim, h))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "            input_dim = h\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:38.736024Z",
          "iopub.execute_input": "2025-10-17T00:35:38.736378Z",
          "iopub.status.idle": "2025-10-17T00:35:38.74276Z",
          "shell.execute_reply.started": "2025-10-17T00:35:38.736352Z",
          "shell.execute_reply": "2025-10-17T00:35:38.741836Z"
        },
        "id": "qzOM0laS1C2P"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Training loop"
      ],
      "metadata": {
        "id": "8V1SVoBd1C2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "dataset = pd.read_csv(\"/content/train.csv\")\n",
        "train_dataset, val_dataset = train_test_split(dataset, train_size=0.7, random_state=SEED)\n",
        "scaler = StandardScaler()\n",
        "X_train, y_train = prepare_data_with_scaler(train_dataset, scaler, fit=True)\n",
        "X_val, y_val = prepare_data_with_scaler(val_dataset, scaler, fit=False)\n",
        "\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_val, y_val = X_val.to(device), y_val.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:46:12.869203Z",
          "iopub.execute_input": "2025-10-17T00:46:12.869487Z",
          "iopub.status.idle": "2025-10-17T00:46:18.983013Z",
          "shell.execute_reply.started": "2025-10-17T00:46:12.869467Z",
          "shell.execute_reply": "2025-10-17T00:46:18.982193Z"
        },
        "id": "Dv6o3n1t1C2Q"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "class Hyperparameters:\n",
        "    def __init__(self, lr, epochs, hidden, dropout_rate, patience):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.hidden = hidden\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.patience = patience\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Hyperparameters(lr={self.lr}, epochs={self.epochs}, hidden={self.hidden}, dropout_rate={self.dropout_rate}, patience={self.patience})\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:35:56.034543Z",
          "iopub.execute_input": "2025-10-17T00:35:56.034924Z",
          "iopub.status.idle": "2025-10-17T00:35:56.042659Z",
          "shell.execute_reply.started": "2025-10-17T00:35:56.034863Z",
          "shell.execute_reply": "2025-10-17T00:35:56.041417Z"
        },
        "id": "R3uFXDIH1C2Q"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "def train(params: Hyperparameters):\n",
        "  # Create model\n",
        "  model = DNN(hidden=params.hidden, dropout_rate=params.dropout_rate).to(device)\n",
        "\n",
        "  class_counts = train_dataset['type'].value_counts()\n",
        "  weights = 1.0 / class_counts.values\n",
        "  weights = torch.FloatTensor(weights).to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
        "\n",
        "  # Training loop with early stopping\n",
        "  best_val_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "  epochs_run = params.epochs\n",
        "  for epoch in range(1, params.epochs + 1):\n",
        "      model.train()\n",
        "      optimizer.zero_grad()\n",
        "      out = model(X_train)\n",
        "      loss = criterion(out, y_train)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss = loss.item()\n",
        "\n",
        "      train_metrics = evaluate(model, X_train, y_train, criterion)\n",
        "      val_metrics = evaluate(model, X_val, y_val, criterion)\n",
        "\n",
        "      if epoch % (params.epochs // 10) == 0:\n",
        "        print(\n",
        "            f\"Epoch {epoch}/{params.epochs} | \"\n",
        "            f\"train_loss={train_loss:.4f} train_acc={train_metrics['accuracy']:.4f} \"\n",
        "            f\"train_f1={train_metrics['f_beta_macro']:.4f} \"\n",
        "            f\"val_loss={val_metrics['loss']:.4f} val_acc={val_metrics['accuracy']:.4f} \"\n",
        "            f\"val_f1={val_metrics['f_beta_macro']:.4f} \"\n",
        "        )\n",
        "\n",
        "      # Early stopping check\n",
        "      if val_metrics['loss'] < best_val_loss:\n",
        "          best_val_loss = val_metrics['loss']\n",
        "          patience_counter = 0\n",
        "      else:\n",
        "          patience_counter += 1\n",
        "          if patience_counter >= params.patience:\n",
        "              print(f\"Early stopping at epoch {epoch}\")\n",
        "              epochs_run = epoch\n",
        "              break\n",
        "\n",
        "  # Return a tuple of fitness values for domination comparison\n",
        "  # Maximize accuracy, f1, and minimize loss, flops.\n",
        "  # For loss and flops, we take the negative value.\n",
        "  return (val_metrics[\"accuracy\"], val_metrics[\"f_beta_macro\"], -val_metrics[\"loss\"], -estimate_flops(model, (1, 32)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-17T00:43:03.337535Z",
          "iopub.execute_input": "2025-10-17T00:43:03.337852Z",
          "iopub.status.idle": "2025-10-17T00:43:03.676338Z",
          "shell.execute_reply.started": "2025-10-17T00:43:03.337828Z",
          "shell.execute_reply": "2025-10-17T00:43:03.675024Z"
        },
        "id": "pEO7m2r11C2R"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "hyperparameters = Hyperparameters(lr=1e-3, epochs=500, hidden=(64, 32, 16, 8), dropout_rate=0.5, patience=100)\n",
        "# result = train(hyperparameters)"
      ],
      "metadata": {
        "id": "fUhJqaHLMxKK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuroevolution"
      ],
      "metadata": {
        "id": "3_uGubzQTmqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_population(pop_size: int) -> List[Hyperparameters]:\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        lr = random.uniform(1e-5, 1e-1)\n",
        "        epochs = random.randint(10, 200)\n",
        "\n",
        "        # Generate variable-length hidden layer tuple\n",
        "        num_layers = random.randint(1, 5)\n",
        "        hidden = tuple(2 ** random.randint(3, 8) for _ in range(num_layers))\n",
        "\n",
        "        dropout_rate = random.uniform(0.0, 0.5)\n",
        "        patience = random.randint(5, 30)\n",
        "\n",
        "        population.append(Hyperparameters(lr, epochs, hidden, dropout_rate, patience))\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "J3EDsF-zX0b1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_dominated(fitnesses: List[Tuple[float, ...]], idx: int) -> int:\n",
        "    \"\"\"Count how many points are dominated by fitnesses[idx]\"\"\"\n",
        "    point = fitnesses[idx]\n",
        "    dominated = 0\n",
        "    for other in fitnesses:\n",
        "        if other == point:\n",
        "            continue\n",
        "        # Check if point dominates other (all >= and at least one >)\n",
        "        if all(p >= o for p, o in zip(point, other)) and any(p > o for p, o in zip(point, other)):\n",
        "            dominated += 1\n",
        "    return dominated"
      ],
      "metadata": {
        "id": "JFWFExuUdf6H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Evolution Strategy to Optimize Hyperparameters"
      ],
      "metadata": {
        "id": "b80BDEmPX5Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evolution_strategy(mu: int, lambda_: int, tau: float, max_gens: int) -> List[Hyperparameters]:\n",
        "    population = init_population(mu)\n",
        "\n",
        "    for generation_number in range(1, max_gens + 1):\n",
        "        print(f\"Generation {generation_number} starting initial evaluation\")\n",
        "        fitnesses = []\n",
        "        for i, member in enumerate(population):\n",
        "            print(f\"Evaluating member {i}: {member}\")\n",
        "            fitnesses.append(train(member))\n",
        "\n",
        "        # fitnesses = [train(member) for member in population]\n",
        "\n",
        "        # Calculate proportion of dominated points for each individual\n",
        "        domination_counts = [count_dominated(fitnesses, i) for i in range(mu)]\n",
        "        domination_proportions = [count / mu for count in domination_counts]\n",
        "\n",
        "        offspring = []\n",
        "        for _ in range(lambda_):\n",
        "            # Select parent using tournament based on domination proportion\n",
        "            candidates = random.sample(range(mu), 2)\n",
        "            parent_idx = max(candidates, key=lambda i: domination_proportions[i])\n",
        "            parent = population[parent_idx]\n",
        "\n",
        "            # Mutate hyperparameters\n",
        "            lr = parent.lr * math.exp(tau * random.gauss(0.0, 1.0))\n",
        "            lr = max(1e-5, min(1e-1, lr))\n",
        "\n",
        "            epochs = int(parent.epochs + random.gauss(0.0, 10))\n",
        "            epochs = max(10, min(500, epochs))\n",
        "\n",
        "            # Mutate hidden layers\n",
        "            hidden = list(parent.hidden)\n",
        "            if random.random() < 0.3:\n",
        "                if len(hidden) > 1 and random.random() < 0.5:\n",
        "                    hidden.pop(random.randrange(len(hidden)))\n",
        "                elif len(hidden) < 5:\n",
        "                    hidden.insert(random.randrange(len(hidden) + 1), 2 ** random.randint(3, 8))\n",
        "            else:\n",
        "                idx = random.randrange(len(hidden))\n",
        "                hidden[idx] = max(8, min(256, int(hidden[idx] + random.gauss(0.0, 16))))\n",
        "\n",
        "            dropout_rate = parent.dropout_rate + random.gauss(0.0, 0.05)\n",
        "            dropout_rate = max(0.0, min(0.5, dropout_rate))\n",
        "\n",
        "            patience = int(parent.patience + random.gauss(0.0, 3))\n",
        "            patience = max(5, min(30, patience))\n",
        "\n",
        "            offspring.append(Hyperparameters(lr, epochs, tuple(hidden), dropout_rate, patience))\n",
        "\n",
        "        offspring_fitnesses = [train(member) for member in offspring]\n",
        "\n",
        "        # Calculate domination for offspring\n",
        "        offspring_domination_counts = [count_dominated(offspring_fitnesses, i) for i in range(lambda_)]\n",
        "        offspring_domination_proportions = [count / lambda_ for count in offspring_domination_counts]\n",
        "\n",
        "        # Logging\n",
        "        best_idx = max(range(mu), key=lambda i: domination_proportions[i])\n",
        "        print(f\"Gen {generation_number} Best: {fitnesses[best_idx]}\")\n",
        "\n",
        "        # Select best member from offspring based on domination\n",
        "        indexed = [(prop, i) for i, prop in enumerate(offspring_domination_proportions)]\n",
        "        indexed.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        population = [offspring[i] for _, i in indexed[:mu]]\n",
        "\n",
        "    return population"
      ],
      "metadata": {
        "id": "-_jNJVH6TmSq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pop = evolution_strategy(mu=10, lambda_=10, tau=0.05, max_gens=10)\n",
        "for member in final_pop:\n",
        "    print(member)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_WgpIjQeQ4n",
        "outputId": "cf5eab01-0c55-4585-ae7a-dacb32bd85cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 1 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.04779887398429091, epochs=148, hidden=(64, 32, 64, 256), dropout_rate=0.19037090294267395, patience=14)\n",
            "Epoch 14/148 | train_loss=0.7306 train_acc=0.0676 train_f1=0.0532 val_loss=0.6898 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/148 | train_loss=0.5724 train_acc=0.0676 train_f1=0.0532 val_loss=0.4762 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 42/148 | train_loss=0.5017 train_acc=0.8950 train_f1=0.3388 val_loss=0.3891 val_acc=0.8961 val_f1=0.3393 \n",
            "Epoch 56/148 | train_loss=0.4836 train_acc=0.9032 train_f1=0.3433 val_loss=0.3681 val_acc=0.9046 val_f1=0.3438 \n",
            "Epoch 70/148 | train_loss=0.4701 train_acc=0.9100 train_f1=0.3474 val_loss=0.3516 val_acc=0.9112 val_f1=0.3479 \n",
            "Epoch 84/148 | train_loss=0.4677 train_acc=0.9049 train_f1=0.3442 val_loss=0.3418 val_acc=0.9065 val_f1=0.3451 \n",
            "Epoch 98/148 | train_loss=0.4609 train_acc=0.9121 train_f1=0.3480 val_loss=0.3311 val_acc=0.9133 val_f1=0.3486 \n",
            "Epoch 112/148 | train_loss=0.4492 train_acc=0.9148 train_f1=0.3493 val_loss=0.3219 val_acc=0.9162 val_f1=0.3500 \n",
            "Epoch 126/148 | train_loss=0.4405 train_acc=0.9257 train_f1=0.3920 val_loss=0.3104 val_acc=0.9268 val_f1=0.3935 \n",
            "Epoch 140/148 | train_loss=0.4407 train_acc=0.9332 train_f1=0.4528 val_loss=0.3014 val_acc=0.9333 val_f1=0.4513 \n",
            "Evaluating member 1: Hyperparameters(lr=0.0588034891954532, epochs=175, hidden=(16, 8), dropout_rate=0.28906020509010083, patience=24)\n",
            "Epoch 17/175 | train_loss=0.4490 train_acc=0.7788 train_f1=0.2843 val_loss=0.3657 val_acc=0.7773 val_f1=0.2836 \n",
            "Epoch 34/175 | train_loss=0.3562 train_acc=0.8971 train_f1=0.3406 val_loss=0.3009 val_acc=0.8968 val_f1=0.3402 \n",
            "Epoch 51/175 | train_loss=0.3147 train_acc=0.9213 train_f1=0.3544 val_loss=0.2658 val_acc=0.9217 val_f1=0.3544 \n",
            "Epoch 68/175 | train_loss=0.2935 train_acc=0.9235 train_f1=0.3557 val_loss=0.2447 val_acc=0.9235 val_f1=0.3555 \n",
            "Epoch 85/175 | train_loss=0.2824 train_acc=0.9268 train_f1=0.3576 val_loss=0.2315 val_acc=0.9271 val_f1=0.3575 \n",
            "Epoch 102/175 | train_loss=0.2751 train_acc=0.9284 train_f1=0.3639 val_loss=0.2254 val_acc=0.9284 val_f1=0.3631 \n",
            "Epoch 119/175 | train_loss=0.2703 train_acc=0.9300 train_f1=0.3776 val_loss=0.2186 val_acc=0.9299 val_f1=0.3763 \n",
            "Epoch 136/175 | train_loss=0.2668 train_acc=0.9332 train_f1=0.3876 val_loss=0.2137 val_acc=0.9327 val_f1=0.3854 \n",
            "Epoch 153/175 | train_loss=0.2633 train_acc=0.9336 train_f1=0.3895 val_loss=0.2100 val_acc=0.9330 val_f1=0.3895 \n",
            "Epoch 170/175 | train_loss=0.2620 train_acc=0.9374 train_f1=0.4022 val_loss=0.2059 val_acc=0.9368 val_f1=0.4012 \n",
            "Evaluating member 2: Hyperparameters(lr=0.05135898587606571, epochs=57, hidden=(256, 16, 128, 32), dropout_rate=0.12171133774361031, patience=12)\n",
            "Epoch 5/57 | train_loss=1.2687 train_acc=0.9044 train_f1=0.1959 val_loss=1.1734 val_acc=0.9043 val_f1=0.1959 \n",
            "Epoch 10/57 | train_loss=0.9803 train_acc=0.0676 train_f1=0.0532 val_loss=0.8642 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 15/57 | train_loss=0.7603 train_acc=0.0676 train_f1=0.0532 val_loss=0.7743 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 20/57 | train_loss=0.7624 train_acc=0.0676 train_f1=0.0532 val_loss=0.7375 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 25/57 | train_loss=0.7454 train_acc=0.0676 train_f1=0.0532 val_loss=0.7410 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 25\n",
            "Evaluating member 3: Hyperparameters(lr=0.004136107327582931, epochs=179, hidden=(128, 128, 32), dropout_rate=0.2208557735129601, patience=26)\n",
            "Epoch 17/179 | train_loss=0.5606 train_acc=0.0676 train_f1=0.0532 val_loss=0.5427 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/179 | train_loss=0.4032 train_acc=0.7632 train_f1=0.2781 val_loss=0.3689 val_acc=0.7637 val_f1=0.2786 \n",
            "Epoch 51/179 | train_loss=0.2951 train_acc=0.9053 train_f1=0.3456 val_loss=0.2676 val_acc=0.9066 val_f1=0.3461 \n",
            "Epoch 68/179 | train_loss=0.2519 train_acc=0.9237 train_f1=0.3719 val_loss=0.2288 val_acc=0.9242 val_f1=0.3734 \n",
            "Epoch 85/179 | train_loss=0.2199 train_acc=0.9308 train_f1=0.4298 val_loss=0.1990 val_acc=0.9309 val_f1=0.4288 \n",
            "Epoch 102/179 | train_loss=0.1949 train_acc=0.9396 train_f1=0.4629 val_loss=0.1746 val_acc=0.9395 val_f1=0.4613 \n",
            "Epoch 119/179 | train_loss=0.1751 train_acc=0.9456 train_f1=0.4825 val_loss=0.1586 val_acc=0.9451 val_f1=0.4803 \n",
            "Epoch 136/179 | train_loss=0.1620 train_acc=0.9490 train_f1=0.4919 val_loss=0.1475 val_acc=0.9486 val_f1=0.4881 \n",
            "Epoch 153/179 | train_loss=0.1536 train_acc=0.9501 train_f1=0.4986 val_loss=0.1387 val_acc=0.9494 val_f1=0.4939 \n",
            "Epoch 170/179 | train_loss=0.1463 train_acc=0.9509 train_f1=0.5062 val_loss=0.1312 val_acc=0.9502 val_f1=0.5017 \n",
            "Evaluating member 4: Hyperparameters(lr=0.035626794442163115, epochs=74, hidden=(256, 32), dropout_rate=0.451058741287829, patience=7)\n",
            "Epoch 7/74 | train_loss=0.5455 train_acc=0.6143 train_f1=0.2239 val_loss=0.4284 val_acc=0.6151 val_f1=0.2245 \n",
            "Epoch 14/74 | train_loss=0.3948 train_acc=0.8288 train_f1=0.3072 val_loss=0.3315 val_acc=0.8288 val_f1=0.3071 \n",
            "Epoch 21/74 | train_loss=0.3409 train_acc=0.8859 train_f1=0.3347 val_loss=0.2934 val_acc=0.8863 val_f1=0.3347 \n",
            "Epoch 28/74 | train_loss=0.3140 train_acc=0.9120 train_f1=0.3496 val_loss=0.2656 val_acc=0.9122 val_f1=0.3494 \n",
            "Epoch 35/74 | train_loss=0.2892 train_acc=0.9254 train_f1=0.3619 val_loss=0.2417 val_acc=0.9259 val_f1=0.3626 \n",
            "Epoch 42/74 | train_loss=0.2683 train_acc=0.9252 train_f1=0.3668 val_loss=0.2231 val_acc=0.9250 val_f1=0.3656 \n",
            "Epoch 49/74 | train_loss=0.2522 train_acc=0.9318 train_f1=0.3957 val_loss=0.2024 val_acc=0.9314 val_f1=0.3952 \n",
            "Epoch 56/74 | train_loss=0.2433 train_acc=0.9391 train_f1=0.4157 val_loss=0.1881 val_acc=0.9381 val_f1=0.4136 \n",
            "Epoch 63/74 | train_loss=0.2230 train_acc=0.9461 train_f1=0.4461 val_loss=0.1724 val_acc=0.9457 val_f1=0.4457 \n",
            "Epoch 70/74 | train_loss=0.2141 train_acc=0.9481 train_f1=0.4597 val_loss=0.1626 val_acc=0.9481 val_f1=0.4589 \n",
            "Evaluating member 5: Hyperparameters(lr=0.09179484420765417, epochs=158, hidden=(128, 8), dropout_rate=0.03583353619112145, patience=21)\n",
            "Epoch 15/158 | train_loss=0.3450 train_acc=0.8275 train_f1=0.3111 val_loss=0.3221 val_acc=0.8272 val_f1=0.3102 \n",
            "Epoch 30/158 | train_loss=0.2403 train_acc=0.9196 train_f1=0.3759 val_loss=0.2285 val_acc=0.9202 val_f1=0.3763 \n",
            "Epoch 45/158 | train_loss=0.1821 train_acc=0.9473 train_f1=0.4519 val_loss=0.1794 val_acc=0.9467 val_f1=0.4476 \n",
            "Epoch 60/158 | train_loss=0.1544 train_acc=0.9508 train_f1=0.4943 val_loss=0.1496 val_acc=0.9499 val_f1=0.4897 \n",
            "Epoch 75/158 | train_loss=0.1326 train_acc=0.9624 train_f1=0.5149 val_loss=0.1379 val_acc=0.9617 val_f1=0.5102 \n",
            "Epoch 90/158 | train_loss=0.1242 train_acc=0.9652 train_f1=0.5240 val_loss=0.1274 val_acc=0.9646 val_f1=0.5192 \n",
            "Epoch 105/158 | train_loss=0.1175 train_acc=0.9614 train_f1=0.5222 val_loss=0.1170 val_acc=0.9610 val_f1=0.5185 \n",
            "Epoch 120/158 | train_loss=0.1090 train_acc=0.9643 train_f1=0.5285 val_loss=0.1146 val_acc=0.9631 val_f1=0.5226 \n",
            "Epoch 135/158 | train_loss=0.1039 train_acc=0.9656 train_f1=0.5309 val_loss=0.1140 val_acc=0.9646 val_f1=0.5243 \n",
            "Epoch 150/158 | train_loss=0.1029 train_acc=0.9719 train_f1=0.5370 val_loss=0.1180 val_acc=0.9705 val_f1=0.5309 \n",
            "Evaluating member 6: Hyperparameters(lr=0.04664679470178938, epochs=139, hidden=(64, 64, 16), dropout_rate=0.07657390889475868, patience=29)\n",
            "Epoch 13/139 | train_loss=0.4619 train_acc=0.5035 train_f1=0.1883 val_loss=0.4399 val_acc=0.5040 val_f1=0.1886 \n",
            "Epoch 26/139 | train_loss=0.3260 train_acc=0.9046 train_f1=0.3448 val_loss=0.3015 val_acc=0.9057 val_f1=0.3452 \n",
            "Epoch 39/139 | train_loss=0.2681 train_acc=0.9192 train_f1=0.3542 val_loss=0.2545 val_acc=0.9198 val_f1=0.3542 \n",
            "Epoch 52/139 | train_loss=0.2325 train_acc=0.9336 train_f1=0.3640 val_loss=0.2195 val_acc=0.9332 val_f1=0.3635 \n",
            "Epoch 65/139 | train_loss=0.2003 train_acc=0.9392 train_f1=0.3803 val_loss=0.1849 val_acc=0.9400 val_f1=0.3824 \n",
            "Epoch 78/139 | train_loss=0.1710 train_acc=0.9509 train_f1=0.4923 val_loss=0.1534 val_acc=0.9515 val_f1=0.4921 \n",
            "Epoch 91/139 | train_loss=0.1526 train_acc=0.9556 train_f1=0.5128 val_loss=0.1376 val_acc=0.9558 val_f1=0.5107 \n",
            "Epoch 104/139 | train_loss=0.1403 train_acc=0.9551 train_f1=0.5205 val_loss=0.1282 val_acc=0.9552 val_f1=0.5183 \n",
            "Epoch 117/139 | train_loss=0.1278 train_acc=0.9542 train_f1=0.5203 val_loss=0.1227 val_acc=0.9544 val_f1=0.5170 \n",
            "Epoch 130/139 | train_loss=0.1228 train_acc=0.9548 train_f1=0.5223 val_loss=0.1184 val_acc=0.9543 val_f1=0.5200 \n",
            "Evaluating member 7: Hyperparameters(lr=0.05269603279046778, epochs=136, hidden=(16, 8, 32, 64, 8), dropout_rate=0.1375428073272184, patience=15)\n",
            "Epoch 13/136 | train_loss=0.7019 train_acc=0.0676 train_f1=0.0532 val_loss=0.6752 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/136 | train_loss=0.5294 train_acc=0.0676 train_f1=0.0532 val_loss=0.4877 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 39/136 | train_loss=0.4192 train_acc=0.8806 train_f1=0.3311 val_loss=0.3829 val_acc=0.8814 val_f1=0.3313 \n",
            "Epoch 52/136 | train_loss=0.3778 train_acc=0.9249 train_f1=0.3560 val_loss=0.3438 val_acc=0.9243 val_f1=0.3554 \n",
            "Epoch 65/136 | train_loss=0.3500 train_acc=0.9299 train_f1=0.3591 val_loss=0.3169 val_acc=0.9307 val_f1=0.3592 \n",
            "Epoch 78/136 | train_loss=0.3432 train_acc=0.9421 train_f1=0.3672 val_loss=0.3065 val_acc=0.9426 val_f1=0.3670 \n",
            "Epoch 91/136 | train_loss=0.3360 train_acc=0.9432 train_f1=0.3683 val_loss=0.2949 val_acc=0.9441 val_f1=0.3687 \n",
            "Epoch 104/136 | train_loss=0.3247 train_acc=0.9473 train_f1=0.3712 val_loss=0.2867 val_acc=0.9478 val_f1=0.3717 \n",
            "Epoch 117/136 | train_loss=0.3153 train_acc=0.9488 train_f1=0.3727 val_loss=0.2774 val_acc=0.9496 val_f1=0.3729 \n",
            "Epoch 130/136 | train_loss=0.3114 train_acc=0.9482 train_f1=0.3725 val_loss=0.2673 val_acc=0.9489 val_f1=0.3729 \n",
            "Evaluating member 8: Hyperparameters(lr=0.09969497941161734, epochs=167, hidden=(8, 64, 8), dropout_rate=0.04329927000222261, patience=6)\n",
            "Epoch 16/167 | train_loss=0.4800 train_acc=0.7782 train_f1=0.2837 val_loss=0.4582 val_acc=0.7769 val_f1=0.2835 \n",
            "Epoch 32/167 | train_loss=0.3317 train_acc=0.9137 train_f1=0.3498 val_loss=0.3058 val_acc=0.9138 val_f1=0.3499 \n",
            "Epoch 48/167 | train_loss=0.2940 train_acc=0.9061 train_f1=0.3459 val_loss=0.2769 val_acc=0.9069 val_f1=0.3462 \n",
            "Epoch 64/167 | train_loss=0.2697 train_acc=0.9160 train_f1=0.3518 val_loss=0.2482 val_acc=0.9164 val_f1=0.3518 \n",
            "Epoch 80/167 | train_loss=0.2485 train_acc=0.9156 train_f1=0.3552 val_loss=0.2280 val_acc=0.9173 val_f1=0.3548 \n",
            "Epoch 96/167 | train_loss=0.2372 train_acc=0.9206 train_f1=0.3961 val_loss=0.2168 val_acc=0.9217 val_f1=0.3953 \n",
            "Epoch 112/167 | train_loss=0.2298 train_acc=0.9235 train_f1=0.4329 val_loss=0.2067 val_acc=0.9246 val_f1=0.4310 \n",
            "Epoch 128/167 | train_loss=0.2225 train_acc=0.9295 train_f1=0.4543 val_loss=0.2004 val_acc=0.9308 val_f1=0.4537 \n",
            "Epoch 144/167 | train_loss=0.2158 train_acc=0.9308 train_f1=0.4601 val_loss=0.1960 val_acc=0.9320 val_f1=0.4595 \n",
            "Epoch 160/167 | train_loss=0.2146 train_acc=0.9380 train_f1=0.4639 val_loss=0.1954 val_acc=0.9387 val_f1=0.4620 \n",
            "Early stopping at epoch 164\n",
            "Evaluating member 9: Hyperparameters(lr=0.04611911987433585, epochs=140, hidden=(32, 128, 32, 128), dropout_rate=0.18981188158811985, patience=21)\n",
            "Epoch 14/140 | train_loss=0.4877 train_acc=0.7126 train_f1=0.2564 val_loss=0.4434 val_acc=0.7139 val_f1=0.2574 \n",
            "Epoch 28/140 | train_loss=0.3503 train_acc=0.8730 train_f1=0.3277 val_loss=0.3082 val_acc=0.8739 val_f1=0.3283 \n",
            "Epoch 42/140 | train_loss=0.3002 train_acc=0.9075 train_f1=0.3468 val_loss=0.2729 val_acc=0.9079 val_f1=0.3468 \n",
            "Epoch 56/140 | train_loss=0.2712 train_acc=0.9216 train_f1=0.3557 val_loss=0.2463 val_acc=0.9219 val_f1=0.3558 \n",
            "Epoch 70/140 | train_loss=0.2520 train_acc=0.9319 train_f1=0.3624 val_loss=0.2279 val_acc=0.9321 val_f1=0.3621 \n",
            "Epoch 84/140 | train_loss=0.2282 train_acc=0.9375 train_f1=0.4068 val_loss=0.2056 val_acc=0.9377 val_f1=0.4050 \n",
            "Epoch 98/140 | train_loss=0.2132 train_acc=0.9458 train_f1=0.4728 val_loss=0.1913 val_acc=0.9462 val_f1=0.4726 \n",
            "Epoch 112/140 | train_loss=0.1971 train_acc=0.9504 train_f1=0.4892 val_loss=0.1745 val_acc=0.9509 val_f1=0.4892 \n",
            "Epoch 126/140 | train_loss=0.1899 train_acc=0.9491 train_f1=0.4913 val_loss=0.1635 val_acc=0.9492 val_f1=0.4909 \n",
            "Epoch 140/140 | train_loss=0.1793 train_acc=0.9504 train_f1=0.4960 val_loss=0.1601 val_acc=0.9505 val_f1=0.4951 \n",
            "Epoch 15/159 | train_loss=0.3794 train_acc=0.6928 train_f1=0.2513 val_loss=0.3741 val_acc=0.6946 val_f1=0.2521 \n",
            "Epoch 30/159 | train_loss=0.2733 train_acc=0.9026 train_f1=0.3434 val_loss=0.2770 val_acc=0.9030 val_f1=0.3434 \n",
            "Epoch 45/159 | train_loss=0.2123 train_acc=0.9150 train_f1=0.3519 val_loss=0.2279 val_acc=0.9147 val_f1=0.3511 \n",
            "Epoch 60/159 | train_loss=0.1692 train_acc=0.9308 train_f1=0.4241 val_loss=0.1875 val_acc=0.9317 val_f1=0.4244 \n",
            "Epoch 75/159 | train_loss=0.1352 train_acc=0.9277 train_f1=0.4662 val_loss=0.1633 val_acc=0.9286 val_f1=0.4645 \n",
            "Epoch 90/159 | train_loss=0.1295 train_acc=0.9349 train_f1=0.4851 val_loss=0.1558 val_acc=0.9357 val_f1=0.4835 \n",
            "Epoch 105/159 | train_loss=0.1057 train_acc=0.9448 train_f1=0.5152 val_loss=0.1330 val_acc=0.9449 val_f1=0.5120 \n",
            "Epoch 120/159 | train_loss=0.0942 train_acc=0.9476 train_f1=0.5174 val_loss=0.1286 val_acc=0.9471 val_f1=0.5141 \n",
            "Epoch 135/159 | train_loss=0.0892 train_acc=0.9474 train_f1=0.5198 val_loss=0.1249 val_acc=0.9468 val_f1=0.5153 \n",
            "Epoch 150/159 | train_loss=0.1199 train_acc=0.9323 train_f1=0.5088 val_loss=0.1530 val_acc=0.9321 val_f1=0.5061 \n",
            "Early stopping at epoch 154\n",
            "Epoch 18/187 | train_loss=0.5841 train_acc=0.2838 train_f1=0.1205 val_loss=0.5258 val_acc=0.2847 val_f1=0.1208 \n",
            "Epoch 36/187 | train_loss=0.3589 train_acc=0.8540 train_f1=0.3179 val_loss=0.3136 val_acc=0.8542 val_f1=0.3181 \n",
            "Epoch 54/187 | train_loss=0.2857 train_acc=0.9137 train_f1=0.3644 val_loss=0.2504 val_acc=0.9146 val_f1=0.3653 \n",
            "Epoch 72/187 | train_loss=0.2478 train_acc=0.9285 train_f1=0.4121 val_loss=0.2177 val_acc=0.9291 val_f1=0.4114 \n",
            "Epoch 90/187 | train_loss=0.2195 train_acc=0.9399 train_f1=0.4527 val_loss=0.1923 val_acc=0.9398 val_f1=0.4537 \n",
            "Epoch 108/187 | train_loss=0.1954 train_acc=0.9457 train_f1=0.4741 val_loss=0.1730 val_acc=0.9454 val_f1=0.4727 \n",
            "Epoch 126/187 | train_loss=0.1803 train_acc=0.9464 train_f1=0.4841 val_loss=0.1562 val_acc=0.9456 val_f1=0.4804 \n",
            "Epoch 144/187 | train_loss=0.1660 train_acc=0.9507 train_f1=0.4971 val_loss=0.1445 val_acc=0.9498 val_f1=0.4920 \n",
            "Epoch 162/187 | train_loss=0.1537 train_acc=0.9535 train_f1=0.5070 val_loss=0.1371 val_acc=0.9529 val_f1=0.5022 \n",
            "Epoch 180/187 | train_loss=0.1456 train_acc=0.9539 train_f1=0.5112 val_loss=0.1292 val_acc=0.9535 val_f1=0.5074 \n",
            "Epoch 7/71 | train_loss=0.6528 train_acc=0.3256 train_f1=0.1333 val_loss=0.5590 val_acc=0.3264 val_f1=0.1336 \n",
            "Epoch 14/71 | train_loss=0.4129 train_acc=0.8368 train_f1=0.3099 val_loss=0.3372 val_acc=0.8368 val_f1=0.3098 \n",
            "Epoch 21/71 | train_loss=0.3504 train_acc=0.8876 train_f1=0.3358 val_loss=0.2986 val_acc=0.8887 val_f1=0.3363 \n",
            "Epoch 28/71 | train_loss=0.3079 train_acc=0.9076 train_f1=0.3475 val_loss=0.2669 val_acc=0.9083 val_f1=0.3477 \n",
            "Epoch 35/71 | train_loss=0.2870 train_acc=0.9123 train_f1=0.3505 val_loss=0.2470 val_acc=0.9121 val_f1=0.3500 \n",
            "Epoch 42/71 | train_loss=0.2673 train_acc=0.9193 train_f1=0.3546 val_loss=0.2319 val_acc=0.9189 val_f1=0.3539 \n",
            "Epoch 49/71 | train_loss=0.2537 train_acc=0.9282 train_f1=0.3603 val_loss=0.2172 val_acc=0.9278 val_f1=0.3593 \n",
            "Epoch 56/71 | train_loss=0.2430 train_acc=0.9328 train_f1=0.3682 val_loss=0.2048 val_acc=0.9326 val_f1=0.3680 \n",
            "Epoch 63/71 | train_loss=0.2278 train_acc=0.9392 train_f1=0.3888 val_loss=0.1928 val_acc=0.9393 val_f1=0.3890 \n",
            "Epoch 70/71 | train_loss=0.2199 train_acc=0.9449 train_f1=0.4197 val_loss=0.1805 val_acc=0.9450 val_f1=0.4206 \n",
            "Epoch 17/176 | train_loss=0.5571 train_acc=0.0756 train_f1=0.0557 val_loss=0.5370 val_acc=0.0756 val_f1=0.0558 \n",
            "Epoch 34/176 | train_loss=0.3767 train_acc=0.8397 train_f1=0.3112 val_loss=0.3447 val_acc=0.8399 val_f1=0.3113 \n",
            "Epoch 51/176 | train_loss=0.2876 train_acc=0.9067 train_f1=0.3468 val_loss=0.2651 val_acc=0.9073 val_f1=0.3468 \n",
            "Epoch 68/176 | train_loss=0.2484 train_acc=0.9285 train_f1=0.3713 val_loss=0.2281 val_acc=0.9286 val_f1=0.3723 \n",
            "Epoch 85/176 | train_loss=0.2163 train_acc=0.9377 train_f1=0.4225 val_loss=0.1985 val_acc=0.9373 val_f1=0.4213 \n",
            "Epoch 102/176 | train_loss=0.1932 train_acc=0.9451 train_f1=0.4661 val_loss=0.1734 val_acc=0.9450 val_f1=0.4653 \n",
            "Epoch 119/176 | train_loss=0.1706 train_acc=0.9504 train_f1=0.4896 val_loss=0.1553 val_acc=0.9499 val_f1=0.4853 \n",
            "Epoch 136/176 | train_loss=0.1570 train_acc=0.9508 train_f1=0.4949 val_loss=0.1430 val_acc=0.9504 val_f1=0.4911 \n",
            "Epoch 153/176 | train_loss=0.1458 train_acc=0.9532 train_f1=0.5047 val_loss=0.1348 val_acc=0.9530 val_f1=0.5005 \n",
            "Epoch 170/176 | train_loss=0.1381 train_acc=0.9560 train_f1=0.5090 val_loss=0.1292 val_acc=0.9557 val_f1=0.5045 \n",
            "Epoch 17/172 | train_loss=0.4332 train_acc=0.7615 train_f1=0.2783 val_loss=0.4128 val_acc=0.7621 val_f1=0.2782 \n",
            "Epoch 34/172 | train_loss=0.3195 train_acc=0.9361 train_f1=0.3674 val_loss=0.3060 val_acc=0.9361 val_f1=0.3668 \n",
            "Epoch 51/172 | train_loss=0.2578 train_acc=0.9445 train_f1=0.3744 val_loss=0.2468 val_acc=0.9442 val_f1=0.3735 \n",
            "Epoch 68/172 | train_loss=0.2647 train_acc=0.9444 train_f1=0.3738 val_loss=0.2422 val_acc=0.9436 val_f1=0.3727 \n",
            "Epoch 85/172 | train_loss=0.2101 train_acc=0.9464 train_f1=0.4148 val_loss=0.1913 val_acc=0.9467 val_f1=0.4157 \n",
            "Epoch 102/172 | train_loss=0.1874 train_acc=0.9529 train_f1=0.4285 val_loss=0.1709 val_acc=0.9534 val_f1=0.4298 \n",
            "Epoch 119/172 | train_loss=0.1736 train_acc=0.9593 train_f1=0.4640 val_loss=0.1578 val_acc=0.9592 val_f1=0.4609 \n",
            "Epoch 136/172 | train_loss=0.1627 train_acc=0.9635 train_f1=0.4907 val_loss=0.1487 val_acc=0.9630 val_f1=0.4881 \n",
            "Epoch 153/172 | train_loss=0.1548 train_acc=0.9640 train_f1=0.5022 val_loss=0.1430 val_acc=0.9630 val_f1=0.4994 \n",
            "Epoch 170/172 | train_loss=0.1547 train_acc=0.9680 train_f1=0.5077 val_loss=0.1404 val_acc=0.9667 val_f1=0.5039 \n",
            "Epoch 15/154 | train_loss=0.7288 train_acc=0.0676 train_f1=0.0532 val_loss=0.7261 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/154 | train_loss=0.5736 train_acc=0.0676 train_f1=0.0532 val_loss=0.4824 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 45/154 | train_loss=0.4038 train_acc=0.8913 train_f1=0.3367 val_loss=0.3203 val_acc=0.8927 val_f1=0.3377 \n",
            "Epoch 60/154 | train_loss=0.3518 train_acc=0.9137 train_f1=0.3492 val_loss=0.3002 val_acc=0.9150 val_f1=0.3497 \n",
            "Epoch 75/154 | train_loss=0.3334 train_acc=0.9259 train_f1=0.3569 val_loss=0.2757 val_acc=0.9267 val_f1=0.3570 \n",
            "Epoch 90/154 | train_loss=0.3114 train_acc=0.9309 train_f1=0.3597 val_loss=0.2573 val_acc=0.9316 val_f1=0.3598 \n",
            "Epoch 105/154 | train_loss=0.2984 train_acc=0.9364 train_f1=0.3672 val_loss=0.2384 val_acc=0.9372 val_f1=0.3678 \n",
            "Epoch 120/154 | train_loss=0.2854 train_acc=0.9470 train_f1=0.4634 val_loss=0.2267 val_acc=0.9484 val_f1=0.4652 \n",
            "Epoch 135/154 | train_loss=0.2814 train_acc=0.9476 train_f1=0.4806 val_loss=0.2194 val_acc=0.9488 val_f1=0.4805 \n",
            "Epoch 150/154 | train_loss=0.2758 train_acc=0.9477 train_f1=0.4878 val_loss=0.2128 val_acc=0.9486 val_f1=0.4876 \n",
            "Epoch 6/60 | train_loss=1.0040 train_acc=0.0676 train_f1=0.0532 val_loss=0.9155 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 12/60 | train_loss=0.8047 train_acc=0.0676 train_f1=0.0532 val_loss=0.7697 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 18/60 | train_loss=0.7658 train_acc=0.0676 train_f1=0.0532 val_loss=0.7359 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 20\n",
            "Epoch 14/146 | train_loss=0.7842 train_acc=0.0676 train_f1=0.0532 val_loss=0.7269 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/146 | train_loss=0.7401 train_acc=0.0676 train_f1=0.0532 val_loss=0.7290 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 40\n",
            "Epoch 14/141 | train_loss=0.5335 train_acc=0.0676 train_f1=0.0532 val_loss=0.4928 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/141 | train_loss=0.3679 train_acc=0.8960 train_f1=0.3396 val_loss=0.3215 val_acc=0.8965 val_f1=0.3397 \n",
            "Epoch 42/141 | train_loss=0.3132 train_acc=0.9123 train_f1=0.3498 val_loss=0.2790 val_acc=0.9133 val_f1=0.3500 \n",
            "Epoch 56/141 | train_loss=0.2920 train_acc=0.9191 train_f1=0.3541 val_loss=0.2644 val_acc=0.9195 val_f1=0.3539 \n",
            "Epoch 70/141 | train_loss=0.2785 train_acc=0.9231 train_f1=0.3570 val_loss=0.2502 val_acc=0.9236 val_f1=0.3567 \n",
            "Epoch 84/141 | train_loss=0.2661 train_acc=0.9325 train_f1=0.3628 val_loss=0.2464 val_acc=0.9326 val_f1=0.3622 \n",
            "Epoch 98/141 | train_loss=0.2610 train_acc=0.9349 train_f1=0.3643 val_loss=0.2391 val_acc=0.9341 val_f1=0.3634 \n",
            "Epoch 112/141 | train_loss=0.2549 train_acc=0.9306 train_f1=0.3616 val_loss=0.2304 val_acc=0.9300 val_f1=0.3608 \n",
            "Epoch 126/141 | train_loss=0.2499 train_acc=0.9319 train_f1=0.3620 val_loss=0.2271 val_acc=0.9307 val_f1=0.3608 \n",
            "Epoch 140/141 | train_loss=0.2458 train_acc=0.9311 train_f1=0.3614 val_loss=0.2194 val_acc=0.9306 val_f1=0.3605 \n",
            "Epoch 12/129 | train_loss=0.5292 train_acc=0.6047 train_f1=0.2210 val_loss=0.4599 val_acc=0.6040 val_f1=0.2210 \n",
            "Epoch 24/129 | train_loss=0.3388 train_acc=0.8837 train_f1=0.3330 val_loss=0.3232 val_acc=0.8836 val_f1=0.3328 \n",
            "Epoch 36/129 | train_loss=0.2806 train_acc=0.8962 train_f1=0.3409 val_loss=0.2726 val_acc=0.8974 val_f1=0.3414 \n",
            "Epoch 48/129 | train_loss=0.2369 train_acc=0.9191 train_f1=0.3913 val_loss=0.2299 val_acc=0.9191 val_f1=0.3925 \n",
            "Epoch 60/129 | train_loss=0.2032 train_acc=0.9279 train_f1=0.4500 val_loss=0.1953 val_acc=0.9277 val_f1=0.4479 \n",
            "Epoch 72/129 | train_loss=0.1774 train_acc=0.9368 train_f1=0.4732 val_loss=0.1708 val_acc=0.9368 val_f1=0.4693 \n",
            "Epoch 84/129 | train_loss=0.1596 train_acc=0.9428 train_f1=0.4910 val_loss=0.1526 val_acc=0.9424 val_f1=0.4863 \n",
            "Epoch 96/129 | train_loss=0.1435 train_acc=0.9499 train_f1=0.5056 val_loss=0.1386 val_acc=0.9489 val_f1=0.5006 \n",
            "Epoch 108/129 | train_loss=0.1304 train_acc=0.9533 train_f1=0.5141 val_loss=0.1301 val_acc=0.9530 val_f1=0.5115 \n",
            "Epoch 120/129 | train_loss=0.1256 train_acc=0.9558 train_f1=0.5177 val_loss=0.1227 val_acc=0.9556 val_f1=0.5140 \n",
            "Gen 1 Best: {'epochs_run': 148, 'val_loss': 0.3002830445766449, 'val_accuracy': 0.9413625662154208, 'val_f1': 0.46475286161310353, 'flops': 23808}\n",
            "Generation 2 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.09752251974755369, epochs=159, hidden=(134, 8), dropout_rate=0.0, patience=16)\n",
            "Epoch 15/159 | train_loss=0.5108 train_acc=0.8884 train_f1=0.3225 val_loss=0.4839 val_acc=0.8893 val_f1=0.3228 \n",
            "Epoch 30/159 | train_loss=0.3313 train_acc=0.8950 train_f1=0.3388 val_loss=0.3394 val_acc=0.8960 val_f1=0.3396 \n",
            "Epoch 45/159 | train_loss=0.2835 train_acc=0.9180 train_f1=0.3529 val_loss=0.2870 val_acc=0.9181 val_f1=0.3531 \n",
            "Epoch 60/159 | train_loss=0.2514 train_acc=0.9182 train_f1=0.3539 val_loss=0.2606 val_acc=0.9179 val_f1=0.3537 \n",
            "Epoch 75/159 | train_loss=0.2276 train_acc=0.9325 train_f1=0.3636 val_loss=0.2397 val_acc=0.9321 val_f1=0.3632 \n",
            "Epoch 90/159 | train_loss=0.2068 train_acc=0.9377 train_f1=0.3852 val_loss=0.2216 val_acc=0.9375 val_f1=0.3860 \n",
            "Epoch 105/159 | train_loss=0.1879 train_acc=0.9353 train_f1=0.4252 val_loss=0.2045 val_acc=0.9342 val_f1=0.4223 \n",
            "Epoch 120/159 | train_loss=0.1819 train_acc=0.9431 train_f1=0.4144 val_loss=0.1982 val_acc=0.9431 val_f1=0.4145 \n",
            "Epoch 135/159 | train_loss=0.1559 train_acc=0.9493 train_f1=0.4590 val_loss=0.1725 val_acc=0.9488 val_f1=0.4550 \n",
            "Epoch 150/159 | train_loss=0.1372 train_acc=0.9552 train_f1=0.4792 val_loss=0.1621 val_acc=0.9542 val_f1=0.4752 \n",
            "Evaluating member 1: Hyperparameters(lr=0.004015092874724383, epochs=187, hidden=(110, 128, 32), dropout_rate=0.21594302926293632, patience=26)\n",
            "Epoch 18/187 | train_loss=0.5964 train_acc=0.2806 train_f1=0.1196 val_loss=0.5473 val_acc=0.2811 val_f1=0.1197 \n",
            "Epoch 36/187 | train_loss=0.4126 train_acc=0.7226 train_f1=0.2627 val_loss=0.3718 val_acc=0.7234 val_f1=0.2632 \n",
            "Epoch 54/187 | train_loss=0.3085 train_acc=0.8951 train_f1=0.3399 val_loss=0.2703 val_acc=0.8963 val_f1=0.3405 \n",
            "Epoch 72/187 | train_loss=0.2594 train_acc=0.9185 train_f1=0.3889 val_loss=0.2317 val_acc=0.9185 val_f1=0.3868 \n",
            "Epoch 90/187 | train_loss=0.2257 train_acc=0.9312 train_f1=0.4429 val_loss=0.1990 val_acc=0.9312 val_f1=0.4419 \n",
            "Epoch 108/187 | train_loss=0.2042 train_acc=0.9403 train_f1=0.4682 val_loss=0.1780 val_acc=0.9395 val_f1=0.4642 \n",
            "Epoch 126/187 | train_loss=0.1868 train_acc=0.9435 train_f1=0.4812 val_loss=0.1629 val_acc=0.9428 val_f1=0.4785 \n",
            "Epoch 144/187 | train_loss=0.1741 train_acc=0.9446 train_f1=0.4855 val_loss=0.1519 val_acc=0.9436 val_f1=0.4806 \n",
            "Epoch 162/187 | train_loss=0.1593 train_acc=0.9483 train_f1=0.4963 val_loss=0.1419 val_acc=0.9473 val_f1=0.4917 \n",
            "Epoch 180/187 | train_loss=0.1515 train_acc=0.9503 train_f1=0.5069 val_loss=0.1337 val_acc=0.9498 val_f1=0.5024 \n",
            "Evaluating member 2: Hyperparameters(lr=0.036584036109887585, epochs=71, hidden=(256, 49), dropout_rate=0.4536090390724842, patience=7)\n",
            "Epoch 7/71 | train_loss=0.7219 train_acc=0.2796 train_f1=0.1192 val_loss=0.6036 val_acc=0.2815 val_f1=0.1198 \n",
            "Epoch 14/71 | train_loss=0.4262 train_acc=0.7442 train_f1=0.2709 val_loss=0.3582 val_acc=0.7458 val_f1=0.2716 \n",
            "Epoch 21/71 | train_loss=0.3569 train_acc=0.9099 train_f1=0.3478 val_loss=0.3014 val_acc=0.9115 val_f1=0.3489 \n",
            "Epoch 28/71 | train_loss=0.3211 train_acc=0.9129 train_f1=0.3502 val_loss=0.2757 val_acc=0.9143 val_f1=0.3509 \n",
            "Epoch 35/71 | train_loss=0.2930 train_acc=0.9208 train_f1=0.3577 val_loss=0.2539 val_acc=0.9213 val_f1=0.3580 \n",
            "Epoch 42/71 | train_loss=0.2698 train_acc=0.9268 train_f1=0.3671 val_loss=0.2380 val_acc=0.9266 val_f1=0.3655 \n",
            "Epoch 49/71 | train_loss=0.2560 train_acc=0.9309 train_f1=0.3787 val_loss=0.2208 val_acc=0.9303 val_f1=0.3779 \n",
            "Epoch 56/71 | train_loss=0.2375 train_acc=0.9338 train_f1=0.3892 val_loss=0.2011 val_acc=0.9337 val_f1=0.3897 \n",
            "Epoch 63/71 | train_loss=0.2198 train_acc=0.9392 train_f1=0.4125 val_loss=0.1855 val_acc=0.9395 val_f1=0.4119 \n",
            "Epoch 70/71 | train_loss=0.2095 train_acc=0.9429 train_f1=0.4352 val_loss=0.1735 val_acc=0.9433 val_f1=0.4347 \n",
            "Evaluating member 3: Hyperparameters(lr=0.004179600176044582, epochs=176, hidden=(143, 128, 32), dropout_rate=0.16879212981898692, patience=28)\n",
            "Epoch 17/176 | train_loss=0.5425 train_acc=0.1862 train_f1=0.0904 val_loss=0.5163 val_acc=0.1870 val_f1=0.0907 \n",
            "Epoch 34/176 | train_loss=0.3364 train_acc=0.8928 train_f1=0.3382 val_loss=0.3039 val_acc=0.8919 val_f1=0.3374 \n",
            "Epoch 51/176 | train_loss=0.2724 train_acc=0.9148 train_f1=0.3513 val_loss=0.2545 val_acc=0.9143 val_f1=0.3508 \n",
            "Epoch 68/176 | train_loss=0.2359 train_acc=0.9282 train_f1=0.3974 val_loss=0.2182 val_acc=0.9280 val_f1=0.3968 \n",
            "Epoch 85/176 | train_loss=0.2038 train_acc=0.9378 train_f1=0.4433 val_loss=0.1873 val_acc=0.9377 val_f1=0.4440 \n",
            "Epoch 102/176 | train_loss=0.1761 train_acc=0.9416 train_f1=0.4723 val_loss=0.1639 val_acc=0.9405 val_f1=0.4679 \n",
            "Epoch 119/176 | train_loss=0.1586 train_acc=0.9470 train_f1=0.4883 val_loss=0.1467 val_acc=0.9463 val_f1=0.4851 \n",
            "Epoch 136/176 | train_loss=0.1460 train_acc=0.9486 train_f1=0.5040 val_loss=0.1342 val_acc=0.9480 val_f1=0.4992 \n",
            "Epoch 153/176 | train_loss=0.1348 train_acc=0.9546 train_f1=0.5130 val_loss=0.1260 val_acc=0.9542 val_f1=0.5085 \n",
            "Epoch 170/176 | train_loss=0.1265 train_acc=0.9540 train_f1=0.5175 val_loss=0.1195 val_acc=0.9538 val_f1=0.5144 \n",
            "Evaluating member 4: Hyperparameters(lr=0.08857977278776764, epochs=172, hidden=(137, 8), dropout_rate=0.04903367079619693, patience=22)\n",
            "Epoch 17/172 | train_loss=0.3450 train_acc=0.8478 train_f1=0.3165 val_loss=0.3143 val_acc=0.8485 val_f1=0.3166 \n",
            "Epoch 34/172 | train_loss=0.2390 train_acc=0.9185 train_f1=0.3740 val_loss=0.2250 val_acc=0.9201 val_f1=0.3756 \n",
            "Epoch 51/172 | train_loss=0.2009 train_acc=0.9346 train_f1=0.4117 val_loss=0.1842 val_acc=0.9346 val_f1=0.4119 \n",
            "Epoch 68/172 | train_loss=0.1797 train_acc=0.9452 train_f1=0.4353 val_loss=0.1655 val_acc=0.9447 val_f1=0.4335 \n",
            "Epoch 85/172 | train_loss=0.1649 train_acc=0.9489 train_f1=0.4601 val_loss=0.1514 val_acc=0.9482 val_f1=0.4578 \n",
            "Epoch 102/172 | train_loss=0.1578 train_acc=0.9490 train_f1=0.4756 val_loss=0.1433 val_acc=0.9485 val_f1=0.4724 \n",
            "Epoch 119/172 | train_loss=0.1473 train_acc=0.9570 train_f1=0.4881 val_loss=0.1408 val_acc=0.9568 val_f1=0.4865 \n",
            "Epoch 136/172 | train_loss=0.1429 train_acc=0.9596 train_f1=0.4938 val_loss=0.1360 val_acc=0.9586 val_f1=0.4906 \n",
            "Epoch 153/172 | train_loss=0.1382 train_acc=0.9578 train_f1=0.4949 val_loss=0.1336 val_acc=0.9569 val_f1=0.4922 \n",
            "Epoch 170/172 | train_loss=0.1350 train_acc=0.9576 train_f1=0.4995 val_loss=0.1348 val_acc=0.9556 val_f1=0.4935 \n",
            "Evaluating member 5: Hyperparameters(lr=0.04319222404381434, epochs=154, hidden=(64, 32, 64, 256), dropout_rate=0.17337338707639155, patience=12)\n",
            "Epoch 15/154 | train_loss=0.7423 train_acc=0.0676 train_f1=0.0532 val_loss=0.7296 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/154 | train_loss=0.7318 train_acc=0.0676 train_f1=0.0532 val_loss=0.7273 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 43\n",
            "Evaluating member 6: Hyperparameters(lr=0.05028189859853015, epochs=60, hidden=(253, 16, 128, 32), dropout_rate=0.17487954122265428, patience=11)\n",
            "Epoch 6/60 | train_loss=1.0718 train_acc=0.8688 train_f1=0.3097 val_loss=0.7682 val_acc=0.8683 val_f1=0.3103 \n",
            "Epoch 12/60 | train_loss=0.6216 train_acc=0.0676 train_f1=0.0532 val_loss=0.5517 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 18/60 | train_loss=0.4607 train_acc=0.8097 train_f1=0.2969 val_loss=0.3619 val_acc=0.8105 val_f1=0.2975 \n",
            "Epoch 24/60 | train_loss=0.4043 train_acc=0.8676 train_f1=0.3250 val_loss=0.3329 val_acc=0.8692 val_f1=0.3256 \n",
            "Epoch 30/60 | train_loss=0.3734 train_acc=0.9146 train_f1=0.3497 val_loss=0.3203 val_acc=0.9153 val_f1=0.3499 \n",
            "Epoch 36/60 | train_loss=0.3604 train_acc=0.9033 train_f1=0.3434 val_loss=0.3009 val_acc=0.9037 val_f1=0.3434 \n",
            "Epoch 42/60 | train_loss=0.3422 train_acc=0.9099 train_f1=0.3470 val_loss=0.2894 val_acc=0.9107 val_f1=0.3472 \n",
            "Epoch 48/60 | train_loss=0.3333 train_acc=0.9105 train_f1=0.3475 val_loss=0.2740 val_acc=0.9124 val_f1=0.3485 \n",
            "Epoch 54/60 | train_loss=0.3246 train_acc=0.9180 train_f1=0.3518 val_loss=0.2688 val_acc=0.9192 val_f1=0.3525 \n",
            "Epoch 60/60 | train_loss=0.3206 train_acc=0.9213 train_f1=0.3538 val_loss=0.2571 val_acc=0.9220 val_f1=0.3540 \n",
            "Evaluating member 7: Hyperparameters(lr=0.04618016097980701, epochs=146, hidden=(32, 128, 32, 128, 32), dropout_rate=0.25494385443387996, patience=26)\n",
            "Epoch 14/146 | train_loss=0.7684 train_acc=0.0676 train_f1=0.0532 val_loss=0.7476 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/146 | train_loss=0.7388 train_acc=0.0676 train_f1=0.0532 val_loss=0.7322 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 42/146 | train_loss=0.7309 train_acc=0.0676 train_f1=0.0532 val_loss=0.7276 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 56/146 | train_loss=0.7291 train_acc=0.0676 train_f1=0.0532 val_loss=0.7267 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 70/146 | train_loss=0.7280 train_acc=0.0676 train_f1=0.0532 val_loss=0.7266 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 84/146 | train_loss=0.7276 train_acc=0.0676 train_f1=0.0532 val_loss=0.7266 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 85\n",
            "Evaluating member 8: Hyperparameters(lr=0.05280890765436989, epochs=141, hidden=(16, 8, 11, 64, 8), dropout_rate=0.11892231550958421, patience=10)\n",
            "Epoch 14/141 | train_loss=0.5687 train_acc=0.5865 train_f1=0.2147 val_loss=0.5453 val_acc=0.5857 val_f1=0.2146 \n",
            "Epoch 28/141 | train_loss=0.4225 train_acc=0.8677 train_f1=0.3248 val_loss=0.3975 val_acc=0.8678 val_f1=0.3245 \n",
            "Epoch 42/141 | train_loss=0.3561 train_acc=0.9141 train_f1=0.3495 val_loss=0.3317 val_acc=0.9148 val_f1=0.3495 \n",
            "Epoch 56/141 | train_loss=0.3302 train_acc=0.9290 train_f1=0.3587 val_loss=0.3106 val_acc=0.9289 val_f1=0.3581 \n",
            "Epoch 70/141 | train_loss=0.3124 train_acc=0.9369 train_f1=0.3641 val_loss=0.2937 val_acc=0.9370 val_f1=0.3636 \n",
            "Epoch 84/141 | train_loss=0.2955 train_acc=0.9420 train_f1=0.3681 val_loss=0.2758 val_acc=0.9420 val_f1=0.3675 \n",
            "Epoch 98/141 | train_loss=0.2829 train_acc=0.9447 train_f1=0.3704 val_loss=0.2620 val_acc=0.9448 val_f1=0.3700 \n",
            "Epoch 112/141 | train_loss=0.2734 train_acc=0.9447 train_f1=0.3703 val_loss=0.2516 val_acc=0.9451 val_f1=0.3703 \n",
            "Epoch 126/141 | train_loss=0.2655 train_acc=0.9414 train_f1=0.3686 val_loss=0.2398 val_acc=0.9418 val_f1=0.3684 \n",
            "Epoch 140/141 | train_loss=0.2573 train_acc=0.9408 train_f1=0.3676 val_loss=0.2319 val_acc=0.9410 val_f1=0.3673 \n",
            "Evaluating member 9: Hyperparameters(lr=0.04963011482378435, epochs=129, hidden=(64, 64, 26), dropout_rate=0.05325680952895186, patience=27)\n",
            "Epoch 12/129 | train_loss=0.4148 train_acc=0.7163 train_f1=0.2597 val_loss=0.4069 val_acc=0.7189 val_f1=0.2608 \n",
            "Epoch 24/129 | train_loss=0.2996 train_acc=0.9070 train_f1=0.3461 val_loss=0.2877 val_acc=0.9080 val_f1=0.3467 \n",
            "Epoch 36/129 | train_loss=0.2499 train_acc=0.9234 train_f1=0.3570 val_loss=0.2438 val_acc=0.9237 val_f1=0.3574 \n",
            "Epoch 48/129 | train_loss=0.2120 train_acc=0.9178 train_f1=0.4100 val_loss=0.2086 val_acc=0.9185 val_f1=0.4114 \n",
            "Epoch 60/129 | train_loss=0.1776 train_acc=0.9328 train_f1=0.4721 val_loss=0.1727 val_acc=0.9332 val_f1=0.4707 \n",
            "Epoch 72/129 | train_loss=0.1518 train_acc=0.9441 train_f1=0.4966 val_loss=0.1490 val_acc=0.9441 val_f1=0.4954 \n",
            "Epoch 84/129 | train_loss=0.1308 train_acc=0.9503 train_f1=0.5118 val_loss=0.1347 val_acc=0.9500 val_f1=0.5097 \n",
            "Epoch 96/129 | train_loss=0.1219 train_acc=0.9554 train_f1=0.5188 val_loss=0.1235 val_acc=0.9551 val_f1=0.5151 \n",
            "Epoch 108/129 | train_loss=0.1147 train_acc=0.9564 train_f1=0.5227 val_loss=0.1174 val_acc=0.9559 val_f1=0.5187 \n",
            "Epoch 120/129 | train_loss=0.1073 train_acc=0.9554 train_f1=0.5218 val_loss=0.1139 val_acc=0.9542 val_f1=0.5173 \n",
            "Epoch 12/122 | train_loss=0.4760 train_acc=0.6125 train_f1=0.2235 val_loss=0.4584 val_acc=0.6132 val_f1=0.2238 \n",
            "Epoch 24/122 | train_loss=0.3044 train_acc=0.8803 train_f1=0.3311 val_loss=0.3032 val_acc=0.8799 val_f1=0.3308 \n",
            "Epoch 36/122 | train_loss=0.2402 train_acc=0.8970 train_f1=0.3907 val_loss=0.2465 val_acc=0.8972 val_f1=0.3889 \n",
            "Epoch 48/122 | train_loss=0.1942 train_acc=0.9198 train_f1=0.4421 val_loss=0.2062 val_acc=0.9197 val_f1=0.4413 \n",
            "Epoch 60/122 | train_loss=0.1582 train_acc=0.9322 train_f1=0.4708 val_loss=0.1776 val_acc=0.9320 val_f1=0.4692 \n",
            "Epoch 72/122 | train_loss=0.1404 train_acc=0.9103 train_f1=0.4725 val_loss=0.1613 val_acc=0.9097 val_f1=0.4705 \n",
            "Epoch 84/122 | train_loss=0.1181 train_acc=0.9524 train_f1=0.5144 val_loss=0.1466 val_acc=0.9519 val_f1=0.5091 \n",
            "Epoch 96/122 | train_loss=0.1046 train_acc=0.9523 train_f1=0.5185 val_loss=0.1300 val_acc=0.9518 val_f1=0.5138 \n",
            "Epoch 108/122 | train_loss=0.0941 train_acc=0.9531 train_f1=0.5211 val_loss=0.1229 val_acc=0.9529 val_f1=0.5172 \n",
            "Epoch 120/122 | train_loss=0.1267 train_acc=0.9617 train_f1=0.5194 val_loss=0.1419 val_acc=0.9608 val_f1=0.5146 \n",
            "Epoch 18/188 | train_loss=0.5779 train_acc=0.3245 train_f1=0.1330 val_loss=0.5414 val_acc=0.3244 val_f1=0.1330 \n",
            "Epoch 36/188 | train_loss=0.4168 train_acc=0.7200 train_f1=0.2615 val_loss=0.3850 val_acc=0.7205 val_f1=0.2618 \n",
            "Epoch 54/188 | train_loss=0.3183 train_acc=0.8917 train_f1=0.3378 val_loss=0.2897 val_acc=0.8921 val_f1=0.3379 \n",
            "Epoch 72/188 | train_loss=0.2746 train_acc=0.9028 train_f1=0.3655 val_loss=0.2514 val_acc=0.9034 val_f1=0.3640 \n",
            "Epoch 90/188 | train_loss=0.2494 train_acc=0.9158 train_f1=0.3967 val_loss=0.2274 val_acc=0.9164 val_f1=0.3977 \n",
            "Epoch 108/188 | train_loss=0.2302 train_acc=0.9231 train_f1=0.4217 val_loss=0.2082 val_acc=0.9234 val_f1=0.4211 \n",
            "Epoch 126/188 | train_loss=0.2125 train_acc=0.9310 train_f1=0.4429 val_loss=0.1925 val_acc=0.9317 val_f1=0.4440 \n",
            "Epoch 144/188 | train_loss=0.1997 train_acc=0.9348 train_f1=0.4522 val_loss=0.1813 val_acc=0.9353 val_f1=0.4523 \n",
            "Epoch 162/188 | train_loss=0.1891 train_acc=0.9418 train_f1=0.4659 val_loss=0.1717 val_acc=0.9417 val_f1=0.4634 \n",
            "Epoch 180/188 | train_loss=0.1826 train_acc=0.9426 train_f1=0.4707 val_loss=0.1644 val_acc=0.9422 val_f1=0.4678 \n",
            "Epoch 17/172 | train_loss=0.4440 train_acc=0.5815 train_f1=0.2133 val_loss=0.4209 val_acc=0.5805 val_f1=0.2131 \n",
            "Epoch 34/172 | train_loss=0.2940 train_acc=0.9092 train_f1=0.3551 val_loss=0.2766 val_acc=0.9086 val_f1=0.3526 \n",
            "Epoch 51/172 | train_loss=0.2485 train_acc=0.9172 train_f1=0.4063 val_loss=0.2353 val_acc=0.9183 val_f1=0.4086 \n",
            "Epoch 68/172 | train_loss=0.2206 train_acc=0.9297 train_f1=0.4288 val_loss=0.2081 val_acc=0.9295 val_f1=0.4293 \n",
            "Epoch 85/172 | train_loss=0.1965 train_acc=0.9361 train_f1=0.4527 val_loss=0.1886 val_acc=0.9355 val_f1=0.4513 \n",
            "Epoch 102/172 | train_loss=0.1782 train_acc=0.9415 train_f1=0.4681 val_loss=0.1729 val_acc=0.9409 val_f1=0.4658 \n",
            "Epoch 119/172 | train_loss=0.1679 train_acc=0.9469 train_f1=0.4792 val_loss=0.1594 val_acc=0.9460 val_f1=0.4776 \n",
            "Epoch 136/172 | train_loss=0.1548 train_acc=0.9496 train_f1=0.4925 val_loss=0.1480 val_acc=0.9484 val_f1=0.4894 \n",
            "Epoch 153/172 | train_loss=0.1441 train_acc=0.9526 train_f1=0.5033 val_loss=0.1398 val_acc=0.9518 val_f1=0.5005 \n",
            "Epoch 170/172 | train_loss=0.1375 train_acc=0.9535 train_f1=0.5077 val_loss=0.1330 val_acc=0.9533 val_f1=0.5047 \n",
            "Epoch 17/173 | train_loss=0.4090 train_acc=0.8545 train_f1=0.3181 val_loss=0.3872 val_acc=0.8544 val_f1=0.3185 \n",
            "Epoch 34/173 | train_loss=0.2794 train_acc=0.9038 train_f1=0.3569 val_loss=0.2694 val_acc=0.9025 val_f1=0.3532 \n",
            "Epoch 51/173 | train_loss=0.2286 train_acc=0.9145 train_f1=0.3935 val_loss=0.2220 val_acc=0.9137 val_f1=0.3924 \n",
            "Epoch 68/173 | train_loss=0.1906 train_acc=0.9260 train_f1=0.4366 val_loss=0.1849 val_acc=0.9264 val_f1=0.4339 \n",
            "Epoch 85/173 | train_loss=0.1611 train_acc=0.9380 train_f1=0.4698 val_loss=0.1561 val_acc=0.9381 val_f1=0.4660 \n",
            "Epoch 102/173 | train_loss=0.1410 train_acc=0.9516 train_f1=0.4877 val_loss=0.1414 val_acc=0.9511 val_f1=0.4853 \n",
            "Epoch 119/173 | train_loss=0.1269 train_acc=0.9550 train_f1=0.5003 val_loss=0.1330 val_acc=0.9543 val_f1=0.4960 \n",
            "Epoch 136/173 | train_loss=0.1187 train_acc=0.9598 train_f1=0.5194 val_loss=0.1251 val_acc=0.9594 val_f1=0.5165 \n",
            "Epoch 153/173 | train_loss=0.1109 train_acc=0.9595 train_f1=0.5224 val_loss=0.1224 val_acc=0.9593 val_f1=0.5187 \n",
            "Epoch 170/173 | train_loss=0.1047 train_acc=0.9617 train_f1=0.5273 val_loss=0.1179 val_acc=0.9609 val_f1=0.5218 \n",
            "Epoch 15/150 | train_loss=0.6025 train_acc=0.0676 train_f1=0.0532 val_loss=0.6109 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/150 | train_loss=0.4608 train_acc=0.1151 train_f1=0.0682 val_loss=0.4466 val_acc=0.1157 val_f1=0.0684 \n",
            "Epoch 45/150 | train_loss=0.3726 train_acc=0.9023 train_f1=0.3436 val_loss=0.3492 val_acc=0.9020 val_f1=0.3433 \n",
            "Epoch 60/150 | train_loss=0.3254 train_acc=0.9093 train_f1=0.3481 val_loss=0.3098 val_acc=0.9098 val_f1=0.3480 \n",
            "Epoch 75/150 | train_loss=0.2991 train_acc=0.9263 train_f1=0.3579 val_loss=0.2837 val_acc=0.9271 val_f1=0.3580 \n",
            "Epoch 90/150 | train_loss=0.2798 train_acc=0.9322 train_f1=0.3621 val_loss=0.2626 val_acc=0.9328 val_f1=0.3622 \n",
            "Epoch 105/150 | train_loss=0.2677 train_acc=0.9375 train_f1=0.3656 val_loss=0.2470 val_acc=0.9378 val_f1=0.3658 \n",
            "Epoch 120/150 | train_loss=0.2525 train_acc=0.9400 train_f1=0.3676 val_loss=0.2320 val_acc=0.9403 val_f1=0.3676 \n",
            "Epoch 135/150 | train_loss=0.2424 train_acc=0.9433 train_f1=0.3703 val_loss=0.2199 val_acc=0.9430 val_f1=0.3697 \n",
            "Epoch 150/150 | train_loss=0.2343 train_acc=0.9510 train_f1=0.4044 val_loss=0.2110 val_acc=0.9505 val_f1=0.4045 \n",
            "Epoch 14/147 | train_loss=0.5641 train_acc=0.7239 train_f1=0.2621 val_loss=0.4718 val_acc=0.7230 val_f1=0.2618 \n",
            "Epoch 28/147 | train_loss=0.4261 train_acc=0.9138 train_f1=0.3504 val_loss=0.3602 val_acc=0.9138 val_f1=0.3501 \n",
            "Epoch 42/147 | train_loss=0.3778 train_acc=0.9363 train_f1=0.3638 val_loss=0.3234 val_acc=0.9371 val_f1=0.3641 \n",
            "Epoch 56/147 | train_loss=0.3568 train_acc=0.9435 train_f1=0.3689 val_loss=0.3111 val_acc=0.9440 val_f1=0.3688 \n",
            "Epoch 70/147 | train_loss=0.3475 train_acc=0.9454 train_f1=0.3701 val_loss=0.2962 val_acc=0.9456 val_f1=0.3699 \n",
            "Epoch 84/147 | train_loss=0.3364 train_acc=0.9471 train_f1=0.3720 val_loss=0.2896 val_acc=0.9474 val_f1=0.3717 \n",
            "Epoch 98/147 | train_loss=0.3311 train_acc=0.9514 train_f1=0.3753 val_loss=0.2851 val_acc=0.9513 val_f1=0.3747 \n",
            "Epoch 112/147 | train_loss=0.3310 train_acc=0.9511 train_f1=0.3751 val_loss=0.2787 val_acc=0.9509 val_f1=0.3744 \n",
            "Epoch 126/147 | train_loss=0.3178 train_acc=0.9531 train_f1=0.3764 val_loss=0.2745 val_acc=0.9526 val_f1=0.3756 \n",
            "Epoch 140/147 | train_loss=0.3197 train_acc=0.9535 train_f1=0.3773 val_loss=0.2706 val_acc=0.9530 val_f1=0.3764 \n",
            "Early stopping at epoch 142\n",
            "Epoch 14/143 | train_loss=0.7802 train_acc=0.0676 train_f1=0.0532 val_loss=0.7453 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/143 | train_loss=0.5507 train_acc=0.8331 train_f1=0.3055 val_loss=0.4359 val_acc=0.8332 val_f1=0.3060 \n",
            "Epoch 42/143 | train_loss=0.4917 train_acc=0.9196 train_f1=0.3527 val_loss=0.3819 val_acc=0.9204 val_f1=0.3530 \n",
            "Epoch 56/143 | train_loss=0.3800 train_acc=0.8942 train_f1=0.3384 val_loss=0.3062 val_acc=0.8948 val_f1=0.3386 \n",
            "Epoch 70/143 | train_loss=0.3384 train_acc=0.9271 train_f1=0.3579 val_loss=0.2877 val_acc=0.9288 val_f1=0.3586 \n",
            "Epoch 84/143 | train_loss=0.3241 train_acc=0.9299 train_f1=0.3596 val_loss=0.2712 val_acc=0.9307 val_f1=0.3598 \n",
            "Epoch 98/143 | train_loss=0.3155 train_acc=0.9361 train_f1=0.3636 val_loss=0.2567 val_acc=0.9366 val_f1=0.3637 \n",
            "Epoch 112/143 | train_loss=0.3013 train_acc=0.9398 train_f1=0.3910 val_loss=0.2461 val_acc=0.9403 val_f1=0.3916 \n",
            "Epoch 126/143 | train_loss=0.2921 train_acc=0.9401 train_f1=0.4448 val_loss=0.2337 val_acc=0.9411 val_f1=0.4459 \n",
            "Epoch 140/143 | train_loss=0.2865 train_acc=0.9437 train_f1=0.4671 val_loss=0.2268 val_acc=0.9443 val_f1=0.4676 \n",
            "Epoch 16/164 | train_loss=0.6033 train_acc=0.0676 train_f1=0.0532 val_loss=0.5513 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/164 | train_loss=0.4051 train_acc=0.8982 train_f1=0.3404 val_loss=0.3372 val_acc=0.8992 val_f1=0.3410 \n",
            "Epoch 48/164 | train_loss=0.3748 train_acc=0.9275 train_f1=0.3580 val_loss=0.3009 val_acc=0.9274 val_f1=0.3576 \n",
            "Epoch 64/164 | train_loss=0.2811 train_acc=0.9204 train_f1=0.3539 val_loss=0.2590 val_acc=0.9220 val_f1=0.3546 \n",
            "Epoch 80/164 | train_loss=0.2478 train_acc=0.9305 train_f1=0.4141 val_loss=0.2272 val_acc=0.9311 val_f1=0.4132 \n",
            "Epoch 96/164 | train_loss=0.2257 train_acc=0.9424 train_f1=0.4576 val_loss=0.2088 val_acc=0.9431 val_f1=0.4582 \n",
            "Epoch 112/164 | train_loss=0.2131 train_acc=0.9409 train_f1=0.4810 val_loss=0.1882 val_acc=0.9408 val_f1=0.4780 \n",
            "Epoch 128/164 | train_loss=0.1945 train_acc=0.9544 train_f1=0.5012 val_loss=0.1778 val_acc=0.9543 val_f1=0.4997 \n",
            "Epoch 144/164 | train_loss=0.1856 train_acc=0.9532 train_f1=0.4890 val_loss=0.1689 val_acc=0.9528 val_f1=0.4863 \n",
            "Epoch 160/164 | train_loss=0.1728 train_acc=0.9572 train_f1=0.5066 val_loss=0.1626 val_acc=0.9562 val_f1=0.5025 \n",
            "Epoch 18/188 | train_loss=0.5696 train_acc=0.0937 train_f1=0.0615 val_loss=0.5245 val_acc=0.0929 val_f1=0.0612 \n",
            "Epoch 36/188 | train_loss=0.3945 train_acc=0.8094 train_f1=0.2973 val_loss=0.3515 val_acc=0.8085 val_f1=0.2968 \n",
            "Epoch 54/188 | train_loss=0.3058 train_acc=0.9052 train_f1=0.3454 val_loss=0.2690 val_acc=0.9061 val_f1=0.3456 \n",
            "Epoch 72/188 | train_loss=0.2657 train_acc=0.9194 train_f1=0.3694 val_loss=0.2344 val_acc=0.9189 val_f1=0.3694 \n",
            "Epoch 90/188 | train_loss=0.2362 train_acc=0.9313 train_f1=0.4190 val_loss=0.2053 val_acc=0.9320 val_f1=0.4205 \n",
            "Epoch 108/188 | train_loss=0.2134 train_acc=0.9400 train_f1=0.4526 val_loss=0.1832 val_acc=0.9401 val_f1=0.4528 \n",
            "Epoch 126/188 | train_loss=0.1940 train_acc=0.9435 train_f1=0.4728 val_loss=0.1658 val_acc=0.9433 val_f1=0.4715 \n",
            "Epoch 144/188 | train_loss=0.1769 train_acc=0.9468 train_f1=0.4885 val_loss=0.1527 val_acc=0.9465 val_f1=0.4874 \n",
            "Epoch 162/188 | train_loss=0.1648 train_acc=0.9476 train_f1=0.4970 val_loss=0.1421 val_acc=0.9476 val_f1=0.4957 \n",
            "Epoch 180/188 | train_loss=0.1560 train_acc=0.9518 train_f1=0.5055 val_loss=0.1339 val_acc=0.9515 val_f1=0.5037 \n",
            "Epoch 16/168 | train_loss=0.4788 train_acc=0.8067 train_f1=0.2965 val_loss=0.4383 val_acc=0.8083 val_f1=0.2969 \n",
            "Epoch 32/168 | train_loss=0.3211 train_acc=0.9255 train_f1=0.3571 val_loss=0.2981 val_acc=0.9249 val_f1=0.3560 \n",
            "Epoch 48/168 | train_loss=0.2606 train_acc=0.9445 train_f1=0.3697 val_loss=0.2353 val_acc=0.9445 val_f1=0.3691 \n",
            "Epoch 64/168 | train_loss=0.2274 train_acc=0.9543 train_f1=0.4341 val_loss=0.1977 val_acc=0.9550 val_f1=0.4348 \n",
            "Epoch 80/168 | train_loss=0.2123 train_acc=0.9597 train_f1=0.4786 val_loss=0.1793 val_acc=0.9597 val_f1=0.4764 \n",
            "Epoch 96/168 | train_loss=0.2002 train_acc=0.9570 train_f1=0.4770 val_loss=0.1690 val_acc=0.9567 val_f1=0.4750 \n",
            "Epoch 112/168 | train_loss=0.1807 train_acc=0.9645 train_f1=0.5018 val_loss=0.1565 val_acc=0.9636 val_f1=0.4955 \n",
            "Epoch 128/168 | train_loss=0.1765 train_acc=0.9626 train_f1=0.5155 val_loss=0.1464 val_acc=0.9611 val_f1=0.5089 \n",
            "Epoch 144/168 | train_loss=0.1734 train_acc=0.9701 train_f1=0.5184 val_loss=0.1421 val_acc=0.9687 val_f1=0.5128 \n",
            "Epoch 160/168 | train_loss=0.1704 train_acc=0.9699 train_f1=0.5175 val_loss=0.1397 val_acc=0.9684 val_f1=0.5126 \n",
            "Gen 2 Best: {'epochs_run': 159, 'val_loss': 0.1745617389678955, 'val_accuracy': 0.9114307435746517, 'val_f1': 0.43917119867441234, 'flops': 5400}\n",
            "Generation 3 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.0498176045387215, epochs=122, hidden=(64, 64, 54), dropout_rate=0.0, patience=26)\n",
            "Epoch 12/122 | train_loss=0.4569 train_acc=0.6931 train_f1=0.2518 val_loss=0.4326 val_acc=0.6928 val_f1=0.2518 \n",
            "Epoch 24/122 | train_loss=0.2795 train_acc=0.8685 train_f1=0.3260 val_loss=0.2896 val_acc=0.8693 val_f1=0.3264 \n",
            "Epoch 36/122 | train_loss=0.2223 train_acc=0.9033 train_f1=0.3905 val_loss=0.2346 val_acc=0.9038 val_f1=0.3902 \n",
            "Epoch 48/122 | train_loss=0.1616 train_acc=0.9184 train_f1=0.4692 val_loss=0.1807 val_acc=0.9195 val_f1=0.4683 \n",
            "Epoch 60/122 | train_loss=0.1280 train_acc=0.9382 train_f1=0.5023 val_loss=0.1526 val_acc=0.9395 val_f1=0.5011 \n",
            "Epoch 72/122 | train_loss=0.1056 train_acc=0.9482 train_f1=0.5100 val_loss=0.1321 val_acc=0.9482 val_f1=0.5060 \n",
            "Epoch 84/122 | train_loss=0.1014 train_acc=0.9405 train_f1=0.5071 val_loss=0.1269 val_acc=0.9396 val_f1=0.5014 \n",
            "Epoch 96/122 | train_loss=0.0837 train_acc=0.9595 train_f1=0.5278 val_loss=0.1291 val_acc=0.9586 val_f1=0.5207 \n",
            "Epoch 108/122 | train_loss=0.0840 train_acc=0.9477 train_f1=0.5004 val_loss=0.1303 val_acc=0.9474 val_f1=0.4972 \n",
            "Epoch 120/122 | train_loss=0.0764 train_acc=0.9531 train_f1=0.5234 val_loss=0.1156 val_acc=0.9522 val_f1=0.5175 \n",
            "Evaluating member 1: Hyperparameters(lr=0.0037380326624547383, epochs=188, hidden=(110, 32), dropout_rate=0.17019138684199359, patience=30)\n",
            "Epoch 18/188 | train_loss=0.6365 train_acc=0.1014 train_f1=0.0639 val_loss=0.5995 val_acc=0.1005 val_f1=0.0636 \n",
            "Epoch 36/188 | train_loss=0.4449 train_acc=0.6740 train_f1=0.2447 val_loss=0.4115 val_acc=0.6723 val_f1=0.2444 \n",
            "Epoch 54/188 | train_loss=0.3309 train_acc=0.8786 train_f1=0.3312 val_loss=0.2973 val_acc=0.8785 val_f1=0.3308 \n",
            "Epoch 72/188 | train_loss=0.2860 train_acc=0.9012 train_f1=0.3612 val_loss=0.2590 val_acc=0.9011 val_f1=0.3616 \n",
            "Epoch 90/188 | train_loss=0.2606 train_acc=0.9153 train_f1=0.3881 val_loss=0.2370 val_acc=0.9160 val_f1=0.3903 \n",
            "Epoch 108/188 | train_loss=0.2394 train_acc=0.9242 train_f1=0.4095 val_loss=0.2194 val_acc=0.9251 val_f1=0.4121 \n",
            "Epoch 126/188 | train_loss=0.2261 train_acc=0.9305 train_f1=0.4274 val_loss=0.2048 val_acc=0.9306 val_f1=0.4270 \n",
            "Epoch 144/188 | train_loss=0.2136 train_acc=0.9363 train_f1=0.4425 val_loss=0.1930 val_acc=0.9359 val_f1=0.4408 \n",
            "Epoch 162/188 | train_loss=0.2042 train_acc=0.9405 train_f1=0.4528 val_loss=0.1836 val_acc=0.9407 val_f1=0.4510 \n",
            "Epoch 180/188 | train_loss=0.1951 train_acc=0.9453 train_f1=0.4652 val_loss=0.1758 val_acc=0.9450 val_f1=0.4636 \n",
            "Evaluating member 2: Hyperparameters(lr=0.003983721754385927, epochs=172, hidden=(143, 128), dropout_rate=0.18758475477585232, patience=27)\n",
            "Epoch 17/172 | train_loss=0.4768 train_acc=0.4675 train_f1=0.1770 val_loss=0.4602 val_acc=0.4695 val_f1=0.1777 \n",
            "Epoch 34/172 | train_loss=0.3088 train_acc=0.8851 train_f1=0.3343 val_loss=0.2913 val_acc=0.8850 val_f1=0.3341 \n",
            "Epoch 51/172 | train_loss=0.2563 train_acc=0.9071 train_f1=0.3856 val_loss=0.2415 val_acc=0.9080 val_f1=0.3839 \n",
            "Epoch 68/172 | train_loss=0.2210 train_acc=0.9259 train_f1=0.4287 val_loss=0.2097 val_acc=0.9259 val_f1=0.4279 \n",
            "Epoch 85/172 | train_loss=0.1999 train_acc=0.9352 train_f1=0.4559 val_loss=0.1880 val_acc=0.9352 val_f1=0.4546 \n",
            "Epoch 102/172 | train_loss=0.1801 train_acc=0.9396 train_f1=0.4684 val_loss=0.1720 val_acc=0.9387 val_f1=0.4656 \n",
            "Epoch 119/172 | train_loss=0.1665 train_acc=0.9448 train_f1=0.4851 val_loss=0.1587 val_acc=0.9435 val_f1=0.4800 \n",
            "Epoch 136/172 | train_loss=0.1572 train_acc=0.9470 train_f1=0.4951 val_loss=0.1479 val_acc=0.9455 val_f1=0.4902 \n",
            "Epoch 153/172 | train_loss=0.1464 train_acc=0.9504 train_f1=0.5034 val_loss=0.1399 val_acc=0.9492 val_f1=0.4987 \n",
            "Epoch 170/172 | train_loss=0.1395 train_acc=0.9517 train_f1=0.5094 val_loss=0.1332 val_acc=0.9507 val_f1=0.5056 \n",
            "Evaluating member 3: Hyperparameters(lr=0.08828113023782906, epochs=173, hidden=(137, 28), dropout_rate=0.06904210126324646, patience=22)\n",
            "Epoch 17/173 | train_loss=0.3206 train_acc=0.8636 train_f1=0.3558 val_loss=0.3085 val_acc=0.8635 val_f1=0.3560 \n",
            "Epoch 34/173 | train_loss=0.2387 train_acc=0.9161 train_f1=0.3998 val_loss=0.2326 val_acc=0.9169 val_f1=0.4002 \n",
            "Epoch 51/173 | train_loss=0.1984 train_acc=0.9301 train_f1=0.4519 val_loss=0.1889 val_acc=0.9302 val_f1=0.4497 \n",
            "Epoch 68/173 | train_loss=0.1674 train_acc=0.9406 train_f1=0.4749 val_loss=0.1599 val_acc=0.9414 val_f1=0.4748 \n",
            "Epoch 85/173 | train_loss=0.1474 train_acc=0.9455 train_f1=0.4926 val_loss=0.1419 val_acc=0.9446 val_f1=0.4872 \n",
            "Epoch 102/173 | train_loss=0.1337 train_acc=0.9497 train_f1=0.5065 val_loss=0.1308 val_acc=0.9488 val_f1=0.5027 \n",
            "Epoch 119/173 | train_loss=0.1242 train_acc=0.9570 train_f1=0.5166 val_loss=0.1244 val_acc=0.9567 val_f1=0.5128 \n",
            "Epoch 136/173 | train_loss=0.1170 train_acc=0.9593 train_f1=0.5229 val_loss=0.1170 val_acc=0.9587 val_f1=0.5192 \n",
            "Epoch 153/173 | train_loss=0.1114 train_acc=0.9596 train_f1=0.5258 val_loss=0.1139 val_acc=0.9590 val_f1=0.5216 \n",
            "Epoch 170/173 | train_loss=0.1061 train_acc=0.9570 train_f1=0.5249 val_loss=0.1096 val_acc=0.9563 val_f1=0.5207 \n",
            "Evaluating member 4: Hyperparameters(lr=0.04928889683893426, epochs=150, hidden=(16, 8, 41, 64, 8), dropout_rate=0.05924800987349301, patience=7)\n",
            "Epoch 15/150 | train_loss=0.5591 train_acc=0.0856 train_f1=0.0589 val_loss=0.5120 val_acc=0.0858 val_f1=0.0590 \n",
            "Epoch 30/150 | train_loss=0.3965 train_acc=0.8862 train_f1=0.3336 val_loss=0.3750 val_acc=0.8874 val_f1=0.3341 \n",
            "Epoch 45/150 | train_loss=0.3389 train_acc=0.9189 train_f1=0.3525 val_loss=0.3205 val_acc=0.9203 val_f1=0.3534 \n",
            "Epoch 60/150 | train_loss=0.3141 train_acc=0.9266 train_f1=0.3576 val_loss=0.2970 val_acc=0.9281 val_f1=0.3581 \n",
            "Epoch 75/150 | train_loss=0.3017 train_acc=0.9351 train_f1=0.3632 val_loss=0.2828 val_acc=0.9354 val_f1=0.3631 \n",
            "Epoch 90/150 | train_loss=0.2909 train_acc=0.9386 train_f1=0.3655 val_loss=0.2736 val_acc=0.9392 val_f1=0.3655 \n",
            "Epoch 105/150 | train_loss=0.2802 train_acc=0.9411 train_f1=0.3674 val_loss=0.2648 val_acc=0.9414 val_f1=0.3671 \n",
            "Epoch 120/150 | train_loss=0.2744 train_acc=0.9448 train_f1=0.3701 val_loss=0.2563 val_acc=0.9448 val_f1=0.3696 \n",
            "Epoch 135/150 | train_loss=0.2715 train_acc=0.9466 train_f1=0.3717 val_loss=0.2513 val_acc=0.9462 val_f1=0.3708 \n",
            "Epoch 150/150 | train_loss=0.2623 train_acc=0.9466 train_f1=0.3713 val_loss=0.2454 val_acc=0.9461 val_f1=0.3704 \n",
            "Evaluating member 5: Hyperparameters(lr=0.05056588938302735, epochs=147, hidden=(16, 8, 11, 64, 8), dropout_rate=0.15063183086549656, patience=14)\n",
            "Epoch 14/147 | train_loss=0.6691 train_acc=0.0676 train_f1=0.0532 val_loss=0.5734 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/147 | train_loss=0.4849 train_acc=0.8443 train_f1=0.3129 val_loss=0.4499 val_acc=0.8439 val_f1=0.3129 \n",
            "Epoch 42/147 | train_loss=0.4037 train_acc=0.9124 train_f1=0.3482 val_loss=0.3748 val_acc=0.9128 val_f1=0.3483 \n",
            "Epoch 56/147 | train_loss=0.3672 train_acc=0.9282 train_f1=0.3572 val_loss=0.3371 val_acc=0.9287 val_f1=0.3574 \n",
            "Epoch 70/147 | train_loss=0.3466 train_acc=0.9366 train_f1=0.3627 val_loss=0.3265 val_acc=0.9376 val_f1=0.3628 \n",
            "Epoch 84/147 | train_loss=0.3330 train_acc=0.9403 train_f1=0.3653 val_loss=0.3130 val_acc=0.9410 val_f1=0.3654 \n",
            "Epoch 98/147 | train_loss=0.3238 train_acc=0.9431 train_f1=0.3676 val_loss=0.3020 val_acc=0.9436 val_f1=0.3674 \n",
            "Epoch 112/147 | train_loss=0.3147 train_acc=0.9463 train_f1=0.3698 val_loss=0.2936 val_acc=0.9468 val_f1=0.3697 \n",
            "Epoch 126/147 | train_loss=0.3086 train_acc=0.9471 train_f1=0.3708 val_loss=0.2885 val_acc=0.9474 val_f1=0.3704 \n",
            "Epoch 140/147 | train_loss=0.3050 train_acc=0.9500 train_f1=0.3729 val_loss=0.2849 val_acc=0.9499 val_f1=0.3723 \n",
            "Evaluating member 6: Hyperparameters(lr=0.04476496318199434, epochs=143, hidden=(64, 32, 64, 256), dropout_rate=0.17394759818073777, patience=9)\n",
            "Epoch 14/143 | train_loss=0.7573 train_acc=0.0676 train_f1=0.0532 val_loss=0.7519 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 26\n",
            "Evaluating member 7: Hyperparameters(lr=0.046839588307688106, epochs=164, hidden=(52, 32, 64, 256), dropout_rate=0.061208766939200965, patience=9)\n",
            "Epoch 16/164 | train_loss=0.7358 train_acc=0.0676 train_f1=0.0532 val_loss=0.7296 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/164 | train_loss=0.7291 train_acc=0.0676 train_f1=0.0532 val_loss=0.7274 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 48/164 | train_loss=0.7271 train_acc=0.0676 train_f1=0.0532 val_loss=0.7268 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 50\n",
            "Evaluating member 8: Hyperparameters(lr=0.003737259937888613, epochs=188, hidden=(109, 128, 32), dropout_rate=0.27252231946079697, patience=30)\n",
            "Epoch 18/188 | train_loss=0.5813 train_acc=0.0713 train_f1=0.0544 val_loss=0.5422 val_acc=0.0712 val_f1=0.0544 \n",
            "Epoch 36/188 | train_loss=0.4005 train_acc=0.7427 train_f1=0.2699 val_loss=0.3563 val_acc=0.7433 val_f1=0.2703 \n",
            "Epoch 54/188 | train_loss=0.3106 train_acc=0.8921 train_f1=0.3382 val_loss=0.2767 val_acc=0.8928 val_f1=0.3382 \n",
            "Epoch 72/188 | train_loss=0.2724 train_acc=0.9149 train_f1=0.3510 val_loss=0.2424 val_acc=0.9147 val_f1=0.3507 \n",
            "Epoch 90/188 | train_loss=0.2444 train_acc=0.9279 train_f1=0.3795 val_loss=0.2145 val_acc=0.9271 val_f1=0.3777 \n",
            "Epoch 108/188 | train_loss=0.2209 train_acc=0.9368 train_f1=0.4252 val_loss=0.1913 val_acc=0.9367 val_f1=0.4240 \n",
            "Epoch 126/188 | train_loss=0.2001 train_acc=0.9421 train_f1=0.4582 val_loss=0.1748 val_acc=0.9417 val_f1=0.4567 \n",
            "Epoch 144/188 | train_loss=0.1881 train_acc=0.9457 train_f1=0.4783 val_loss=0.1601 val_acc=0.9447 val_f1=0.4757 \n",
            "Epoch 162/188 | train_loss=0.1749 train_acc=0.9465 train_f1=0.4838 val_loss=0.1501 val_acc=0.9457 val_f1=0.4818 \n",
            "Epoch 180/188 | train_loss=0.1619 train_acc=0.9499 train_f1=0.4929 val_loss=0.1415 val_acc=0.9494 val_f1=0.4901 \n",
            "Evaluating member 9: Hyperparameters(lr=0.09114420201971393, epochs=168, hidden=(119, 8), dropout_rate=0.11639836341337245, patience=25)\n",
            "Epoch 16/168 | train_loss=0.3638 train_acc=0.8651 train_f1=0.3233 val_loss=0.3409 val_acc=0.8653 val_f1=0.3236 \n",
            "Epoch 32/168 | train_loss=0.2710 train_acc=0.9023 train_f1=0.3448 val_loss=0.2491 val_acc=0.9041 val_f1=0.3455 \n",
            "Epoch 48/168 | train_loss=0.2316 train_acc=0.9346 train_f1=0.3856 val_loss=0.2129 val_acc=0.9357 val_f1=0.3856 \n",
            "Epoch 64/168 | train_loss=0.2040 train_acc=0.9381 train_f1=0.4378 val_loss=0.1841 val_acc=0.9387 val_f1=0.4379 \n",
            "Epoch 80/168 | train_loss=0.1837 train_acc=0.9435 train_f1=0.4642 val_loss=0.1660 val_acc=0.9435 val_f1=0.4651 \n",
            "Epoch 96/168 | train_loss=0.1695 train_acc=0.9476 train_f1=0.4868 val_loss=0.1515 val_acc=0.9478 val_f1=0.4852 \n",
            "Epoch 112/168 | train_loss=0.1572 train_acc=0.9539 train_f1=0.4989 val_loss=0.1423 val_acc=0.9533 val_f1=0.4948 \n",
            "Epoch 128/168 | train_loss=0.1480 train_acc=0.9536 train_f1=0.5035 val_loss=0.1338 val_acc=0.9528 val_f1=0.4993 \n",
            "Epoch 144/168 | train_loss=0.1405 train_acc=0.9539 train_f1=0.5015 val_loss=0.1283 val_acc=0.9532 val_f1=0.4981 \n",
            "Epoch 160/168 | train_loss=0.1337 train_acc=0.9549 train_f1=0.5093 val_loss=0.1248 val_acc=0.9541 val_f1=0.5054 \n",
            "Epoch 13/133 | train_loss=0.6746 train_acc=0.0676 train_f1=0.0532 val_loss=0.4977 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/133 | train_loss=0.5153 train_acc=0.8901 train_f1=0.3349 val_loss=0.4150 val_acc=0.8900 val_f1=0.3350 \n",
            "Epoch 39/133 | train_loss=0.3987 train_acc=0.8872 train_f1=0.3348 val_loss=0.3379 val_acc=0.8872 val_f1=0.3345 \n",
            "Epoch 52/133 | train_loss=0.3730 train_acc=0.9168 train_f1=0.3510 val_loss=0.3056 val_acc=0.9173 val_f1=0.3511 \n",
            "Epoch 65/133 | train_loss=0.3554 train_acc=0.9329 train_f1=0.3613 val_loss=0.2850 val_acc=0.9339 val_f1=0.3618 \n",
            "Epoch 78/133 | train_loss=0.3008 train_acc=0.9377 train_f1=0.3649 val_loss=0.2641 val_acc=0.9382 val_f1=0.3649 \n",
            "Epoch 91/133 | train_loss=0.2826 train_acc=0.9393 train_f1=0.3661 val_loss=0.2429 val_acc=0.9397 val_f1=0.3661 \n",
            "Epoch 104/133 | train_loss=0.2717 train_acc=0.9443 train_f1=0.4294 val_loss=0.2380 val_acc=0.9445 val_f1=0.4289 \n",
            "Epoch 117/133 | train_loss=0.2577 train_acc=0.9400 train_f1=0.4593 val_loss=0.2253 val_acc=0.9409 val_f1=0.4598 \n",
            "Epoch 130/133 | train_loss=0.2500 train_acc=0.9447 train_f1=0.4760 val_loss=0.2199 val_acc=0.9456 val_f1=0.4766 \n",
            "Epoch 16/168 | train_loss=0.4671 train_acc=0.7124 train_f1=0.2585 val_loss=0.3968 val_acc=0.7126 val_f1=0.2588 \n",
            "Epoch 32/168 | train_loss=0.3342 train_acc=0.9139 train_f1=0.3501 val_loss=0.2946 val_acc=0.9138 val_f1=0.3497 \n",
            "Epoch 48/168 | train_loss=0.2907 train_acc=0.9224 train_f1=0.3553 val_loss=0.2615 val_acc=0.9222 val_f1=0.3551 \n",
            "Epoch 64/168 | train_loss=0.2652 train_acc=0.9288 train_f1=0.3595 val_loss=0.2356 val_acc=0.9287 val_f1=0.3593 \n",
            "Epoch 80/168 | train_loss=0.2477 train_acc=0.9299 train_f1=0.4065 val_loss=0.2103 val_acc=0.9291 val_f1=0.4059 \n",
            "Epoch 96/168 | train_loss=0.2254 train_acc=0.9400 train_f1=0.4399 val_loss=0.1919 val_acc=0.9391 val_f1=0.4369 \n",
            "Epoch 112/168 | train_loss=0.2142 train_acc=0.9438 train_f1=0.4742 val_loss=0.1791 val_acc=0.9438 val_f1=0.4733 \n",
            "Epoch 128/168 | train_loss=0.2053 train_acc=0.9455 train_f1=0.4774 val_loss=0.1724 val_acc=0.9454 val_f1=0.4756 \n",
            "Epoch 144/168 | train_loss=0.2016 train_acc=0.9486 train_f1=0.4744 val_loss=0.1681 val_acc=0.9485 val_f1=0.4718 \n",
            "Epoch 160/168 | train_loss=0.1933 train_acc=0.9474 train_f1=0.4871 val_loss=0.1650 val_acc=0.9469 val_f1=0.4855 \n",
            "Epoch 16/169 | train_loss=0.7312 train_acc=0.0676 train_f1=0.0532 val_loss=0.7303 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 24\n",
            "Epoch 14/145 | train_loss=0.5848 train_acc=0.7928 train_f1=0.2870 val_loss=0.4293 val_acc=0.7959 val_f1=0.2893 \n",
            "Epoch 28/145 | train_loss=0.3640 train_acc=0.8767 train_f1=0.3292 val_loss=0.3209 val_acc=0.8769 val_f1=0.3292 \n",
            "Epoch 42/145 | train_loss=0.3185 train_acc=0.8826 train_f1=0.3337 val_loss=0.2837 val_acc=0.8836 val_f1=0.3339 \n",
            "Epoch 56/145 | train_loss=0.2929 train_acc=0.9175 train_f1=0.3535 val_loss=0.2603 val_acc=0.9178 val_f1=0.3533 \n",
            "Epoch 70/145 | train_loss=0.2715 train_acc=0.9284 train_f1=0.3676 val_loss=0.2401 val_acc=0.9281 val_f1=0.3663 \n",
            "Epoch 84/145 | train_loss=0.2602 train_acc=0.9254 train_f1=0.4326 val_loss=0.2250 val_acc=0.9258 val_f1=0.4338 \n",
            "Epoch 98/145 | train_loss=0.2476 train_acc=0.9335 train_f1=0.4463 val_loss=0.2153 val_acc=0.9331 val_f1=0.4451 \n",
            "Epoch 112/145 | train_loss=0.2359 train_acc=0.9399 train_f1=0.4557 val_loss=0.2050 val_acc=0.9401 val_f1=0.4550 \n",
            "Epoch 126/145 | train_loss=0.2257 train_acc=0.9409 train_f1=0.4656 val_loss=0.1923 val_acc=0.9402 val_f1=0.4629 \n",
            "Epoch 140/145 | train_loss=0.2092 train_acc=0.9471 train_f1=0.4803 val_loss=0.1830 val_acc=0.9475 val_f1=0.4783 \n",
            "Epoch 17/172 | train_loss=0.3456 train_acc=0.8816 train_f1=0.3547 val_loss=0.3169 val_acc=0.8811 val_f1=0.3535 \n",
            "Epoch 34/172 | train_loss=0.2621 train_acc=0.8915 train_f1=0.3941 val_loss=0.2515 val_acc=0.8917 val_f1=0.3950 \n",
            "Epoch 51/172 | train_loss=0.2309 train_acc=0.9176 train_f1=0.4273 val_loss=0.2223 val_acc=0.9173 val_f1=0.4254 \n",
            "Epoch 68/172 | train_loss=0.2137 train_acc=0.9291 train_f1=0.4444 val_loss=0.2055 val_acc=0.9295 val_f1=0.4436 \n",
            "Epoch 85/172 | train_loss=0.2012 train_acc=0.9353 train_f1=0.4558 val_loss=0.1931 val_acc=0.9356 val_f1=0.4557 \n",
            "Epoch 102/172 | train_loss=0.1915 train_acc=0.9387 train_f1=0.4661 val_loss=0.1843 val_acc=0.9386 val_f1=0.4633 \n",
            "Epoch 119/172 | train_loss=0.1839 train_acc=0.9427 train_f1=0.4757 val_loss=0.1759 val_acc=0.9415 val_f1=0.4700 \n",
            "Epoch 136/172 | train_loss=0.1799 train_acc=0.9448 train_f1=0.4796 val_loss=0.1696 val_acc=0.9441 val_f1=0.4760 \n",
            "Epoch 153/172 | train_loss=0.1740 train_acc=0.9443 train_f1=0.4859 val_loss=0.1648 val_acc=0.9434 val_f1=0.4798 \n",
            "Epoch 170/172 | train_loss=0.1704 train_acc=0.9472 train_f1=0.4870 val_loss=0.1618 val_acc=0.9463 val_f1=0.4816 \n",
            "Epoch 19/193 | train_loss=0.6303 train_acc=0.0676 train_f1=0.0532 val_loss=0.5787 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/193 | train_loss=0.4302 train_acc=0.7212 train_f1=0.2620 val_loss=0.3830 val_acc=0.7210 val_f1=0.2621 \n",
            "Epoch 57/193 | train_loss=0.3176 train_acc=0.8943 train_f1=0.3394 val_loss=0.2769 val_acc=0.8948 val_f1=0.3397 \n",
            "Epoch 76/193 | train_loss=0.2772 train_acc=0.9165 train_f1=0.3524 val_loss=0.2469 val_acc=0.9165 val_f1=0.3522 \n",
            "Epoch 95/193 | train_loss=0.2498 train_acc=0.9311 train_f1=0.3786 val_loss=0.2218 val_acc=0.9310 val_f1=0.3782 \n",
            "Epoch 114/193 | train_loss=0.2286 train_acc=0.9381 train_f1=0.4254 val_loss=0.1985 val_acc=0.9371 val_f1=0.4234 \n",
            "Epoch 133/193 | train_loss=0.2080 train_acc=0.9429 train_f1=0.4601 val_loss=0.1786 val_acc=0.9422 val_f1=0.4577 \n",
            "Epoch 152/193 | train_loss=0.1927 train_acc=0.9455 train_f1=0.4761 val_loss=0.1636 val_acc=0.9446 val_f1=0.4722 \n",
            "Epoch 171/193 | train_loss=0.1790 train_acc=0.9485 train_f1=0.4894 val_loss=0.1516 val_acc=0.9478 val_f1=0.4862 \n",
            "Epoch 190/193 | train_loss=0.1677 train_acc=0.9506 train_f1=0.4956 val_loss=0.1424 val_acc=0.9500 val_f1=0.4914 \n",
            "Epoch 13/134 | train_loss=0.5994 train_acc=0.7091 train_f1=0.2567 val_loss=0.4956 val_acc=0.7090 val_f1=0.2566 \n",
            "Epoch 26/134 | train_loss=0.3208 train_acc=0.8451 train_f1=0.3150 val_loss=0.3183 val_acc=0.8455 val_f1=0.3149 \n",
            "Epoch 39/134 | train_loss=0.2455 train_acc=0.8894 train_f1=0.3823 val_loss=0.2435 val_acc=0.8912 val_f1=0.3840 \n",
            "Epoch 52/134 | train_loss=0.1906 train_acc=0.9126 train_f1=0.4492 val_loss=0.1914 val_acc=0.9131 val_f1=0.4468 \n",
            "Epoch 65/134 | train_loss=0.1532 train_acc=0.9325 train_f1=0.4845 val_loss=0.1616 val_acc=0.9334 val_f1=0.4823 \n",
            "Epoch 78/134 | train_loss=0.1301 train_acc=0.9481 train_f1=0.5053 val_loss=0.1428 val_acc=0.9481 val_f1=0.5012 \n",
            "Epoch 91/134 | train_loss=0.1166 train_acc=0.9486 train_f1=0.5095 val_loss=0.1278 val_acc=0.9482 val_f1=0.5042 \n",
            "Epoch 104/134 | train_loss=0.1047 train_acc=0.9581 train_f1=0.5247 val_loss=0.1220 val_acc=0.9580 val_f1=0.5200 \n",
            "Epoch 117/134 | train_loss=0.0988 train_acc=0.9625 train_f1=0.5335 val_loss=0.1278 val_acc=0.9614 val_f1=0.5281 \n",
            "Epoch 130/134 | train_loss=0.0923 train_acc=0.9556 train_f1=0.5262 val_loss=0.1149 val_acc=0.9549 val_f1=0.5205 \n",
            "Epoch 18/182 | train_loss=0.5594 train_acc=0.0676 train_f1=0.0532 val_loss=0.5409 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/182 | train_loss=0.3893 train_acc=0.8144 train_f1=0.3002 val_loss=0.3487 val_acc=0.8143 val_f1=0.3004 \n",
            "Epoch 54/182 | train_loss=0.2960 train_acc=0.9038 train_f1=0.3451 val_loss=0.2670 val_acc=0.9035 val_f1=0.3447 \n",
            "Epoch 72/182 | train_loss=0.2604 train_acc=0.9243 train_f1=0.3807 val_loss=0.2340 val_acc=0.9247 val_f1=0.3823 \n",
            "Epoch 90/182 | train_loss=0.2338 train_acc=0.9352 train_f1=0.4354 val_loss=0.2055 val_acc=0.9358 val_f1=0.4371 \n",
            "Epoch 108/182 | train_loss=0.2141 train_acc=0.9436 train_f1=0.4633 val_loss=0.1877 val_acc=0.9433 val_f1=0.4622 \n",
            "Epoch 126/182 | train_loss=0.1985 train_acc=0.9481 train_f1=0.4753 val_loss=0.1743 val_acc=0.9478 val_f1=0.4736 \n",
            "Epoch 144/182 | train_loss=0.1859 train_acc=0.9508 train_f1=0.4886 val_loss=0.1647 val_acc=0.9509 val_f1=0.4861 \n",
            "Epoch 162/182 | train_loss=0.1736 train_acc=0.9524 train_f1=0.4969 val_loss=0.1586 val_acc=0.9524 val_f1=0.4935 \n",
            "Epoch 180/182 | train_loss=0.1704 train_acc=0.9535 train_f1=0.5031 val_loss=0.1529 val_acc=0.9529 val_f1=0.4990 \n",
            "Epoch 13/133 | train_loss=0.5261 train_acc=0.0676 train_f1=0.0532 val_loss=0.4674 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/133 | train_loss=0.3705 train_acc=0.8787 train_f1=0.3307 val_loss=0.3248 val_acc=0.8793 val_f1=0.3307 \n",
            "Epoch 39/133 | train_loss=0.3272 train_acc=0.9107 train_f1=0.3481 val_loss=0.2972 val_acc=0.9114 val_f1=0.3480 \n",
            "Epoch 52/133 | train_loss=0.3055 train_acc=0.9255 train_f1=0.3572 val_loss=0.2867 val_acc=0.9260 val_f1=0.3572 \n",
            "Epoch 65/133 | train_loss=0.2932 train_acc=0.9350 train_f1=0.3634 val_loss=0.2664 val_acc=0.9353 val_f1=0.3633 \n",
            "Epoch 78/133 | train_loss=0.2774 train_acc=0.9327 train_f1=0.3627 val_loss=0.2512 val_acc=0.9326 val_f1=0.3625 \n",
            "Epoch 91/133 | train_loss=0.2687 train_acc=0.9381 train_f1=0.3661 val_loss=0.2473 val_acc=0.9382 val_f1=0.3660 \n",
            "Epoch 104/133 | train_loss=0.2587 train_acc=0.9404 train_f1=0.3680 val_loss=0.2350 val_acc=0.9395 val_f1=0.3673 \n",
            "Epoch 117/133 | train_loss=0.2509 train_acc=0.9411 train_f1=0.3685 val_loss=0.2227 val_acc=0.9406 val_f1=0.3678 \n",
            "Epoch 130/133 | train_loss=0.2474 train_acc=0.9405 train_f1=0.3676 val_loss=0.2168 val_acc=0.9402 val_f1=0.3672 \n",
            "Epoch 16/168 | train_loss=0.5068 train_acc=0.6801 train_f1=0.2462 val_loss=0.4794 val_acc=0.6795 val_f1=0.2463 \n",
            "Epoch 32/168 | train_loss=0.2936 train_acc=0.8952 train_f1=0.3392 val_loss=0.2894 val_acc=0.8962 val_f1=0.3396 \n",
            "Epoch 48/168 | train_loss=0.2495 train_acc=0.9314 train_f1=0.3610 val_loss=0.2532 val_acc=0.9311 val_f1=0.3604 \n",
            "Epoch 64/168 | train_loss=0.2168 train_acc=0.9343 train_f1=0.4483 val_loss=0.2219 val_acc=0.9348 val_f1=0.4499 \n",
            "Epoch 80/168 | train_loss=0.1929 train_acc=0.9289 train_f1=0.4683 val_loss=0.1858 val_acc=0.9281 val_f1=0.4673 \n",
            "Epoch 96/168 | train_loss=0.1649 train_acc=0.9424 train_f1=0.4887 val_loss=0.1690 val_acc=0.9421 val_f1=0.4854 \n",
            "Epoch 112/168 | train_loss=0.1530 train_acc=0.9523 train_f1=0.5034 val_loss=0.1614 val_acc=0.9524 val_f1=0.4999 \n",
            "Epoch 128/168 | train_loss=0.1688 train_acc=0.9620 train_f1=0.4976 val_loss=0.1781 val_acc=0.9620 val_f1=0.4967 \n",
            "Early stopping at epoch 128\n",
            "Gen 3 Best: {'epochs_run': 122, 'val_loss': 0.1290973275899887, 'val_accuracy': 0.959988718854228, 'val_f1': 0.5253351561497672, 'flops': 9870}\n",
            "Generation 4 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.04749071862741443, epochs=133, hidden=(64, 32, 64, 256), dropout_rate=0.21680934376801636, patience=10)\n",
            "Epoch 13/133 | train_loss=0.7970 train_acc=0.0676 train_f1=0.0532 val_loss=0.7538 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/133 | train_loss=0.7364 train_acc=0.0676 train_f1=0.0532 val_loss=0.7343 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 39/133 | train_loss=0.7284 train_acc=0.0676 train_f1=0.0532 val_loss=0.7269 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 52/133 | train_loss=0.7279 train_acc=0.0676 train_f1=0.0532 val_loss=0.7267 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 56\n",
            "Evaluating member 1: Hyperparameters(lr=0.048408966893673334, epochs=168, hidden=(29, 32, 64, 256), dropout_rate=0.1377008240370332, patience=12)\n",
            "Epoch 16/168 | train_loss=0.7345 train_acc=0.0676 train_f1=0.0532 val_loss=0.7183 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/168 | train_loss=0.5501 train_acc=0.7929 train_f1=0.2877 val_loss=0.4702 val_acc=0.7931 val_f1=0.2882 \n",
            "Epoch 48/168 | train_loss=0.4697 train_acc=0.8950 train_f1=0.3381 val_loss=0.3663 val_acc=0.8948 val_f1=0.3379 \n",
            "Epoch 64/168 | train_loss=0.4348 train_acc=0.9047 train_f1=0.3439 val_loss=0.3279 val_acc=0.9049 val_f1=0.3436 \n",
            "Epoch 80/168 | train_loss=0.4075 train_acc=0.9210 train_f1=0.3532 val_loss=0.2985 val_acc=0.9216 val_f1=0.3534 \n",
            "Epoch 96/168 | train_loss=0.3541 train_acc=0.9198 train_f1=0.4292 val_loss=0.2561 val_acc=0.9216 val_f1=0.4303 \n",
            "Epoch 112/168 | train_loss=0.3246 train_acc=0.9293 train_f1=0.4415 val_loss=0.2469 val_acc=0.9294 val_f1=0.4413 \n",
            "Epoch 128/168 | train_loss=0.2900 train_acc=0.9441 train_f1=0.4632 val_loss=0.2237 val_acc=0.9444 val_f1=0.4622 \n",
            "Epoch 144/168 | train_loss=0.2703 train_acc=0.9471 train_f1=0.4752 val_loss=0.2182 val_acc=0.9480 val_f1=0.4757 \n",
            "Epoch 160/168 | train_loss=0.2605 train_acc=0.9448 train_f1=0.4840 val_loss=0.2103 val_acc=0.9459 val_f1=0.4831 \n",
            "Evaluating member 2: Hyperparameters(lr=0.046662938503347004, epochs=169, hidden=(52, 32, 64, 250), dropout_rate=0.16799177905744944, patience=9)\n",
            "Epoch 16/169 | train_loss=0.7330 train_acc=0.0676 train_f1=0.0532 val_loss=0.7267 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 25\n",
            "Evaluating member 3: Hyperparameters(lr=0.04011731912625363, epochs=145, hidden=(39, 32, 64, 256), dropout_rate=0.15894954067805872, patience=7)\n",
            "Epoch 14/145 | train_loss=0.5538 train_acc=0.7570 train_f1=0.2711 val_loss=0.4799 val_acc=0.7559 val_f1=0.2711 \n",
            "Epoch 28/145 | train_loss=0.3470 train_acc=0.8716 train_f1=0.3267 val_loss=0.3184 val_acc=0.8732 val_f1=0.3274 \n",
            "Epoch 42/145 | train_loss=0.3077 train_acc=0.9127 train_f1=0.3488 val_loss=0.2864 val_acc=0.9133 val_f1=0.3489 \n",
            "Epoch 56/145 | train_loss=0.2821 train_acc=0.9140 train_f1=0.3504 val_loss=0.2488 val_acc=0.9143 val_f1=0.3502 \n",
            "Epoch 70/145 | train_loss=0.2560 train_acc=0.9349 train_f1=0.4351 val_loss=0.2240 val_acc=0.9348 val_f1=0.4346 \n",
            "Epoch 84/145 | train_loss=0.2303 train_acc=0.9387 train_f1=0.4724 val_loss=0.2014 val_acc=0.9381 val_f1=0.4710 \n",
            "Epoch 98/145 | train_loss=0.2167 train_acc=0.9448 train_f1=0.4794 val_loss=0.1890 val_acc=0.9442 val_f1=0.4764 \n",
            "Epoch 112/145 | train_loss=0.2047 train_acc=0.9455 train_f1=0.4855 val_loss=0.1737 val_acc=0.9449 val_f1=0.4829 \n",
            "Epoch 126/145 | train_loss=0.1932 train_acc=0.9470 train_f1=0.4909 val_loss=0.1619 val_acc=0.9461 val_f1=0.4871 \n",
            "Epoch 140/145 | train_loss=0.1851 train_acc=0.9506 train_f1=0.5056 val_loss=0.1596 val_acc=0.9500 val_f1=0.5024 \n",
            "Evaluating member 4: Hyperparameters(lr=0.09122175903398319, epochs=172, hidden=(28,), dropout_rate=0.16742460785149363, patience=21)\n",
            "Epoch 17/172 | train_loss=0.3527 train_acc=0.8677 train_f1=0.3258 val_loss=0.3212 val_acc=0.8676 val_f1=0.3249 \n",
            "Epoch 34/172 | train_loss=0.2577 train_acc=0.8957 train_f1=0.3915 val_loss=0.2493 val_acc=0.8970 val_f1=0.3938 \n",
            "Epoch 51/172 | train_loss=0.2259 train_acc=0.9183 train_f1=0.4270 val_loss=0.2158 val_acc=0.9192 val_f1=0.4270 \n",
            "Epoch 68/172 | train_loss=0.2095 train_acc=0.9320 train_f1=0.4473 val_loss=0.2010 val_acc=0.9325 val_f1=0.4456 \n",
            "Epoch 85/172 | train_loss=0.1994 train_acc=0.9387 train_f1=0.4607 val_loss=0.1899 val_acc=0.9381 val_f1=0.4567 \n",
            "Epoch 102/172 | train_loss=0.1893 train_acc=0.9420 train_f1=0.4656 val_loss=0.1806 val_acc=0.9412 val_f1=0.4617 \n",
            "Epoch 119/172 | train_loss=0.1833 train_acc=0.9457 train_f1=0.4751 val_loss=0.1751 val_acc=0.9449 val_f1=0.4725 \n",
            "Epoch 136/172 | train_loss=0.1818 train_acc=0.9452 train_f1=0.4774 val_loss=0.1707 val_acc=0.9444 val_f1=0.4745 \n",
            "Epoch 153/172 | train_loss=0.1770 train_acc=0.9475 train_f1=0.4813 val_loss=0.1667 val_acc=0.9465 val_f1=0.4782 \n",
            "Epoch 170/172 | train_loss=0.1734 train_acc=0.9469 train_f1=0.4806 val_loss=0.1643 val_acc=0.9456 val_f1=0.4786 \n",
            "Evaluating member 5: Hyperparameters(lr=0.003349774902219666, epochs=193, hidden=(109, 112, 32), dropout_rate=0.2348435768000065, patience=29)\n",
            "Epoch 19/193 | train_loss=0.6280 train_acc=0.5124 train_f1=0.1911 val_loss=0.5683 val_acc=0.5122 val_f1=0.1912 \n",
            "Epoch 38/193 | train_loss=0.3825 train_acc=0.8045 train_f1=0.2958 val_loss=0.3366 val_acc=0.8059 val_f1=0.2965 \n",
            "Epoch 57/193 | train_loss=0.3005 train_acc=0.9012 train_f1=0.3433 val_loss=0.2662 val_acc=0.9022 val_f1=0.3436 \n",
            "Epoch 76/193 | train_loss=0.2645 train_acc=0.9219 train_f1=0.3603 val_loss=0.2350 val_acc=0.9218 val_f1=0.3601 \n",
            "Epoch 95/193 | train_loss=0.2330 train_acc=0.9293 train_f1=0.4147 val_loss=0.2046 val_acc=0.9292 val_f1=0.4144 \n",
            "Epoch 114/193 | train_loss=0.2082 train_acc=0.9379 train_f1=0.4558 val_loss=0.1811 val_acc=0.9379 val_f1=0.4546 \n",
            "Epoch 133/193 | train_loss=0.1873 train_acc=0.9411 train_f1=0.4745 val_loss=0.1636 val_acc=0.9403 val_f1=0.4708 \n",
            "Epoch 152/193 | train_loss=0.1722 train_acc=0.9446 train_f1=0.4924 val_loss=0.1494 val_acc=0.9436 val_f1=0.4882 \n",
            "Epoch 171/193 | train_loss=0.1604 train_acc=0.9476 train_f1=0.5033 val_loss=0.1383 val_acc=0.9468 val_f1=0.4999 \n",
            "Epoch 190/193 | train_loss=0.1481 train_acc=0.9503 train_f1=0.5114 val_loss=0.1296 val_acc=0.9495 val_f1=0.5089 \n",
            "Evaluating member 6: Hyperparameters(lr=0.05327927900858706, epochs=134, hidden=(64, 57, 54), dropout_rate=0.013679089183122398, patience=20)\n",
            "Epoch 13/134 | train_loss=0.5542 train_acc=0.0676 train_f1=0.0532 val_loss=0.5247 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/134 | train_loss=0.3152 train_acc=0.8717 train_f1=0.3270 val_loss=0.3038 val_acc=0.8724 val_f1=0.3274 \n",
            "Epoch 39/134 | train_loss=0.2510 train_acc=0.9143 train_f1=0.4128 val_loss=0.2470 val_acc=0.9146 val_f1=0.4135 \n",
            "Epoch 52/134 | train_loss=0.2025 train_acc=0.9291 train_f1=0.4474 val_loss=0.2056 val_acc=0.9293 val_f1=0.4454 \n",
            "Epoch 65/134 | train_loss=0.1786 train_acc=0.9394 train_f1=0.4742 val_loss=0.1837 val_acc=0.9397 val_f1=0.4719 \n",
            "Epoch 78/134 | train_loss=0.1582 train_acc=0.9413 train_f1=0.4880 val_loss=0.1624 val_acc=0.9407 val_f1=0.4829 \n",
            "Epoch 91/134 | train_loss=0.1355 train_acc=0.9465 train_f1=0.5029 val_loss=0.1433 val_acc=0.9461 val_f1=0.4998 \n",
            "Epoch 104/134 | train_loss=0.1203 train_acc=0.9530 train_f1=0.5154 val_loss=0.1317 val_acc=0.9524 val_f1=0.5114 \n",
            "Epoch 117/134 | train_loss=0.1092 train_acc=0.9586 train_f1=0.5250 val_loss=0.1256 val_acc=0.9578 val_f1=0.5199 \n",
            "Epoch 130/134 | train_loss=0.1050 train_acc=0.9561 train_f1=0.5249 val_loss=0.1220 val_acc=0.9552 val_f1=0.5189 \n",
            "Evaluating member 7: Hyperparameters(lr=0.00398953294911611, epochs=182, hidden=(32, 143, 128), dropout_rate=0.20245828047596787, patience=27)\n",
            "Epoch 18/182 | train_loss=0.5440 train_acc=0.0676 train_f1=0.0532 val_loss=0.5256 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/182 | train_loss=0.3830 train_acc=0.8172 train_f1=0.3013 val_loss=0.3493 val_acc=0.8171 val_f1=0.3012 \n",
            "Epoch 54/182 | train_loss=0.2936 train_acc=0.9115 train_f1=0.3489 val_loss=0.2668 val_acc=0.9120 val_f1=0.3491 \n",
            "Epoch 72/182 | train_loss=0.2586 train_acc=0.9236 train_f1=0.3752 val_loss=0.2350 val_acc=0.9234 val_f1=0.3737 \n",
            "Epoch 90/182 | train_loss=0.2332 train_acc=0.9350 train_f1=0.4252 val_loss=0.2063 val_acc=0.9351 val_f1=0.4250 \n",
            "Epoch 108/182 | train_loss=0.2100 train_acc=0.9429 train_f1=0.4594 val_loss=0.1845 val_acc=0.9427 val_f1=0.4558 \n",
            "Epoch 126/182 | train_loss=0.1911 train_acc=0.9466 train_f1=0.4749 val_loss=0.1708 val_acc=0.9463 val_f1=0.4717 \n",
            "Epoch 144/182 | train_loss=0.1822 train_acc=0.9472 train_f1=0.4880 val_loss=0.1597 val_acc=0.9468 val_f1=0.4846 \n",
            "Epoch 162/182 | train_loss=0.1699 train_acc=0.9514 train_f1=0.4987 val_loss=0.1521 val_acc=0.9508 val_f1=0.4951 \n",
            "Epoch 180/182 | train_loss=0.1653 train_acc=0.9525 train_f1=0.5049 val_loss=0.1449 val_acc=0.9521 val_f1=0.5019 \n",
            "Evaluating member 8: Hyperparameters(lr=0.05367754604352836, epochs=133, hidden=(16, 8, 11, 64, 8), dropout_rate=0.13027519414525632, patience=17)\n",
            "Epoch 13/133 | train_loss=0.6753 train_acc=0.0676 train_f1=0.0532 val_loss=0.5814 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 26/133 | train_loss=0.5010 train_acc=0.8203 train_f1=0.3008 val_loss=0.4582 val_acc=0.8200 val_f1=0.3010 \n",
            "Epoch 39/133 | train_loss=0.4039 train_acc=0.9064 train_f1=0.3446 val_loss=0.3340 val_acc=0.9067 val_f1=0.3445 \n",
            "Epoch 52/133 | train_loss=0.3765 train_acc=0.9143 train_f1=0.3491 val_loss=0.3219 val_acc=0.9158 val_f1=0.3498 \n",
            "Epoch 65/133 | train_loss=0.3559 train_acc=0.9266 train_f1=0.3568 val_loss=0.3097 val_acc=0.9276 val_f1=0.3573 \n",
            "Epoch 78/133 | train_loss=0.3435 train_acc=0.9299 train_f1=0.3594 val_loss=0.2953 val_acc=0.9298 val_f1=0.3592 \n",
            "Epoch 91/133 | train_loss=0.3366 train_acc=0.9349 train_f1=0.3625 val_loss=0.2875 val_acc=0.9354 val_f1=0.3627 \n",
            "Epoch 104/133 | train_loss=0.3302 train_acc=0.9394 train_f1=0.3655 val_loss=0.2834 val_acc=0.9396 val_f1=0.3655 \n",
            "Epoch 117/133 | train_loss=0.3272 train_acc=0.9385 train_f1=0.3650 val_loss=0.2771 val_acc=0.9387 val_f1=0.3648 \n",
            "Epoch 130/133 | train_loss=0.3184 train_acc=0.9382 train_f1=0.3649 val_loss=0.2721 val_acc=0.9383 val_f1=0.3649 \n",
            "Evaluating member 9: Hyperparameters(lr=0.04666448641284696, epochs=168, hidden=(52, 32, 56, 256), dropout_rate=0.006145750989779507, patience=8)\n",
            "Epoch 16/168 | train_loss=0.5267 train_acc=0.7327 train_f1=0.2616 val_loss=0.4945 val_acc=0.7329 val_f1=0.2626 \n",
            "Epoch 32/168 | train_loss=0.3241 train_acc=0.9047 train_f1=0.3446 val_loss=0.3061 val_acc=0.9055 val_f1=0.3450 \n",
            "Epoch 48/168 | train_loss=0.2737 train_acc=0.9205 train_f1=0.3542 val_loss=0.2695 val_acc=0.9215 val_f1=0.3544 \n",
            "Epoch 64/168 | train_loss=0.2438 train_acc=0.9252 train_f1=0.3689 val_loss=0.2462 val_acc=0.9257 val_f1=0.3668 \n",
            "Epoch 80/168 | train_loss=0.2076 train_acc=0.9251 train_f1=0.4422 val_loss=0.2100 val_acc=0.9255 val_f1=0.4398 \n",
            "Epoch 96/168 | train_loss=0.1842 train_acc=0.9360 train_f1=0.4674 val_loss=0.1863 val_acc=0.9355 val_f1=0.4655 \n",
            "Epoch 112/168 | train_loss=0.1635 train_acc=0.9418 train_f1=0.4861 val_loss=0.1699 val_acc=0.9411 val_f1=0.4831 \n",
            "Epoch 128/168 | train_loss=0.1410 train_acc=0.9478 train_f1=0.4999 val_loss=0.1485 val_acc=0.9474 val_f1=0.4969 \n",
            "Early stopping at epoch 143\n",
            "Epoch 17/175 | train_loss=0.5531 train_acc=0.0738 train_f1=0.0552 val_loss=0.5239 val_acc=0.0738 val_f1=0.0552 \n",
            "Epoch 34/175 | train_loss=0.3866 train_acc=0.8443 train_f1=0.3135 val_loss=0.3354 val_acc=0.8448 val_f1=0.3140 \n",
            "Epoch 51/175 | train_loss=0.3068 train_acc=0.8907 train_f1=0.3376 val_loss=0.2728 val_acc=0.8918 val_f1=0.3378 \n",
            "Epoch 68/175 | train_loss=0.2759 train_acc=0.9146 train_f1=0.3509 val_loss=0.2470 val_acc=0.9147 val_f1=0.3507 \n",
            "Epoch 85/175 | train_loss=0.2502 train_acc=0.9270 train_f1=0.3879 val_loss=0.2229 val_acc=0.9272 val_f1=0.3870 \n",
            "Epoch 102/175 | train_loss=0.2304 train_acc=0.9364 train_f1=0.4417 val_loss=0.2007 val_acc=0.9367 val_f1=0.4429 \n",
            "Epoch 119/175 | train_loss=0.2210 train_acc=0.9419 train_f1=0.4660 val_loss=0.1876 val_acc=0.9420 val_f1=0.4651 \n",
            "Epoch 136/175 | train_loss=0.2071 train_acc=0.9440 train_f1=0.4749 val_loss=0.1784 val_acc=0.9438 val_f1=0.4729 \n",
            "Epoch 153/175 | train_loss=0.1976 train_acc=0.9475 train_f1=0.4833 val_loss=0.1711 val_acc=0.9474 val_f1=0.4815 \n",
            "Epoch 170/175 | train_loss=0.1918 train_acc=0.9473 train_f1=0.4883 val_loss=0.1645 val_acc=0.9473 val_f1=0.4864 \n",
            "Epoch 18/183 | train_loss=0.5490 train_acc=0.0677 train_f1=0.0532 val_loss=0.5282 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/183 | train_loss=0.3625 train_acc=0.8102 train_f1=0.2977 val_loss=0.3361 val_acc=0.8113 val_f1=0.2985 \n",
            "Epoch 54/183 | train_loss=0.2771 train_acc=0.9022 train_f1=0.3450 val_loss=0.2603 val_acc=0.9029 val_f1=0.3448 \n",
            "Epoch 72/183 | train_loss=0.2373 train_acc=0.9273 train_f1=0.4018 val_loss=0.2221 val_acc=0.9274 val_f1=0.4028 \n",
            "Epoch 90/183 | train_loss=0.2046 train_acc=0.9387 train_f1=0.4468 val_loss=0.1932 val_acc=0.9384 val_f1=0.4450 \n",
            "Epoch 108/183 | train_loss=0.1811 train_acc=0.9425 train_f1=0.4688 val_loss=0.1708 val_acc=0.9425 val_f1=0.4661 \n",
            "Epoch 126/183 | train_loss=0.1630 train_acc=0.9474 train_f1=0.4857 val_loss=0.1543 val_acc=0.9473 val_f1=0.4824 \n",
            "Epoch 144/183 | train_loss=0.1506 train_acc=0.9498 train_f1=0.4990 val_loss=0.1416 val_acc=0.9498 val_f1=0.4953 \n",
            "Epoch 162/183 | train_loss=0.1396 train_acc=0.9516 train_f1=0.5048 val_loss=0.1323 val_acc=0.9521 val_f1=0.5029 \n",
            "Epoch 180/183 | train_loss=0.1291 train_acc=0.9532 train_f1=0.5145 val_loss=0.1244 val_acc=0.9539 val_f1=0.5138 \n",
            "Epoch 16/169 | train_loss=0.5804 train_acc=0.0676 train_f1=0.0532 val_loss=0.5488 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/169 | train_loss=0.3696 train_acc=0.8754 train_f1=0.3288 val_loss=0.3239 val_acc=0.8755 val_f1=0.3289 \n",
            "Epoch 48/169 | train_loss=0.3175 train_acc=0.9009 train_f1=0.3421 val_loss=0.2904 val_acc=0.9020 val_f1=0.3428 \n",
            "Epoch 64/169 | train_loss=0.2896 train_acc=0.9178 train_f1=0.3519 val_loss=0.2650 val_acc=0.9185 val_f1=0.3521 \n",
            "Epoch 80/169 | train_loss=0.2697 train_acc=0.9286 train_f1=0.4267 val_loss=0.2405 val_acc=0.9297 val_f1=0.4263 \n",
            "Epoch 96/169 | train_loss=0.2531 train_acc=0.9394 train_f1=0.4501 val_loss=0.2268 val_acc=0.9407 val_f1=0.4516 \n",
            "Epoch 112/169 | train_loss=0.2455 train_acc=0.9427 train_f1=0.4712 val_loss=0.2176 val_acc=0.9429 val_f1=0.4690 \n",
            "Epoch 128/169 | train_loss=0.2342 train_acc=0.9367 train_f1=0.4729 val_loss=0.2076 val_acc=0.9363 val_f1=0.4715 \n",
            "Epoch 144/169 | train_loss=0.2300 train_acc=0.9366 train_f1=0.4650 val_loss=0.2010 val_acc=0.9368 val_f1=0.4644 \n",
            "Epoch 160/169 | train_loss=0.2180 train_acc=0.9425 train_f1=0.4729 val_loss=0.1948 val_acc=0.9421 val_f1=0.4722 \n",
            "Epoch 19/198 | train_loss=0.6882 train_acc=0.1286 train_f1=0.0725 val_loss=0.5622 val_acc=0.1280 val_f1=0.0723 \n",
            "Epoch 38/198 | train_loss=0.5257 train_acc=0.4814 train_f1=0.1816 val_loss=0.4684 val_acc=0.4819 val_f1=0.1818 \n",
            "Epoch 57/198 | train_loss=0.3840 train_acc=0.8638 train_f1=0.3232 val_loss=0.3179 val_acc=0.8649 val_f1=0.3237 \n",
            "Epoch 76/198 | train_loss=0.3171 train_acc=0.9073 train_f1=0.3465 val_loss=0.2648 val_acc=0.9076 val_f1=0.3465 \n",
            "Epoch 95/198 | train_loss=0.2826 train_acc=0.9214 train_f1=0.3553 val_loss=0.2401 val_acc=0.9214 val_f1=0.3548 \n",
            "Epoch 114/198 | train_loss=0.2552 train_acc=0.9306 train_f1=0.3613 val_loss=0.2210 val_acc=0.9301 val_f1=0.3607 \n",
            "Epoch 133/198 | train_loss=0.2302 train_acc=0.9348 train_f1=0.3645 val_loss=0.2046 val_acc=0.9345 val_f1=0.3640 \n",
            "Epoch 152/198 | train_loss=0.2103 train_acc=0.9408 train_f1=0.3685 val_loss=0.1900 val_acc=0.9402 val_f1=0.3679 \n",
            "Epoch 171/198 | train_loss=0.2034 train_acc=0.9414 train_f1=0.3690 val_loss=0.1769 val_acc=0.9411 val_f1=0.3682 \n",
            "Epoch 190/198 | train_loss=0.1882 train_acc=0.9439 train_f1=0.3720 val_loss=0.1675 val_acc=0.9437 val_f1=0.3713 \n",
            "Epoch 17/175 | train_loss=0.3335 train_acc=0.8844 train_f1=0.3491 val_loss=0.3045 val_acc=0.8851 val_f1=0.3488 \n",
            "Epoch 34/175 | train_loss=0.2665 train_acc=0.9143 train_f1=0.3977 val_loss=0.2482 val_acc=0.9137 val_f1=0.3962 \n",
            "Epoch 51/175 | train_loss=0.2405 train_acc=0.9207 train_f1=0.4249 val_loss=0.2259 val_acc=0.9204 val_f1=0.4230 \n",
            "Epoch 68/175 | train_loss=0.2261 train_acc=0.9304 train_f1=0.4372 val_loss=0.2118 val_acc=0.9307 val_f1=0.4365 \n",
            "Epoch 85/175 | train_loss=0.2161 train_acc=0.9374 train_f1=0.4464 val_loss=0.2007 val_acc=0.9375 val_f1=0.4443 \n",
            "Epoch 102/175 | train_loss=0.2071 train_acc=0.9398 train_f1=0.4567 val_loss=0.1911 val_acc=0.9397 val_f1=0.4561 \n",
            "Epoch 119/175 | train_loss=0.2007 train_acc=0.9417 train_f1=0.4642 val_loss=0.1827 val_acc=0.9419 val_f1=0.4636 \n",
            "Epoch 136/175 | train_loss=0.1947 train_acc=0.9413 train_f1=0.4655 val_loss=0.1766 val_acc=0.9406 val_f1=0.4630 \n",
            "Epoch 153/175 | train_loss=0.1906 train_acc=0.9443 train_f1=0.4759 val_loss=0.1706 val_acc=0.9435 val_f1=0.4727 \n",
            "Epoch 170/175 | train_loss=0.1866 train_acc=0.9468 train_f1=0.4770 val_loss=0.1681 val_acc=0.9454 val_f1=0.4724 \n",
            "Epoch 15/155 | train_loss=0.4289 train_acc=0.7983 train_f1=0.2916 val_loss=0.4135 val_acc=0.7994 val_f1=0.2925 \n",
            "Epoch 30/155 | train_loss=0.3197 train_acc=0.8582 train_f1=0.3213 val_loss=0.2895 val_acc=0.8598 val_f1=0.3220 \n",
            "Epoch 45/155 | train_loss=0.2713 train_acc=0.9097 train_f1=0.3484 val_loss=0.2482 val_acc=0.9094 val_f1=0.3479 \n",
            "Epoch 60/155 | train_loss=0.2427 train_acc=0.9215 train_f1=0.3978 val_loss=0.2183 val_acc=0.9200 val_f1=0.3938 \n",
            "Epoch 75/155 | train_loss=0.2186 train_acc=0.9323 train_f1=0.4544 val_loss=0.1953 val_acc=0.9317 val_f1=0.4504 \n",
            "Epoch 90/155 | train_loss=0.2027 train_acc=0.9399 train_f1=0.4803 val_loss=0.1783 val_acc=0.9395 val_f1=0.4780 \n",
            "Epoch 105/155 | train_loss=0.1874 train_acc=0.9449 train_f1=0.4879 val_loss=0.1646 val_acc=0.9445 val_f1=0.4863 \n",
            "Epoch 120/155 | train_loss=0.1772 train_acc=0.9468 train_f1=0.4963 val_loss=0.1571 val_acc=0.9462 val_f1=0.4946 \n",
            "Epoch 135/155 | train_loss=0.1706 train_acc=0.9466 train_f1=0.5002 val_loss=0.1465 val_acc=0.9467 val_f1=0.4980 \n",
            "Epoch 150/155 | train_loss=0.1592 train_acc=0.9477 train_f1=0.5066 val_loss=0.1403 val_acc=0.9481 val_f1=0.5058 \n",
            "Epoch 17/175 | train_loss=0.5992 train_acc=0.1172 train_f1=0.0689 val_loss=0.5554 val_acc=0.1170 val_f1=0.0688 \n",
            "Epoch 34/175 | train_loss=0.4350 train_acc=0.7762 train_f1=0.2834 val_loss=0.3825 val_acc=0.7771 val_f1=0.2843 \n",
            "Epoch 51/175 | train_loss=0.3479 train_acc=0.8798 train_f1=0.3316 val_loss=0.2997 val_acc=0.8800 val_f1=0.3315 \n",
            "Epoch 68/175 | train_loss=0.3109 train_acc=0.9044 train_f1=0.3451 val_loss=0.2733 val_acc=0.9056 val_f1=0.3457 \n",
            "Epoch 85/175 | train_loss=0.2873 train_acc=0.9264 train_f1=0.3581 val_loss=0.2579 val_acc=0.9266 val_f1=0.3581 \n",
            "Epoch 102/175 | train_loss=0.2704 train_acc=0.9381 train_f1=0.3715 val_loss=0.2441 val_acc=0.9381 val_f1=0.3719 \n",
            "Epoch 119/175 | train_loss=0.2534 train_acc=0.9462 train_f1=0.4146 val_loss=0.2289 val_acc=0.9464 val_f1=0.4139 \n",
            "Epoch 136/175 | train_loss=0.2379 train_acc=0.9523 train_f1=0.4458 val_loss=0.2212 val_acc=0.9526 val_f1=0.4457 \n",
            "Epoch 153/175 | train_loss=0.2272 train_acc=0.9572 train_f1=0.4699 val_loss=0.2131 val_acc=0.9571 val_f1=0.4677 \n",
            "Epoch 170/175 | train_loss=0.2222 train_acc=0.9572 train_f1=0.4795 val_loss=0.2070 val_acc=0.9569 val_f1=0.4757 \n",
            "Epoch 21/211 | train_loss=0.6022 train_acc=0.1036 train_f1=0.0646 val_loss=0.5492 val_acc=0.1037 val_f1=0.0647 \n",
            "Epoch 42/211 | train_loss=0.4027 train_acc=0.7614 train_f1=0.2776 val_loss=0.3552 val_acc=0.7617 val_f1=0.2781 \n",
            "Epoch 63/211 | train_loss=0.3052 train_acc=0.8964 train_f1=0.3407 val_loss=0.2693 val_acc=0.8975 val_f1=0.3410 \n",
            "Epoch 84/211 | train_loss=0.2665 train_acc=0.9192 train_f1=0.3543 val_loss=0.2369 val_acc=0.9191 val_f1=0.3539 \n",
            "Epoch 105/211 | train_loss=0.2380 train_acc=0.9288 train_f1=0.4012 val_loss=0.2083 val_acc=0.9291 val_f1=0.4020 \n",
            "Epoch 126/211 | train_loss=0.2138 train_acc=0.9396 train_f1=0.4466 val_loss=0.1843 val_acc=0.9397 val_f1=0.4480 \n",
            "Epoch 147/211 | train_loss=0.1962 train_acc=0.9417 train_f1=0.4646 val_loss=0.1684 val_acc=0.9414 val_f1=0.4627 \n",
            "Epoch 168/211 | train_loss=0.1832 train_acc=0.9444 train_f1=0.4777 val_loss=0.1559 val_acc=0.9438 val_f1=0.4738 \n",
            "Epoch 189/211 | train_loss=0.1693 train_acc=0.9471 train_f1=0.4938 val_loss=0.1444 val_acc=0.9464 val_f1=0.4886 \n",
            "Epoch 210/211 | train_loss=0.1581 train_acc=0.9496 train_f1=0.5031 val_loss=0.1356 val_acc=0.9495 val_f1=0.4998 \n",
            "Epoch 17/173 | train_loss=0.6381 train_acc=0.0676 train_f1=0.0532 val_loss=0.5928 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/173 | train_loss=0.4980 train_acc=0.8766 train_f1=0.3284 val_loss=0.3881 val_acc=0.8777 val_f1=0.3290 \n",
            "Epoch 51/173 | train_loss=0.4550 train_acc=0.9103 train_f1=0.3473 val_loss=0.3433 val_acc=0.9106 val_f1=0.3472 \n",
            "Early stopping at epoch 60\n",
            "Epoch 12/123 | train_loss=0.5660 train_acc=0.0676 train_f1=0.0532 val_loss=0.5279 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 24/123 | train_loss=0.4525 train_acc=0.7618 train_f1=0.2777 val_loss=0.4084 val_acc=0.7609 val_f1=0.2773 \n",
            "Epoch 36/123 | train_loss=0.3834 train_acc=0.9078 train_f1=0.3461 val_loss=0.3412 val_acc=0.9074 val_f1=0.3455 \n",
            "Epoch 48/123 | train_loss=0.3482 train_acc=0.9170 train_f1=0.3516 val_loss=0.3178 val_acc=0.9179 val_f1=0.3516 \n",
            "Epoch 60/123 | train_loss=0.3307 train_acc=0.9341 train_f1=0.3619 val_loss=0.3113 val_acc=0.9341 val_f1=0.3616 \n",
            "Epoch 72/123 | train_loss=0.3189 train_acc=0.9402 train_f1=0.3659 val_loss=0.2995 val_acc=0.9398 val_f1=0.3651 \n",
            "Epoch 84/123 | train_loss=0.3095 train_acc=0.9433 train_f1=0.3685 val_loss=0.2907 val_acc=0.9430 val_f1=0.3677 \n",
            "Epoch 96/123 | train_loss=0.2988 train_acc=0.9458 train_f1=0.3708 val_loss=0.2794 val_acc=0.9454 val_f1=0.3699 \n",
            "Epoch 108/123 | train_loss=0.2949 train_acc=0.9447 train_f1=0.3702 val_loss=0.2742 val_acc=0.9445 val_f1=0.3694 \n",
            "Epoch 120/123 | train_loss=0.2892 train_acc=0.9463 train_f1=0.3716 val_loss=0.2688 val_acc=0.9459 val_f1=0.3706 \n",
            "Gen 4 Best: {'epochs_run': 56, 'val_loss': 0.7268398404121399, 'val_accuracy': 0.06757651559741024, 'val_f1': 0.053197034634541875, 'flops': 23808}\n",
            "Generation 5 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.004215843623207322, epochs=175, hidden=(32, 143, 121), dropout_rate=0.29256894898218655, patience=30)\n",
            "Epoch 17/175 | train_loss=0.5694 train_acc=0.0676 train_f1=0.0532 val_loss=0.5501 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/175 | train_loss=0.4449 train_acc=0.6721 train_f1=0.2447 val_loss=0.4054 val_acc=0.6720 val_f1=0.2448 \n",
            "Epoch 51/175 | train_loss=0.3290 train_acc=0.8922 train_f1=0.3383 val_loss=0.2821 val_acc=0.8920 val_f1=0.3381 \n",
            "Epoch 68/175 | train_loss=0.2883 train_acc=0.9098 train_f1=0.3483 val_loss=0.2569 val_acc=0.9099 val_f1=0.3482 \n",
            "Epoch 85/175 | train_loss=0.2604 train_acc=0.9222 train_f1=0.3557 val_loss=0.2354 val_acc=0.9222 val_f1=0.3552 \n",
            "Epoch 102/175 | train_loss=0.2387 train_acc=0.9352 train_f1=0.4003 val_loss=0.2116 val_acc=0.9348 val_f1=0.3985 \n",
            "Epoch 119/175 | train_loss=0.2180 train_acc=0.9447 train_f1=0.4561 val_loss=0.1923 val_acc=0.9445 val_f1=0.4531 \n",
            "Epoch 136/175 | train_loss=0.2031 train_acc=0.9485 train_f1=0.4792 val_loss=0.1796 val_acc=0.9488 val_f1=0.4778 \n",
            "Epoch 153/175 | train_loss=0.1914 train_acc=0.9503 train_f1=0.4875 val_loss=0.1699 val_acc=0.9507 val_f1=0.4859 \n",
            "Epoch 170/175 | train_loss=0.1825 train_acc=0.9519 train_f1=0.4970 val_loss=0.1649 val_acc=0.9526 val_f1=0.4960 \n",
            "Evaluating member 1: Hyperparameters(lr=0.003668599973473764, epochs=183, hidden=(112, 112, 32), dropout_rate=0.10493317819227185, patience=28)\n",
            "Epoch 18/183 | train_loss=0.5614 train_acc=0.3265 train_f1=0.1336 val_loss=0.5248 val_acc=0.3267 val_f1=0.1338 \n",
            "Epoch 36/183 | train_loss=0.4272 train_acc=0.6431 train_f1=0.2342 val_loss=0.4117 val_acc=0.6418 val_f1=0.2338 \n",
            "Epoch 54/183 | train_loss=0.3052 train_acc=0.9044 train_f1=0.3447 val_loss=0.2890 val_acc=0.9049 val_f1=0.3447 \n",
            "Epoch 72/183 | train_loss=0.2606 train_acc=0.9215 train_f1=0.3554 val_loss=0.2472 val_acc=0.9210 val_f1=0.3549 \n",
            "Epoch 90/183 | train_loss=0.2317 train_acc=0.9320 train_f1=0.3648 val_loss=0.2210 val_acc=0.9314 val_f1=0.3642 \n",
            "Epoch 108/183 | train_loss=0.2064 train_acc=0.9405 train_f1=0.4212 val_loss=0.1963 val_acc=0.9405 val_f1=0.4222 \n",
            "Epoch 126/183 | train_loss=0.1859 train_acc=0.9469 train_f1=0.4674 val_loss=0.1747 val_acc=0.9460 val_f1=0.4643 \n",
            "Epoch 144/183 | train_loss=0.1684 train_acc=0.9493 train_f1=0.4823 val_loss=0.1593 val_acc=0.9488 val_f1=0.4799 \n",
            "Epoch 162/183 | train_loss=0.1561 train_acc=0.9517 train_f1=0.4909 val_loss=0.1487 val_acc=0.9512 val_f1=0.4854 \n",
            "Epoch 180/183 | train_loss=0.1455 train_acc=0.9530 train_f1=0.5004 val_loss=0.1400 val_acc=0.9522 val_f1=0.4950 \n",
            "Evaluating member 2: Hyperparameters(lr=0.04847186143470714, epochs=169, hidden=(14, 32, 64, 256), dropout_rate=0.10937531086490793, patience=13)\n",
            "Epoch 16/169 | train_loss=0.6658 train_acc=0.0676 train_f1=0.0532 val_loss=0.5842 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/169 | train_loss=0.4307 train_acc=0.8351 train_f1=0.3093 val_loss=0.3706 val_acc=0.8355 val_f1=0.3096 \n",
            "Epoch 48/169 | train_loss=0.3240 train_acc=0.8802 train_f1=0.3317 val_loss=0.2920 val_acc=0.8801 val_f1=0.3313 \n",
            "Epoch 64/169 | train_loss=0.2889 train_acc=0.9174 train_f1=0.3667 val_loss=0.2522 val_acc=0.9171 val_f1=0.3632 \n",
            "Epoch 80/169 | train_loss=0.2687 train_acc=0.9257 train_f1=0.4023 val_loss=0.2357 val_acc=0.9257 val_f1=0.3962 \n",
            "Epoch 96/169 | train_loss=0.2514 train_acc=0.9297 train_f1=0.4346 val_loss=0.2208 val_acc=0.9297 val_f1=0.4321 \n",
            "Epoch 112/169 | train_loss=0.2394 train_acc=0.9323 train_f1=0.4469 val_loss=0.2114 val_acc=0.9327 val_f1=0.4460 \n",
            "Epoch 128/169 | train_loss=0.2326 train_acc=0.9362 train_f1=0.4505 val_loss=0.2027 val_acc=0.9363 val_f1=0.4479 \n",
            "Epoch 144/169 | train_loss=0.2252 train_acc=0.9400 train_f1=0.4636 val_loss=0.1987 val_acc=0.9409 val_f1=0.4632 \n",
            "Epoch 160/169 | train_loss=0.2162 train_acc=0.9416 train_f1=0.4825 val_loss=0.1999 val_acc=0.9419 val_f1=0.4820 \n",
            "Evaluating member 3: Hyperparameters(lr=0.003419136052828527, epochs=198, hidden=(109, 112, 16, 32), dropout_rate=0.25977891426282773, patience=28)\n",
            "Epoch 19/198 | train_loss=0.6784 train_acc=0.0676 train_f1=0.0532 val_loss=0.5940 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/198 | train_loss=0.5330 train_acc=0.2674 train_f1=0.1155 val_loss=0.4869 val_acc=0.2690 val_f1=0.1160 \n",
            "Epoch 57/198 | train_loss=0.3772 train_acc=0.8723 train_f1=0.3271 val_loss=0.3123 val_acc=0.8723 val_f1=0.3269 \n",
            "Epoch 76/198 | train_loss=0.3100 train_acc=0.9058 train_f1=0.3458 val_loss=0.2643 val_acc=0.9065 val_f1=0.3459 \n",
            "Epoch 95/198 | train_loss=0.2721 train_acc=0.9219 train_f1=0.3554 val_loss=0.2386 val_acc=0.9217 val_f1=0.3549 \n",
            "Epoch 114/198 | train_loss=0.2466 train_acc=0.9305 train_f1=0.3608 val_loss=0.2182 val_acc=0.9299 val_f1=0.3600 \n",
            "Epoch 133/198 | train_loss=0.2285 train_acc=0.9355 train_f1=0.3644 val_loss=0.1995 val_acc=0.9349 val_f1=0.3637 \n",
            "Epoch 152/198 | train_loss=0.2098 train_acc=0.9395 train_f1=0.3674 val_loss=0.1851 val_acc=0.9396 val_f1=0.3671 \n",
            "Epoch 171/198 | train_loss=0.1935 train_acc=0.9442 train_f1=0.3755 val_loss=0.1729 val_acc=0.9438 val_f1=0.3748 \n",
            "Epoch 190/198 | train_loss=0.1816 train_acc=0.9535 train_f1=0.4601 val_loss=0.1596 val_acc=0.9539 val_f1=0.4587 \n",
            "Evaluating member 4: Hyperparameters(lr=0.09282700570303513, epochs=175, hidden=(26,), dropout_rate=0.211488293145889, patience=25)\n",
            "Epoch 17/175 | train_loss=0.3289 train_acc=0.8798 train_f1=0.3383 val_loss=0.2918 val_acc=0.8802 val_f1=0.3396 \n",
            "Epoch 34/175 | train_loss=0.2623 train_acc=0.9133 train_f1=0.4081 val_loss=0.2429 val_acc=0.9133 val_f1=0.4076 \n",
            "Epoch 51/175 | train_loss=0.2382 train_acc=0.9261 train_f1=0.4326 val_loss=0.2222 val_acc=0.9266 val_f1=0.4341 \n",
            "Epoch 68/175 | train_loss=0.2259 train_acc=0.9323 train_f1=0.4430 val_loss=0.2100 val_acc=0.9323 val_f1=0.4425 \n",
            "Epoch 85/175 | train_loss=0.2130 train_acc=0.9365 train_f1=0.4482 val_loss=0.1990 val_acc=0.9367 val_f1=0.4488 \n",
            "Epoch 102/175 | train_loss=0.2068 train_acc=0.9384 train_f1=0.4580 val_loss=0.1902 val_acc=0.9379 val_f1=0.4579 \n",
            "Epoch 119/175 | train_loss=0.1989 train_acc=0.9383 train_f1=0.4595 val_loss=0.1841 val_acc=0.9378 val_f1=0.4576 \n",
            "Epoch 136/175 | train_loss=0.1968 train_acc=0.9444 train_f1=0.4664 val_loss=0.1797 val_acc=0.9436 val_f1=0.4642 \n",
            "Epoch 153/175 | train_loss=0.1897 train_acc=0.9465 train_f1=0.4736 val_loss=0.1736 val_acc=0.9454 val_f1=0.4706 \n",
            "Epoch 170/175 | train_loss=0.1846 train_acc=0.9469 train_f1=0.4760 val_loss=0.1684 val_acc=0.9464 val_f1=0.4734 \n",
            "Evaluating member 5: Hyperparameters(lr=0.04094411515234966, epochs=155, hidden=(39, 64, 256), dropout_rate=0.1935012255158155, patience=8)\n",
            "Epoch 15/155 | train_loss=0.4299 train_acc=0.7887 train_f1=0.2884 val_loss=0.4072 val_acc=0.7885 val_f1=0.2884 \n",
            "Epoch 30/155 | train_loss=0.3136 train_acc=0.8644 train_f1=0.3234 val_loss=0.2881 val_acc=0.8653 val_f1=0.3237 \n",
            "Epoch 45/155 | train_loss=0.2695 train_acc=0.9000 train_f1=0.3426 val_loss=0.2442 val_acc=0.8999 val_f1=0.3424 \n",
            "Epoch 60/155 | train_loss=0.2332 train_acc=0.9335 train_f1=0.4403 val_loss=0.2031 val_acc=0.9336 val_f1=0.4378 \n",
            "Epoch 75/155 | train_loss=0.2119 train_acc=0.9418 train_f1=0.4733 val_loss=0.1803 val_acc=0.9415 val_f1=0.4698 \n",
            "Epoch 90/155 | train_loss=0.1919 train_acc=0.9447 train_f1=0.4854 val_loss=0.1635 val_acc=0.9441 val_f1=0.4822 \n",
            "Epoch 105/155 | train_loss=0.1769 train_acc=0.9465 train_f1=0.4918 val_loss=0.1543 val_acc=0.9462 val_f1=0.4889 \n",
            "Epoch 120/155 | train_loss=0.1716 train_acc=0.9481 train_f1=0.5001 val_loss=0.1463 val_acc=0.9482 val_f1=0.4978 \n",
            "Epoch 135/155 | train_loss=0.1640 train_acc=0.9484 train_f1=0.4977 val_loss=0.1424 val_acc=0.9483 val_f1=0.4961 \n",
            "Epoch 150/155 | train_loss=0.1547 train_acc=0.9510 train_f1=0.5078 val_loss=0.1390 val_acc=0.9510 val_f1=0.5056 \n",
            "Evaluating member 6: Hyperparameters(lr=0.003865496945491269, epochs=175, hidden=(19, 143, 128), dropout_rate=0.27936922294642036, patience=24)\n",
            "Epoch 17/175 | train_loss=0.6002 train_acc=0.2406 train_f1=0.1071 val_loss=0.5589 val_acc=0.2417 val_f1=0.1075 \n",
            "Epoch 34/175 | train_loss=0.4414 train_acc=0.7570 train_f1=0.2756 val_loss=0.3894 val_acc=0.7569 val_f1=0.2760 \n",
            "Epoch 51/175 | train_loss=0.3477 train_acc=0.8906 train_f1=0.3371 val_loss=0.2941 val_acc=0.8910 val_f1=0.3373 \n",
            "Epoch 68/175 | train_loss=0.3077 train_acc=0.9139 train_f1=0.3504 val_loss=0.2710 val_acc=0.9149 val_f1=0.3508 \n",
            "Epoch 85/175 | train_loss=0.2839 train_acc=0.9285 train_f1=0.3588 val_loss=0.2531 val_acc=0.9290 val_f1=0.3588 \n",
            "Epoch 102/175 | train_loss=0.2682 train_acc=0.9381 train_f1=0.3787 val_loss=0.2391 val_acc=0.9382 val_f1=0.3796 \n",
            "Epoch 119/175 | train_loss=0.2535 train_acc=0.9450 train_f1=0.4145 val_loss=0.2291 val_acc=0.9450 val_f1=0.4143 \n",
            "Epoch 136/175 | train_loss=0.2426 train_acc=0.9510 train_f1=0.4517 val_loss=0.2222 val_acc=0.9513 val_f1=0.4511 \n",
            "Epoch 153/175 | train_loss=0.2320 train_acc=0.9548 train_f1=0.4721 val_loss=0.2182 val_acc=0.9548 val_f1=0.4707 \n",
            "Epoch 170/175 | train_loss=0.2231 train_acc=0.9564 train_f1=0.4805 val_loss=0.2144 val_acc=0.9567 val_f1=0.4807 \n",
            "Evaluating member 7: Hyperparameters(lr=0.0033057872913921016, epochs=211, hidden=(109, 124, 32), dropout_rate=0.25983452071234386, patience=28)\n",
            "Epoch 21/211 | train_loss=0.5642 train_acc=0.1021 train_f1=0.0641 val_loss=0.5236 val_acc=0.1013 val_f1=0.0639 \n",
            "Epoch 42/211 | train_loss=0.3581 train_acc=0.8660 train_f1=0.3241 val_loss=0.3108 val_acc=0.8667 val_f1=0.3243 \n",
            "Epoch 63/211 | train_loss=0.2885 train_acc=0.9091 train_f1=0.3477 val_loss=0.2551 val_acc=0.9095 val_f1=0.3479 \n",
            "Epoch 84/211 | train_loss=0.2516 train_acc=0.9300 train_f1=0.3905 val_loss=0.2205 val_acc=0.9301 val_f1=0.3909 \n",
            "Epoch 105/211 | train_loss=0.2251 train_acc=0.9374 train_f1=0.4333 val_loss=0.1958 val_acc=0.9378 val_f1=0.4334 \n",
            "Epoch 126/211 | train_loss=0.2041 train_acc=0.9460 train_f1=0.4719 val_loss=0.1747 val_acc=0.9457 val_f1=0.4690 \n",
            "Epoch 147/211 | train_loss=0.1859 train_acc=0.9489 train_f1=0.4804 val_loss=0.1604 val_acc=0.9483 val_f1=0.4769 \n",
            "Epoch 168/211 | train_loss=0.1694 train_acc=0.9495 train_f1=0.4900 val_loss=0.1490 val_acc=0.9489 val_f1=0.4860 \n",
            "Epoch 189/211 | train_loss=0.1639 train_acc=0.9517 train_f1=0.4993 val_loss=0.1418 val_acc=0.9519 val_f1=0.4964 \n",
            "Epoch 210/211 | train_loss=0.1546 train_acc=0.9521 train_f1=0.5052 val_loss=0.1355 val_acc=0.9519 val_f1=0.5015 \n",
            "Evaluating member 8: Hyperparameters(lr=0.0465863290251413, epochs=173, hidden=(52, 32, 64, 245), dropout_rate=0.1522617456503818, patience=6)\n",
            "Epoch 17/173 | train_loss=0.6031 train_acc=0.6558 train_f1=0.2373 val_loss=0.4799 val_acc=0.6571 val_f1=0.2378 \n",
            "Epoch 34/173 | train_loss=0.4719 train_acc=0.9328 train_f1=0.3598 val_loss=0.3683 val_acc=0.9334 val_f1=0.3601 \n",
            "Epoch 51/173 | train_loss=0.4454 train_acc=0.9145 train_f1=0.3496 val_loss=0.3373 val_acc=0.9150 val_f1=0.3497 \n",
            "Epoch 68/173 | train_loss=0.4374 train_acc=0.9221 train_f1=0.3543 val_loss=0.3214 val_acc=0.9230 val_f1=0.3546 \n",
            "Epoch 85/173 | train_loss=0.4252 train_acc=0.9269 train_f1=0.3570 val_loss=0.3033 val_acc=0.9282 val_f1=0.3576 \n",
            "Early stopping at epoch 101\n",
            "Evaluating member 9: Hyperparameters(lr=0.05539832083775733, epochs=123, hidden=(8, 8, 11, 64, 8), dropout_rate=0.1116045978508323, patience=17)\n",
            "Epoch 12/123 | train_loss=0.6248 train_acc=0.0676 train_f1=0.0532 val_loss=0.5643 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 24/123 | train_loss=0.4675 train_acc=0.7686 train_f1=0.2800 val_loss=0.4109 val_acc=0.7680 val_f1=0.2801 \n",
            "Epoch 36/123 | train_loss=0.3689 train_acc=0.9103 train_f1=0.3474 val_loss=0.3185 val_acc=0.9101 val_f1=0.3472 \n",
            "Epoch 48/123 | train_loss=0.3350 train_acc=0.8997 train_f1=0.3417 val_loss=0.3013 val_acc=0.9003 val_f1=0.3420 \n",
            "Epoch 60/123 | train_loss=0.3165 train_acc=0.9033 train_f1=0.3441 val_loss=0.2852 val_acc=0.9030 val_f1=0.3437 \n",
            "Epoch 72/123 | train_loss=0.3018 train_acc=0.9139 train_f1=0.3504 val_loss=0.2779 val_acc=0.9139 val_f1=0.3501 \n",
            "Epoch 84/123 | train_loss=0.2955 train_acc=0.9127 train_f1=0.3500 val_loss=0.2682 val_acc=0.9123 val_f1=0.3493 \n",
            "Epoch 96/123 | train_loss=0.2900 train_acc=0.9194 train_f1=0.3540 val_loss=0.2645 val_acc=0.9190 val_f1=0.3534 \n",
            "Epoch 108/123 | train_loss=0.2821 train_acc=0.9304 train_f1=0.3605 val_loss=0.2635 val_acc=0.9304 val_f1=0.3601 \n",
            "Epoch 120/123 | train_loss=0.2812 train_acc=0.9275 train_f1=0.3590 val_loss=0.2602 val_acc=0.9270 val_f1=0.3584 \n",
            "Epoch 21/211 | train_loss=0.6612 train_acc=0.0676 train_f1=0.0532 val_loss=0.6013 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 42/211 | train_loss=0.5032 train_acc=0.1755 train_f1=0.0872 val_loss=0.4654 val_acc=0.1762 val_f1=0.0874 \n",
            "Epoch 63/211 | train_loss=0.3419 train_acc=0.8935 train_f1=0.3387 val_loss=0.2902 val_acc=0.8934 val_f1=0.3385 \n",
            "Epoch 84/211 | train_loss=0.2788 train_acc=0.9134 train_f1=0.3503 val_loss=0.2390 val_acc=0.9133 val_f1=0.3500 \n",
            "Epoch 105/211 | train_loss=0.2416 train_acc=0.9268 train_f1=0.3638 val_loss=0.2115 val_acc=0.9272 val_f1=0.3657 \n",
            "Epoch 126/211 | train_loss=0.2197 train_acc=0.9377 train_f1=0.4004 val_loss=0.1912 val_acc=0.9379 val_f1=0.4023 \n",
            "Epoch 147/211 | train_loss=0.2012 train_acc=0.9410 train_f1=0.4369 val_loss=0.1735 val_acc=0.9410 val_f1=0.4355 \n",
            "Epoch 168/211 | train_loss=0.1823 train_acc=0.9465 train_f1=0.4769 val_loss=0.1598 val_acc=0.9470 val_f1=0.4740 \n",
            "Epoch 189/211 | train_loss=0.1723 train_acc=0.9510 train_f1=0.5000 val_loss=0.1474 val_acc=0.9511 val_f1=0.4981 \n",
            "Epoch 210/211 | train_loss=0.1607 train_acc=0.9534 train_f1=0.5118 val_loss=0.1374 val_acc=0.9533 val_f1=0.5098 \n",
            "Epoch 18/183 | train_loss=0.6232 train_acc=0.4262 train_f1=0.1640 val_loss=0.5819 val_acc=0.4265 val_f1=0.1641 \n",
            "Epoch 36/183 | train_loss=0.4476 train_acc=0.7109 train_f1=0.2578 val_loss=0.4128 val_acc=0.7109 val_f1=0.2581 \n",
            "Epoch 54/183 | train_loss=0.3198 train_acc=0.8643 train_f1=0.3236 val_loss=0.2981 val_acc=0.8640 val_f1=0.3236 \n",
            "Epoch 72/183 | train_loss=0.2718 train_acc=0.9061 train_f1=0.3463 val_loss=0.2558 val_acc=0.9070 val_f1=0.3464 \n",
            "Epoch 90/183 | train_loss=0.2424 train_acc=0.9239 train_f1=0.3612 val_loss=0.2306 val_acc=0.9238 val_f1=0.3598 \n",
            "Epoch 108/183 | train_loss=0.2213 train_acc=0.9339 train_f1=0.3948 val_loss=0.2094 val_acc=0.9338 val_f1=0.3951 \n",
            "Epoch 126/183 | train_loss=0.2012 train_acc=0.9404 train_f1=0.4321 val_loss=0.1891 val_acc=0.9403 val_f1=0.4304 \n",
            "Epoch 144/183 | train_loss=0.1825 train_acc=0.9442 train_f1=0.4616 val_loss=0.1709 val_acc=0.9438 val_f1=0.4590 \n",
            "Epoch 162/183 | train_loss=0.1674 train_acc=0.9479 train_f1=0.4794 val_loss=0.1579 val_acc=0.9477 val_f1=0.4765 \n",
            "Epoch 180/183 | train_loss=0.1527 train_acc=0.9516 train_f1=0.4946 val_loss=0.1473 val_acc=0.9512 val_f1=0.4911 \n",
            "Epoch 18/184 | train_loss=0.5050 train_acc=0.7941 train_f1=0.2886 val_loss=0.4188 val_acc=0.7949 val_f1=0.2895 \n",
            "Epoch 36/184 | train_loss=0.3362 train_acc=0.9018 train_f1=0.3426 val_loss=0.2952 val_acc=0.9028 val_f1=0.3430 \n",
            "Epoch 54/184 | train_loss=0.2773 train_acc=0.9116 train_f1=0.3497 val_loss=0.2505 val_acc=0.9122 val_f1=0.3499 \n",
            "Epoch 72/184 | train_loss=0.2471 train_acc=0.9291 train_f1=0.4093 val_loss=0.2158 val_acc=0.9280 val_f1=0.4072 \n",
            "Epoch 90/184 | train_loss=0.2181 train_acc=0.9375 train_f1=0.4732 val_loss=0.1844 val_acc=0.9377 val_f1=0.4711 \n",
            "Epoch 108/184 | train_loss=0.2039 train_acc=0.9414 train_f1=0.4873 val_loss=0.1695 val_acc=0.9411 val_f1=0.4838 \n",
            "Epoch 126/184 | train_loss=0.1905 train_acc=0.9486 train_f1=0.5013 val_loss=0.1592 val_acc=0.9478 val_f1=0.4969 \n",
            "Early stopping at epoch 133\n",
            "Epoch 19/191 | train_loss=0.3530 train_acc=0.8802 train_f1=0.3316 val_loss=0.3190 val_acc=0.8791 val_f1=0.3311 \n",
            "Epoch 38/191 | train_loss=0.2692 train_acc=0.9162 train_f1=0.3544 val_loss=0.2403 val_acc=0.9162 val_f1=0.3543 \n",
            "Epoch 57/191 | train_loss=0.2277 train_acc=0.9326 train_f1=0.4337 val_loss=0.1992 val_acc=0.9330 val_f1=0.4322 \n",
            "Epoch 76/191 | train_loss=0.1974 train_acc=0.9446 train_f1=0.4830 val_loss=0.1688 val_acc=0.9453 val_f1=0.4815 \n",
            "Epoch 95/191 | train_loss=0.1773 train_acc=0.9501 train_f1=0.4980 val_loss=0.1522 val_acc=0.9504 val_f1=0.4974 \n",
            "Epoch 114/191 | train_loss=0.1695 train_acc=0.9491 train_f1=0.5063 val_loss=0.1421 val_acc=0.9493 val_f1=0.5034 \n",
            "Epoch 133/191 | train_loss=0.1608 train_acc=0.9514 train_f1=0.5100 val_loss=0.1350 val_acc=0.9522 val_f1=0.5074 \n",
            "Early stopping at epoch 144\n",
            "Epoch 16/169 | train_loss=0.3760 train_acc=0.8525 train_f1=0.3175 val_loss=0.3342 val_acc=0.8516 val_f1=0.3170 \n",
            "Epoch 32/169 | train_loss=0.2933 train_acc=0.9263 train_f1=0.3584 val_loss=0.2613 val_acc=0.9266 val_f1=0.3580 \n",
            "Epoch 48/169 | train_loss=0.2533 train_acc=0.9383 train_f1=0.4005 val_loss=0.2250 val_acc=0.9380 val_f1=0.3982 \n",
            "Epoch 64/169 | train_loss=0.2354 train_acc=0.9400 train_f1=0.4307 val_loss=0.1991 val_acc=0.9401 val_f1=0.4272 \n",
            "Epoch 80/169 | train_loss=0.2208 train_acc=0.9436 train_f1=0.4522 val_loss=0.1838 val_acc=0.9435 val_f1=0.4473 \n",
            "Epoch 96/169 | train_loss=0.2101 train_acc=0.9433 train_f1=0.4572 val_loss=0.1748 val_acc=0.9431 val_f1=0.4547 \n",
            "Epoch 112/169 | train_loss=0.2053 train_acc=0.9456 train_f1=0.4693 val_loss=0.1675 val_acc=0.9459 val_f1=0.4668 \n",
            "Epoch 128/169 | train_loss=0.2010 train_acc=0.9472 train_f1=0.4744 val_loss=0.1611 val_acc=0.9473 val_f1=0.4706 \n",
            "Epoch 144/169 | train_loss=0.1912 train_acc=0.9514 train_f1=0.4829 val_loss=0.1543 val_acc=0.9514 val_f1=0.4797 \n",
            "Epoch 160/169 | train_loss=0.1878 train_acc=0.9517 train_f1=0.4846 val_loss=0.1498 val_acc=0.9514 val_f1=0.4814 \n",
            "Epoch 17/172 | train_loss=0.4719 train_acc=0.8423 train_f1=0.3110 val_loss=0.3993 val_acc=0.8419 val_f1=0.3112 \n",
            "Epoch 34/172 | train_loss=0.3193 train_acc=0.8760 train_f1=0.3295 val_loss=0.2977 val_acc=0.8772 val_f1=0.3302 \n",
            "Epoch 51/172 | train_loss=0.2748 train_acc=0.9219 train_f1=0.3557 val_loss=0.2553 val_acc=0.9219 val_f1=0.3557 \n",
            "Epoch 68/172 | train_loss=0.2502 train_acc=0.9273 train_f1=0.3613 val_loss=0.2274 val_acc=0.9269 val_f1=0.3597 \n",
            "Epoch 85/172 | train_loss=0.2283 train_acc=0.9364 train_f1=0.4302 val_loss=0.2025 val_acc=0.9362 val_f1=0.4296 \n",
            "Epoch 102/172 | train_loss=0.2081 train_acc=0.9383 train_f1=0.4607 val_loss=0.1789 val_acc=0.9393 val_f1=0.4623 \n",
            "Epoch 119/172 | train_loss=0.1896 train_acc=0.9438 train_f1=0.4868 val_loss=0.1650 val_acc=0.9438 val_f1=0.4861 \n",
            "Epoch 136/172 | train_loss=0.1788 train_acc=0.9362 train_f1=0.4856 val_loss=0.1559 val_acc=0.9356 val_f1=0.4816 \n",
            "Epoch 153/172 | train_loss=0.1756 train_acc=0.9461 train_f1=0.4998 val_loss=0.1526 val_acc=0.9457 val_f1=0.4964 \n",
            "Epoch 170/172 | train_loss=0.1713 train_acc=0.9418 train_f1=0.4879 val_loss=0.1508 val_acc=0.9414 val_f1=0.4857 \n",
            "Epoch 14/145 | train_loss=0.4133 train_acc=0.7624 train_f1=0.2776 val_loss=0.3916 val_acc=0.7624 val_f1=0.2780 \n",
            "Epoch 28/145 | train_loss=0.3139 train_acc=0.8740 train_f1=0.3292 val_loss=0.2829 val_acc=0.8750 val_f1=0.3296 \n",
            "Epoch 42/145 | train_loss=0.2665 train_acc=0.9024 train_f1=0.3653 val_loss=0.2408 val_acc=0.9030 val_f1=0.3662 \n",
            "Epoch 56/145 | train_loss=0.2358 train_acc=0.9270 train_f1=0.4335 val_loss=0.2089 val_acc=0.9276 val_f1=0.4342 \n",
            "Epoch 70/145 | train_loss=0.2143 train_acc=0.9317 train_f1=0.4492 val_loss=0.1919 val_acc=0.9312 val_f1=0.4457 \n",
            "Epoch 84/145 | train_loss=0.1983 train_acc=0.9397 train_f1=0.4714 val_loss=0.1797 val_acc=0.9400 val_f1=0.4705 \n",
            "Epoch 98/145 | train_loss=0.1849 train_acc=0.9458 train_f1=0.4832 val_loss=0.1677 val_acc=0.9458 val_f1=0.4816 \n",
            "Epoch 112/145 | train_loss=0.1774 train_acc=0.9483 train_f1=0.4929 val_loss=0.1586 val_acc=0.9483 val_f1=0.4911 \n",
            "Epoch 126/145 | train_loss=0.1681 train_acc=0.9509 train_f1=0.4941 val_loss=0.1545 val_acc=0.9509 val_f1=0.4924 \n",
            "Epoch 140/145 | train_loss=0.1641 train_acc=0.9502 train_f1=0.4984 val_loss=0.1489 val_acc=0.9498 val_f1=0.4943 \n",
            "Epoch 16/163 | train_loss=0.5686 train_acc=0.1859 train_f1=0.0903 val_loss=0.5296 val_acc=0.1861 val_f1=0.0904 \n",
            "Epoch 32/163 | train_loss=0.4312 train_acc=0.7436 train_f1=0.2702 val_loss=0.3823 val_acc=0.7414 val_f1=0.2696 \n",
            "Epoch 48/163 | train_loss=0.3378 train_acc=0.8839 train_f1=0.3336 val_loss=0.2906 val_acc=0.8839 val_f1=0.3335 \n",
            "Epoch 64/163 | train_loss=0.3011 train_acc=0.9027 train_f1=0.3438 val_loss=0.2635 val_acc=0.9025 val_f1=0.3433 \n",
            "Epoch 80/163 | train_loss=0.2744 train_acc=0.9201 train_f1=0.3657 val_loss=0.2401 val_acc=0.9201 val_f1=0.3648 \n",
            "Epoch 96/163 | train_loss=0.2554 train_acc=0.9260 train_f1=0.3993 val_loss=0.2222 val_acc=0.9263 val_f1=0.4005 \n",
            "Epoch 112/163 | train_loss=0.2402 train_acc=0.9320 train_f1=0.4327 val_loss=0.2081 val_acc=0.9322 val_f1=0.4338 \n",
            "Epoch 128/163 | train_loss=0.2269 train_acc=0.9371 train_f1=0.4532 val_loss=0.1958 val_acc=0.9376 val_f1=0.4531 \n",
            "Epoch 144/163 | train_loss=0.2134 train_acc=0.9410 train_f1=0.4680 val_loss=0.1861 val_acc=0.9415 val_f1=0.4674 \n",
            "Epoch 160/163 | train_loss=0.2087 train_acc=0.9448 train_f1=0.4789 val_loss=0.1776 val_acc=0.9448 val_f1=0.4775 \n",
            "Epoch 17/174 | train_loss=0.5582 train_acc=0.0676 train_f1=0.0532 val_loss=0.5355 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/174 | train_loss=0.3861 train_acc=0.8866 train_f1=0.3349 val_loss=0.3478 val_acc=0.8858 val_f1=0.3344 \n",
            "Epoch 51/174 | train_loss=0.2919 train_acc=0.9247 train_f1=0.3565 val_loss=0.2661 val_acc=0.9248 val_f1=0.3565 \n",
            "Epoch 68/174 | train_loss=0.2573 train_acc=0.9296 train_f1=0.3599 val_loss=0.2325 val_acc=0.9300 val_f1=0.3599 \n",
            "Epoch 85/174 | train_loss=0.2266 train_acc=0.9389 train_f1=0.4268 val_loss=0.2039 val_acc=0.9390 val_f1=0.4270 \n",
            "Epoch 102/174 | train_loss=0.2068 train_acc=0.9454 train_f1=0.4665 val_loss=0.1839 val_acc=0.9451 val_f1=0.4632 \n",
            "Epoch 119/174 | train_loss=0.1889 train_acc=0.9488 train_f1=0.4861 val_loss=0.1657 val_acc=0.9478 val_f1=0.4815 \n",
            "Epoch 136/174 | train_loss=0.1764 train_acc=0.9525 train_f1=0.4944 val_loss=0.1549 val_acc=0.9520 val_f1=0.4907 \n",
            "Epoch 153/174 | train_loss=0.1630 train_acc=0.9535 train_f1=0.5063 val_loss=0.1450 val_acc=0.9526 val_f1=0.5016 \n",
            "Epoch 170/174 | train_loss=0.1574 train_acc=0.9553 train_f1=0.5140 val_loss=0.1377 val_acc=0.9545 val_f1=0.5091 \n",
            "Epoch 19/195 | train_loss=0.9096 train_acc=0.0676 train_f1=0.0532 val_loss=0.6413 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/195 | train_loss=0.7704 train_acc=0.0676 train_f1=0.0532 val_loss=0.5541 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 57/195 | train_loss=0.5811 train_acc=0.0676 train_f1=0.0532 val_loss=0.5012 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 76/195 | train_loss=0.5020 train_acc=0.7264 train_f1=0.2651 val_loss=0.4235 val_acc=0.7261 val_f1=0.2647 \n",
            "Epoch 95/195 | train_loss=0.4235 train_acc=0.9239 train_f1=0.3568 val_loss=0.3246 val_acc=0.9244 val_f1=0.3567 \n",
            "Epoch 114/195 | train_loss=0.3617 train_acc=0.9208 train_f1=0.3547 val_loss=0.2673 val_acc=0.9217 val_f1=0.3550 \n",
            "Epoch 133/195 | train_loss=0.3246 train_acc=0.9248 train_f1=0.3572 val_loss=0.2389 val_acc=0.9240 val_f1=0.3562 \n",
            "Epoch 152/195 | train_loss=0.2981 train_acc=0.9260 train_f1=0.3574 val_loss=0.2180 val_acc=0.9260 val_f1=0.3571 \n",
            "Epoch 171/195 | train_loss=0.2751 train_acc=0.9297 train_f1=0.3595 val_loss=0.2038 val_acc=0.9299 val_f1=0.3592 \n",
            "Epoch 190/195 | train_loss=0.2478 train_acc=0.9331 train_f1=0.3615 val_loss=0.1918 val_acc=0.9330 val_f1=0.3610 \n",
            "Gen 5 Best: {'epochs_run': 175, 'val_loss': 0.16539525985717773, 'val_accuracy': 0.9526314498724741, 'val_f1': 0.4992935224838201, 'flops': 23508}\n",
            "Generation 6 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.0032284874084684933, epochs=211, hidden=(109, 124, 16, 32), dropout_rate=0.24182371855596943, patience=25)\n",
            "Epoch 21/211 | train_loss=0.6882 train_acc=0.0676 train_f1=0.0532 val_loss=0.6356 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 42/211 | train_loss=0.5507 train_acc=0.0676 train_f1=0.0532 val_loss=0.5120 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 63/211 | train_loss=0.4400 train_acc=0.8064 train_f1=0.2969 val_loss=0.3899 val_acc=0.8078 val_f1=0.2976 \n",
            "Epoch 84/211 | train_loss=0.3255 train_acc=0.9226 train_f1=0.3555 val_loss=0.2724 val_acc=0.9229 val_f1=0.3556 \n",
            "Epoch 105/211 | train_loss=0.2779 train_acc=0.9314 train_f1=0.3610 val_loss=0.2418 val_acc=0.9318 val_f1=0.3609 \n",
            "Epoch 126/211 | train_loss=0.2464 train_acc=0.9363 train_f1=0.3648 val_loss=0.2186 val_acc=0.9359 val_f1=0.3643 \n",
            "Epoch 147/211 | train_loss=0.2210 train_acc=0.9419 train_f1=0.3716 val_loss=0.1981 val_acc=0.9414 val_f1=0.3710 \n",
            "Epoch 168/211 | train_loss=0.1983 train_acc=0.9478 train_f1=0.4116 val_loss=0.1788 val_acc=0.9482 val_f1=0.4125 \n",
            "Epoch 189/211 | train_loss=0.1831 train_acc=0.9547 train_f1=0.4943 val_loss=0.1586 val_acc=0.9550 val_f1=0.4939 \n",
            "Epoch 210/211 | train_loss=0.1633 train_acc=0.9561 train_f1=0.5143 val_loss=0.1412 val_acc=0.9560 val_f1=0.5123 \n",
            "Evaluating member 1: Hyperparameters(lr=0.003390085868233796, epochs=183, hidden=(114, 112, 32), dropout_rate=0.07688200515495704, patience=30)\n",
            "Epoch 18/183 | train_loss=0.5324 train_acc=0.2733 train_f1=0.1173 val_loss=0.5176 val_acc=0.2752 val_f1=0.1180 \n",
            "Epoch 36/183 | train_loss=0.3248 train_acc=0.8995 train_f1=0.3413 val_loss=0.3080 val_acc=0.8990 val_f1=0.3408 \n",
            "Epoch 54/183 | train_loss=0.2671 train_acc=0.9192 train_f1=0.3539 val_loss=0.2546 val_acc=0.9190 val_f1=0.3534 \n",
            "Epoch 72/183 | train_loss=0.2283 train_acc=0.9306 train_f1=0.3947 val_loss=0.2183 val_acc=0.9306 val_f1=0.3950 \n",
            "Epoch 90/183 | train_loss=0.1971 train_acc=0.9413 train_f1=0.4421 val_loss=0.1885 val_acc=0.9415 val_f1=0.4435 \n",
            "Epoch 108/183 | train_loss=0.1718 train_acc=0.9472 train_f1=0.4732 val_loss=0.1657 val_acc=0.9468 val_f1=0.4705 \n",
            "Epoch 126/183 | train_loss=0.1530 train_acc=0.9498 train_f1=0.4870 val_loss=0.1491 val_acc=0.9491 val_f1=0.4837 \n",
            "Epoch 144/183 | train_loss=0.1391 train_acc=0.9545 train_f1=0.5010 val_loss=0.1368 val_acc=0.9536 val_f1=0.4967 \n",
            "Epoch 162/183 | train_loss=0.1292 train_acc=0.9557 train_f1=0.5097 val_loss=0.1279 val_acc=0.9549 val_f1=0.5064 \n",
            "Epoch 180/183 | train_loss=0.1178 train_acc=0.9590 train_f1=0.5197 val_loss=0.1218 val_acc=0.9584 val_f1=0.5156 \n",
            "Evaluating member 2: Hyperparameters(lr=0.046137799127505985, epochs=184, hidden=(52, 41, 64, 245), dropout_rate=0.12998252637644092, patience=5)\n",
            "Early stopping at epoch 14\n",
            "Evaluating member 3: Hyperparameters(lr=0.046350757518368174, epochs=191, hidden=(32, 64, 245), dropout_rate=0.15973448952903327, patience=7)\n",
            "Epoch 19/191 | train_loss=0.3625 train_acc=0.8921 train_f1=0.3369 val_loss=0.3205 val_acc=0.8929 val_f1=0.3374 \n",
            "Epoch 38/191 | train_loss=0.2800 train_acc=0.9010 train_f1=0.3430 val_loss=0.2537 val_acc=0.9013 val_f1=0.3430 \n",
            "Epoch 57/191 | train_loss=0.2424 train_acc=0.9159 train_f1=0.4200 val_loss=0.2111 val_acc=0.9174 val_f1=0.4227 \n",
            "Epoch 76/191 | train_loss=0.2144 train_acc=0.9262 train_f1=0.4560 val_loss=0.1892 val_acc=0.9265 val_f1=0.4538 \n",
            "Epoch 95/191 | train_loss=0.1996 train_acc=0.9361 train_f1=0.4722 val_loss=0.1735 val_acc=0.9363 val_f1=0.4699 \n",
            "Epoch 114/191 | train_loss=0.1848 train_acc=0.9414 train_f1=0.4855 val_loss=0.1608 val_acc=0.9413 val_f1=0.4827 \n",
            "Epoch 133/191 | train_loss=0.1752 train_acc=0.9420 train_f1=0.4866 val_loss=0.1513 val_acc=0.9419 val_f1=0.4833 \n",
            "Early stopping at epoch 144\n",
            "Evaluating member 4: Hyperparameters(lr=0.09900602380823531, epochs=169, hidden=(26, 16), dropout_rate=0.18918660649204072, patience=29)\n",
            "Epoch 16/169 | train_loss=0.3617 train_acc=0.8035 train_f1=0.2956 val_loss=0.3362 val_acc=0.8037 val_f1=0.2956 \n",
            "Epoch 32/169 | train_loss=0.2887 train_acc=0.9082 train_f1=0.3470 val_loss=0.2563 val_acc=0.9094 val_f1=0.3472 \n",
            "Epoch 48/169 | train_loss=0.2524 train_acc=0.9232 train_f1=0.3898 val_loss=0.2177 val_acc=0.9229 val_f1=0.3884 \n",
            "Epoch 64/169 | train_loss=0.2287 train_acc=0.9347 train_f1=0.4261 val_loss=0.1893 val_acc=0.9349 val_f1=0.4234 \n",
            "Epoch 80/169 | train_loss=0.2111 train_acc=0.9412 train_f1=0.4537 val_loss=0.1704 val_acc=0.9416 val_f1=0.4523 \n",
            "Epoch 96/169 | train_loss=0.1983 train_acc=0.9309 train_f1=0.4654 val_loss=0.1621 val_acc=0.9304 val_f1=0.4630 \n",
            "Epoch 112/169 | train_loss=0.1891 train_acc=0.9443 train_f1=0.4876 val_loss=0.1510 val_acc=0.9439 val_f1=0.4860 \n",
            "Epoch 128/169 | train_loss=0.1834 train_acc=0.9416 train_f1=0.4888 val_loss=0.1486 val_acc=0.9411 val_f1=0.4864 \n",
            "Epoch 144/169 | train_loss=0.1803 train_acc=0.9403 train_f1=0.4898 val_loss=0.1461 val_acc=0.9398 val_f1=0.4881 \n",
            "Epoch 160/169 | train_loss=0.1741 train_acc=0.9439 train_f1=0.4970 val_loss=0.1435 val_acc=0.9431 val_f1=0.4939 \n",
            "Evaluating member 5: Hyperparameters(lr=0.045775583635279615, epochs=172, hidden=(17, 32, 64, 256), dropout_rate=0.08537715504141016, patience=11)\n",
            "Epoch 17/172 | train_loss=0.4100 train_acc=0.8824 train_f1=0.3306 val_loss=0.3847 val_acc=0.8830 val_f1=0.3315 \n",
            "Epoch 34/172 | train_loss=0.3066 train_acc=0.8802 train_f1=0.3322 val_loss=0.2857 val_acc=0.8811 val_f1=0.3325 \n",
            "Epoch 51/172 | train_loss=0.2686 train_acc=0.9148 train_f1=0.3517 val_loss=0.2504 val_acc=0.9151 val_f1=0.3518 \n",
            "Epoch 68/172 | train_loss=0.2432 train_acc=0.9213 train_f1=0.3776 val_loss=0.2243 val_acc=0.9210 val_f1=0.3750 \n",
            "Epoch 85/172 | train_loss=0.2139 train_acc=0.9238 train_f1=0.4502 val_loss=0.1934 val_acc=0.9238 val_f1=0.4500 \n",
            "Epoch 102/172 | train_loss=0.1954 train_acc=0.9348 train_f1=0.4733 val_loss=0.1775 val_acc=0.9350 val_f1=0.4726 \n",
            "Epoch 119/172 | train_loss=0.1854 train_acc=0.9406 train_f1=0.4894 val_loss=0.1701 val_acc=0.9403 val_f1=0.4876 \n",
            "Epoch 136/172 | train_loss=0.1763 train_acc=0.9428 train_f1=0.4970 val_loss=0.1629 val_acc=0.9430 val_f1=0.4957 \n",
            "Epoch 153/172 | train_loss=0.1720 train_acc=0.9451 train_f1=0.5018 val_loss=0.1592 val_acc=0.9444 val_f1=0.5010 \n",
            "Epoch 170/172 | train_loss=0.1670 train_acc=0.9479 train_f1=0.5039 val_loss=0.1578 val_acc=0.9467 val_f1=0.5009 \n",
            "Evaluating member 6: Hyperparameters(lr=0.039668783164358096, epochs=145, hidden=(39, 71, 256), dropout_rate=0.14502552501924787, patience=9)\n",
            "Epoch 14/145 | train_loss=0.4526 train_acc=0.8171 train_f1=0.3004 val_loss=0.3820 val_acc=0.8180 val_f1=0.3012 \n",
            "Epoch 28/145 | train_loss=0.3157 train_acc=0.8966 train_f1=0.3406 val_loss=0.2934 val_acc=0.8975 val_f1=0.3412 \n",
            "Epoch 42/145 | train_loss=0.2843 train_acc=0.9042 train_f1=0.3454 val_loss=0.2613 val_acc=0.9045 val_f1=0.3455 \n",
            "Epoch 56/145 | train_loss=0.2520 train_acc=0.9255 train_f1=0.4026 val_loss=0.2305 val_acc=0.9254 val_f1=0.4032 \n",
            "Epoch 70/145 | train_loss=0.2304 train_acc=0.9309 train_f1=0.4471 val_loss=0.2027 val_acc=0.9315 val_f1=0.4486 \n",
            "Epoch 84/145 | train_loss=0.2100 train_acc=0.9358 train_f1=0.4687 val_loss=0.1880 val_acc=0.9367 val_f1=0.4693 \n",
            "Epoch 98/145 | train_loss=0.1960 train_acc=0.9373 train_f1=0.4815 val_loss=0.1760 val_acc=0.9371 val_f1=0.4793 \n",
            "Epoch 112/145 | train_loss=0.1829 train_acc=0.9435 train_f1=0.4880 val_loss=0.1670 val_acc=0.9433 val_f1=0.4852 \n",
            "Epoch 126/145 | train_loss=0.1732 train_acc=0.9445 train_f1=0.4914 val_loss=0.1579 val_acc=0.9442 val_f1=0.4894 \n",
            "Epoch 140/145 | train_loss=0.1669 train_acc=0.9456 train_f1=0.4938 val_loss=0.1507 val_acc=0.9449 val_f1=0.4907 \n",
            "Evaluating member 7: Hyperparameters(lr=0.003785795722660389, epochs=163, hidden=(33, 143, 128), dropout_rate=0.3063550605110126, patience=24)\n",
            "Epoch 16/163 | train_loss=0.5844 train_acc=0.0676 train_f1=0.0532 val_loss=0.5495 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/163 | train_loss=0.4833 train_acc=0.5694 train_f1=0.2095 val_loss=0.4542 val_acc=0.5694 val_f1=0.2096 \n",
            "Epoch 48/163 | train_loss=0.3650 train_acc=0.8542 train_f1=0.3181 val_loss=0.3211 val_acc=0.8541 val_f1=0.3179 \n",
            "Epoch 64/163 | train_loss=0.3071 train_acc=0.9000 train_f1=0.3419 val_loss=0.2704 val_acc=0.9003 val_f1=0.3418 \n",
            "Epoch 80/163 | train_loss=0.2763 train_acc=0.9132 train_f1=0.3494 val_loss=0.2463 val_acc=0.9137 val_f1=0.3494 \n",
            "Epoch 96/163 | train_loss=0.2524 train_acc=0.9249 train_f1=0.3767 val_loss=0.2243 val_acc=0.9247 val_f1=0.3747 \n",
            "Epoch 112/163 | train_loss=0.2357 train_acc=0.9329 train_f1=0.4267 val_loss=0.2068 val_acc=0.9332 val_f1=0.4269 \n",
            "Epoch 128/163 | train_loss=0.2237 train_acc=0.9378 train_f1=0.4564 val_loss=0.1930 val_acc=0.9378 val_f1=0.4542 \n",
            "Epoch 144/163 | train_loss=0.2107 train_acc=0.9449 train_f1=0.4768 val_loss=0.1825 val_acc=0.9447 val_f1=0.4729 \n",
            "Epoch 160/163 | train_loss=0.2032 train_acc=0.9486 train_f1=0.4853 val_loss=0.1747 val_acc=0.9484 val_f1=0.4816 \n",
            "Evaluating member 8: Hyperparameters(lr=0.004072071035348088, epochs=174, hidden=(64, 19, 143, 128), dropout_rate=0.18158066836900638, patience=26)\n",
            "Epoch 17/174 | train_loss=0.5785 train_acc=0.5003 train_f1=0.1871 val_loss=0.5318 val_acc=0.5005 val_f1=0.1872 \n",
            "Epoch 34/174 | train_loss=0.3558 train_acc=0.8438 train_f1=0.3131 val_loss=0.3173 val_acc=0.8436 val_f1=0.3127 \n",
            "Epoch 51/174 | train_loss=0.2875 train_acc=0.9136 train_f1=0.3503 val_loss=0.2638 val_acc=0.9138 val_f1=0.3501 \n",
            "Epoch 68/174 | train_loss=0.2539 train_acc=0.9277 train_f1=0.3825 val_loss=0.2304 val_acc=0.9269 val_f1=0.3831 \n",
            "Epoch 85/174 | train_loss=0.2230 train_acc=0.9405 train_f1=0.4495 val_loss=0.2032 val_acc=0.9408 val_f1=0.4483 \n",
            "Epoch 102/174 | train_loss=0.1978 train_acc=0.9470 train_f1=0.4808 val_loss=0.1765 val_acc=0.9474 val_f1=0.4796 \n",
            "Epoch 119/174 | train_loss=0.1795 train_acc=0.9503 train_f1=0.5005 val_loss=0.1575 val_acc=0.9506 val_f1=0.4988 \n",
            "Epoch 136/174 | train_loss=0.1662 train_acc=0.9544 train_f1=0.5085 val_loss=0.1482 val_acc=0.9545 val_f1=0.5069 \n",
            "Epoch 153/174 | train_loss=0.1551 train_acc=0.9547 train_f1=0.5148 val_loss=0.1414 val_acc=0.9547 val_f1=0.5118 \n",
            "Epoch 170/174 | train_loss=0.1500 train_acc=0.9555 train_f1=0.5172 val_loss=0.1355 val_acc=0.9549 val_f1=0.5139 \n",
            "Evaluating member 9: Hyperparameters(lr=0.0035640923136273633, epochs=195, hidden=(109, 112, 8, 32), dropout_rate=0.2986852506288171, patience=27)\n",
            "Epoch 19/195 | train_loss=0.6601 train_acc=0.1131 train_f1=0.0676 val_loss=0.6007 val_acc=0.1131 val_f1=0.0676 \n",
            "Epoch 38/195 | train_loss=0.5371 train_acc=0.4915 train_f1=0.1848 val_loss=0.4984 val_acc=0.4913 val_f1=0.1848 \n",
            "Epoch 57/195 | train_loss=0.4468 train_acc=0.7941 train_f1=0.2920 val_loss=0.4064 val_acc=0.7949 val_f1=0.2924 \n",
            "Epoch 76/195 | train_loss=0.3796 train_acc=0.9017 train_f1=0.3443 val_loss=0.3307 val_acc=0.9031 val_f1=0.3450 \n",
            "Epoch 95/195 | train_loss=0.3213 train_acc=0.9148 train_f1=0.3514 val_loss=0.2716 val_acc=0.9162 val_f1=0.3520 \n",
            "Epoch 114/195 | train_loss=0.2877 train_acc=0.9207 train_f1=0.3549 val_loss=0.2420 val_acc=0.9211 val_f1=0.3549 \n",
            "Epoch 133/195 | train_loss=0.2602 train_acc=0.9312 train_f1=0.3617 val_loss=0.2233 val_acc=0.9308 val_f1=0.3613 \n",
            "Epoch 152/195 | train_loss=0.2430 train_acc=0.9370 train_f1=0.3652 val_loss=0.2087 val_acc=0.9367 val_f1=0.3646 \n",
            "Epoch 171/195 | train_loss=0.2268 train_acc=0.9389 train_f1=0.3661 val_loss=0.1956 val_acc=0.9388 val_f1=0.3657 \n",
            "Epoch 190/195 | train_loss=0.2171 train_acc=0.9389 train_f1=0.3660 val_loss=0.1805 val_acc=0.9388 val_f1=0.3656 \n",
            "Epoch 16/166 | train_loss=0.6383 train_acc=0.0676 train_f1=0.0532 val_loss=0.6065 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/166 | train_loss=0.4581 train_acc=0.7391 train_f1=0.2687 val_loss=0.4266 val_acc=0.7390 val_f1=0.2690 \n",
            "Epoch 48/166 | train_loss=0.3195 train_acc=0.9092 train_f1=0.3474 val_loss=0.2839 val_acc=0.9094 val_f1=0.3473 \n",
            "Epoch 64/166 | train_loss=0.2762 train_acc=0.9250 train_f1=0.3571 val_loss=0.2468 val_acc=0.9251 val_f1=0.3571 \n",
            "Epoch 80/166 | train_loss=0.2475 train_acc=0.9321 train_f1=0.3659 val_loss=0.2215 val_acc=0.9321 val_f1=0.3659 \n",
            "Epoch 96/166 | train_loss=0.2274 train_acc=0.9413 train_f1=0.4288 val_loss=0.1975 val_acc=0.9409 val_f1=0.4295 \n",
            "Epoch 112/166 | train_loss=0.2084 train_acc=0.9452 train_f1=0.4672 val_loss=0.1773 val_acc=0.9449 val_f1=0.4645 \n",
            "Epoch 128/166 | train_loss=0.1937 train_acc=0.9486 train_f1=0.4928 val_loss=0.1635 val_acc=0.9476 val_f1=0.4885 \n",
            "Epoch 144/166 | train_loss=0.1844 train_acc=0.9524 train_f1=0.5034 val_loss=0.1545 val_acc=0.9515 val_f1=0.4993 \n",
            "Epoch 160/166 | train_loss=0.1753 train_acc=0.9523 train_f1=0.5076 val_loss=0.1459 val_acc=0.9518 val_f1=0.5041 \n",
            "Epoch 20/202 | train_loss=0.4287 train_acc=0.8004 train_f1=0.2934 val_loss=0.3800 val_acc=0.8010 val_f1=0.2939 \n",
            "Epoch 40/202 | train_loss=0.3206 train_acc=0.8969 train_f1=0.3403 val_loss=0.2884 val_acc=0.8973 val_f1=0.3403 \n",
            "Epoch 60/202 | train_loss=0.2875 train_acc=0.9015 train_f1=0.3434 val_loss=0.2469 val_acc=0.9018 val_f1=0.3433 \n",
            "Epoch 80/202 | train_loss=0.2592 train_acc=0.9235 train_f1=0.4323 val_loss=0.2170 val_acc=0.9239 val_f1=0.4288 \n",
            "Epoch 100/202 | train_loss=0.2345 train_acc=0.9330 train_f1=0.4676 val_loss=0.1977 val_acc=0.9338 val_f1=0.4687 \n",
            "Epoch 120/202 | train_loss=0.2191 train_acc=0.9359 train_f1=0.4771 val_loss=0.1850 val_acc=0.9366 val_f1=0.4766 \n",
            "Epoch 140/202 | train_loss=0.2082 train_acc=0.9401 train_f1=0.4835 val_loss=0.1765 val_acc=0.9407 val_f1=0.4829 \n",
            "Epoch 160/202 | train_loss=0.1997 train_acc=0.9405 train_f1=0.4865 val_loss=0.1690 val_acc=0.9405 val_f1=0.4849 \n",
            "Epoch 180/202 | train_loss=0.1955 train_acc=0.9394 train_f1=0.4863 val_loss=0.1627 val_acc=0.9390 val_f1=0.4833 \n",
            "Early stopping at epoch 185\n",
            "Epoch 18/186 | train_loss=0.3365 train_acc=0.8372 train_f1=0.3380 val_loss=0.3059 val_acc=0.8364 val_f1=0.3394 \n",
            "Epoch 36/186 | train_loss=0.2603 train_acc=0.9037 train_f1=0.4014 val_loss=0.2436 val_acc=0.9050 val_f1=0.4027 \n",
            "Epoch 54/186 | train_loss=0.2297 train_acc=0.9254 train_f1=0.4281 val_loss=0.2180 val_acc=0.9259 val_f1=0.4275 \n",
            "Epoch 72/186 | train_loss=0.2168 train_acc=0.9348 train_f1=0.4432 val_loss=0.2039 val_acc=0.9350 val_f1=0.4428 \n",
            "Epoch 90/186 | train_loss=0.2055 train_acc=0.9384 train_f1=0.4513 val_loss=0.1932 val_acc=0.9386 val_f1=0.4507 \n",
            "Epoch 108/186 | train_loss=0.1955 train_acc=0.9434 train_f1=0.4618 val_loss=0.1838 val_acc=0.9429 val_f1=0.4593 \n",
            "Epoch 126/186 | train_loss=0.1904 train_acc=0.9457 train_f1=0.4709 val_loss=0.1761 val_acc=0.9450 val_f1=0.4690 \n",
            "Epoch 144/186 | train_loss=0.1846 train_acc=0.9471 train_f1=0.4763 val_loss=0.1715 val_acc=0.9464 val_f1=0.4750 \n",
            "Epoch 162/186 | train_loss=0.1817 train_acc=0.9471 train_f1=0.4819 val_loss=0.1683 val_acc=0.9460 val_f1=0.4779 \n",
            "Epoch 180/186 | train_loss=0.1789 train_acc=0.9494 train_f1=0.4857 val_loss=0.1649 val_acc=0.9481 val_f1=0.4805 \n",
            "Epoch 18/187 | train_loss=0.4141 train_acc=0.8138 train_f1=0.2993 val_loss=0.3731 val_acc=0.8136 val_f1=0.2994 \n",
            "Epoch 36/187 | train_loss=0.3055 train_acc=0.8798 train_f1=0.3321 val_loss=0.2733 val_acc=0.8803 val_f1=0.3324 \n",
            "Epoch 54/187 | train_loss=0.2681 train_acc=0.9128 train_f1=0.3498 val_loss=0.2342 val_acc=0.9130 val_f1=0.3498 \n",
            "Epoch 72/187 | train_loss=0.2399 train_acc=0.9291 train_f1=0.4352 val_loss=0.2076 val_acc=0.9294 val_f1=0.4349 \n",
            "Epoch 90/187 | train_loss=0.2211 train_acc=0.9326 train_f1=0.4669 val_loss=0.1886 val_acc=0.9331 val_f1=0.4661 \n",
            "Epoch 108/187 | train_loss=0.2057 train_acc=0.9379 train_f1=0.4759 val_loss=0.1764 val_acc=0.9382 val_f1=0.4749 \n",
            "Epoch 126/187 | train_loss=0.1962 train_acc=0.9390 train_f1=0.4880 val_loss=0.1672 val_acc=0.9388 val_f1=0.4858 \n",
            "Epoch 144/187 | train_loss=0.1907 train_acc=0.9443 train_f1=0.4927 val_loss=0.1626 val_acc=0.9439 val_f1=0.4893 \n",
            "Early stopping at epoch 153\n",
            "Epoch 16/165 | train_loss=0.4188 train_acc=0.8868 train_f1=0.3335 val_loss=0.3654 val_acc=0.8864 val_f1=0.3336 \n",
            "Epoch 32/165 | train_loss=0.2872 train_acc=0.8931 train_f1=0.3392 val_loss=0.2713 val_acc=0.8936 val_f1=0.3393 \n",
            "Epoch 48/165 | train_loss=0.2462 train_acc=0.9110 train_f1=0.3739 val_loss=0.2318 val_acc=0.9113 val_f1=0.3732 \n",
            "Epoch 64/165 | train_loss=0.2125 train_acc=0.9209 train_f1=0.4388 val_loss=0.1976 val_acc=0.9212 val_f1=0.4382 \n",
            "Epoch 80/165 | train_loss=0.1850 train_acc=0.9374 train_f1=0.4735 val_loss=0.1723 val_acc=0.9370 val_f1=0.4702 \n",
            "Epoch 96/165 | train_loss=0.1751 train_acc=0.9432 train_f1=0.4976 val_loss=0.1619 val_acc=0.9424 val_f1=0.4933 \n",
            "Epoch 112/165 | train_loss=0.1625 train_acc=0.9469 train_f1=0.5048 val_loss=0.1517 val_acc=0.9453 val_f1=0.4984 \n",
            "Epoch 128/165 | train_loss=0.1547 train_acc=0.9465 train_f1=0.5043 val_loss=0.1468 val_acc=0.9444 val_f1=0.4978 \n",
            "Epoch 144/165 | train_loss=0.1517 train_acc=0.9452 train_f1=0.5105 val_loss=0.1426 val_acc=0.9433 val_f1=0.5045 \n",
            "Epoch 160/165 | train_loss=0.1495 train_acc=0.9451 train_f1=0.5099 val_loss=0.1421 val_acc=0.9435 val_f1=0.5040 \n",
            "Epoch 20/208 | train_loss=0.6518 train_acc=0.0676 train_f1=0.0532 val_loss=0.6206 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/208 | train_loss=0.5157 train_acc=0.3163 train_f1=0.1305 val_loss=0.4737 val_acc=0.3173 val_f1=0.1309 \n",
            "Epoch 60/208 | train_loss=0.3981 train_acc=0.9125 train_f1=0.3500 val_loss=0.3579 val_acc=0.9130 val_f1=0.3502 \n",
            "Epoch 80/208 | train_loss=0.3246 train_acc=0.9367 train_f1=0.3645 val_loss=0.2826 val_acc=0.9371 val_f1=0.3646 \n",
            "Epoch 100/208 | train_loss=0.2884 train_acc=0.9424 train_f1=0.3687 val_loss=0.2641 val_acc=0.9427 val_f1=0.3687 \n",
            "Epoch 120/208 | train_loss=0.2715 train_acc=0.9476 train_f1=0.3732 val_loss=0.2475 val_acc=0.9475 val_f1=0.3728 \n",
            "Epoch 140/208 | train_loss=0.2534 train_acc=0.9494 train_f1=0.3748 val_loss=0.2291 val_acc=0.9489 val_f1=0.3743 \n",
            "Epoch 160/208 | train_loss=0.2380 train_acc=0.9513 train_f1=0.3763 val_loss=0.2166 val_acc=0.9511 val_f1=0.3760 \n",
            "Epoch 180/208 | train_loss=0.2254 train_acc=0.9523 train_f1=0.3772 val_loss=0.2021 val_acc=0.9521 val_f1=0.3767 \n",
            "Epoch 200/208 | train_loss=0.2127 train_acc=0.9545 train_f1=0.3793 val_loss=0.1967 val_acc=0.9537 val_f1=0.3784 \n",
            "Epoch 20/204 | train_loss=0.7413 train_acc=0.0676 train_f1=0.0532 val_loss=0.6736 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/204 | train_loss=0.5863 train_acc=0.0676 train_f1=0.0532 val_loss=0.5454 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 60/204 | train_loss=0.4968 train_acc=0.4946 train_f1=0.1858 val_loss=0.4605 val_acc=0.4945 val_f1=0.1858 \n",
            "Epoch 80/204 | train_loss=0.3947 train_acc=0.8892 train_f1=0.3365 val_loss=0.3400 val_acc=0.8890 val_f1=0.3361 \n",
            "Epoch 100/204 | train_loss=0.3410 train_acc=0.9192 train_f1=0.3534 val_loss=0.2863 val_acc=0.9203 val_f1=0.3538 \n",
            "Epoch 120/204 | train_loss=0.3081 train_acc=0.9259 train_f1=0.3572 val_loss=0.2641 val_acc=0.9265 val_f1=0.3574 \n",
            "Epoch 140/204 | train_loss=0.2877 train_acc=0.9295 train_f1=0.3594 val_loss=0.2492 val_acc=0.9299 val_f1=0.3592 \n",
            "Epoch 160/204 | train_loss=0.2657 train_acc=0.9337 train_f1=0.3620 val_loss=0.2367 val_acc=0.9336 val_f1=0.3615 \n",
            "Epoch 180/204 | train_loss=0.2525 train_acc=0.9383 train_f1=0.3651 val_loss=0.2241 val_acc=0.9383 val_f1=0.3646 \n",
            "Epoch 200/204 | train_loss=0.2440 train_acc=0.9399 train_f1=0.3737 val_loss=0.2137 val_acc=0.9401 val_f1=0.3736 \n",
            "Epoch 18/187 | train_loss=0.5854 train_acc=0.0929 train_f1=0.0612 val_loss=0.5613 val_acc=0.0934 val_f1=0.0614 \n",
            "Epoch 36/187 | train_loss=0.4476 train_acc=0.5524 train_f1=0.2042 val_loss=0.4356 val_acc=0.5517 val_f1=0.2040 \n",
            "Epoch 54/187 | train_loss=0.2865 train_acc=0.9065 train_f1=0.3464 val_loss=0.2800 val_acc=0.9070 val_f1=0.3464 \n",
            "Epoch 72/187 | train_loss=0.2338 train_acc=0.9250 train_f1=0.3576 val_loss=0.2293 val_acc=0.9246 val_f1=0.3570 \n",
            "Epoch 90/187 | train_loss=0.1832 train_acc=0.9414 train_f1=0.4531 val_loss=0.1803 val_acc=0.9408 val_f1=0.4505 \n",
            "Epoch 108/187 | train_loss=0.1458 train_acc=0.9454 train_f1=0.4915 val_loss=0.1489 val_acc=0.9448 val_f1=0.4889 \n",
            "Epoch 126/187 | train_loss=0.1250 train_acc=0.9488 train_f1=0.5094 val_loss=0.1277 val_acc=0.9492 val_f1=0.5071 \n",
            "Epoch 144/187 | train_loss=0.1080 train_acc=0.9522 train_f1=0.5156 val_loss=0.1167 val_acc=0.9514 val_f1=0.5120 \n",
            "Epoch 162/187 | train_loss=0.0990 train_acc=0.9583 train_f1=0.5243 val_loss=0.1078 val_acc=0.9578 val_f1=0.5210 \n",
            "Epoch 180/187 | train_loss=0.0906 train_acc=0.9627 train_f1=0.5345 val_loss=0.1046 val_acc=0.9620 val_f1=0.5305 \n",
            "Epoch 18/189 | train_loss=0.3481 train_acc=0.8741 train_f1=0.3279 val_loss=0.3174 val_acc=0.8752 val_f1=0.3283 \n",
            "Epoch 36/189 | train_loss=0.2772 train_acc=0.9087 train_f1=0.3480 val_loss=0.2599 val_acc=0.9088 val_f1=0.3475 \n",
            "Epoch 54/189 | train_loss=0.2420 train_acc=0.9251 train_f1=0.3939 val_loss=0.2253 val_acc=0.9248 val_f1=0.3911 \n",
            "Epoch 72/189 | train_loss=0.2117 train_acc=0.9304 train_f1=0.4531 val_loss=0.1933 val_acc=0.9300 val_f1=0.4506 \n",
            "Epoch 90/189 | train_loss=0.1842 train_acc=0.9395 train_f1=0.4808 val_loss=0.1687 val_acc=0.9397 val_f1=0.4785 \n",
            "Epoch 108/189 | train_loss=0.1677 train_acc=0.9460 train_f1=0.4984 val_loss=0.1540 val_acc=0.9453 val_f1=0.4945 \n",
            "Epoch 126/189 | train_loss=0.1575 train_acc=0.9459 train_f1=0.5060 val_loss=0.1473 val_acc=0.9450 val_f1=0.5028 \n",
            "Epoch 144/189 | train_loss=0.1464 train_acc=0.9501 train_f1=0.5071 val_loss=0.1402 val_acc=0.9503 val_f1=0.5037 \n",
            "Early stopping at epoch 153\n",
            "Epoch 19/196 | train_loss=0.6617 train_acc=0.3014 train_f1=0.1260 val_loss=0.5866 val_acc=0.3001 val_f1=0.1256 \n",
            "Epoch 38/196 | train_loss=0.5686 train_acc=0.4945 train_f1=0.1858 val_loss=0.4910 val_acc=0.4941 val_f1=0.1857 \n",
            "Epoch 57/196 | train_loss=0.4876 train_acc=0.7797 train_f1=0.2860 val_loss=0.4170 val_acc=0.7800 val_f1=0.2862 \n",
            "Epoch 76/196 | train_loss=0.4047 train_acc=0.9097 train_f1=0.3486 val_loss=0.3156 val_acc=0.9107 val_f1=0.3490 \n",
            "Epoch 95/196 | train_loss=0.3503 train_acc=0.9171 train_f1=0.3528 val_loss=0.2638 val_acc=0.9165 val_f1=0.3521 \n",
            "Epoch 114/196 | train_loss=0.3097 train_acc=0.9266 train_f1=0.3587 val_loss=0.2380 val_acc=0.9261 val_f1=0.3580 \n",
            "Epoch 133/196 | train_loss=0.2831 train_acc=0.9291 train_f1=0.3599 val_loss=0.2191 val_acc=0.9285 val_f1=0.3591 \n",
            "Epoch 152/196 | train_loss=0.2515 train_acc=0.9343 train_f1=0.3627 val_loss=0.2028 val_acc=0.9338 val_f1=0.3619 \n",
            "Epoch 171/196 | train_loss=0.2362 train_acc=0.9332 train_f1=0.3614 val_loss=0.1895 val_acc=0.9334 val_f1=0.3611 \n",
            "Epoch 190/196 | train_loss=0.2110 train_acc=0.9352 train_f1=0.3624 val_loss=0.1788 val_acc=0.9355 val_f1=0.3622 \n",
            "Gen 6 Best: {'epochs_run': 211, 'val_loss': 0.14033566415309906, 'val_accuracy': 0.9550961349813616, 'val_f1': 0.512956895430126, 'flops': 19660}\n",
            "Generation 7 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.004038975263396996, epochs=166, hidden=(64, 19, 143, 129), dropout_rate=0.21988872192157918, patience=23)\n",
            "Epoch 16/166 | train_loss=0.5881 train_acc=0.0676 train_f1=0.0532 val_loss=0.5589 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/166 | train_loss=0.4428 train_acc=0.7698 train_f1=0.2813 val_loss=0.4125 val_acc=0.7692 val_f1=0.2812 \n",
            "Epoch 48/166 | train_loss=0.3179 train_acc=0.9191 train_f1=0.3532 val_loss=0.2818 val_acc=0.9183 val_f1=0.3526 \n",
            "Epoch 64/166 | train_loss=0.2771 train_acc=0.9296 train_f1=0.3602 val_loss=0.2438 val_acc=0.9289 val_f1=0.3595 \n",
            "Epoch 80/166 | train_loss=0.2409 train_acc=0.9372 train_f1=0.3971 val_loss=0.2098 val_acc=0.9369 val_f1=0.3980 \n",
            "Epoch 96/166 | train_loss=0.2156 train_acc=0.9440 train_f1=0.4636 val_loss=0.1842 val_acc=0.9437 val_f1=0.4644 \n",
            "Epoch 112/166 | train_loss=0.1961 train_acc=0.9466 train_f1=0.4853 val_loss=0.1672 val_acc=0.9462 val_f1=0.4851 \n",
            "Epoch 128/166 | train_loss=0.1838 train_acc=0.9512 train_f1=0.4982 val_loss=0.1558 val_acc=0.9512 val_f1=0.4966 \n",
            "Epoch 144/166 | train_loss=0.1722 train_acc=0.9515 train_f1=0.5081 val_loss=0.1488 val_acc=0.9517 val_f1=0.5061 \n",
            "Epoch 160/166 | train_loss=0.1635 train_acc=0.9526 train_f1=0.5120 val_loss=0.1407 val_acc=0.9525 val_f1=0.5086 \n",
            "Evaluating member 1: Hyperparameters(lr=0.04901103349141309, epochs=202, hidden=(32, 51, 245), dropout_rate=0.2455505762032803, patience=6)\n",
            "Epoch 20/202 | train_loss=0.4205 train_acc=0.8994 train_f1=0.3396 val_loss=0.3610 val_acc=0.8997 val_f1=0.3398 \n",
            "Epoch 40/202 | train_loss=0.3063 train_acc=0.8802 train_f1=0.3315 val_loss=0.2698 val_acc=0.8813 val_f1=0.3322 \n",
            "Epoch 60/202 | train_loss=0.2659 train_acc=0.9168 train_f1=0.3655 val_loss=0.2273 val_acc=0.9177 val_f1=0.3626 \n",
            "Epoch 80/202 | train_loss=0.2345 train_acc=0.9311 train_f1=0.4578 val_loss=0.1990 val_acc=0.9322 val_f1=0.4573 \n",
            "Epoch 100/202 | train_loss=0.2180 train_acc=0.9339 train_f1=0.4690 val_loss=0.1821 val_acc=0.9338 val_f1=0.4662 \n",
            "Epoch 120/202 | train_loss=0.2036 train_acc=0.9360 train_f1=0.4812 val_loss=0.1684 val_acc=0.9361 val_f1=0.4776 \n",
            "Early stopping at epoch 136\n",
            "Evaluating member 2: Hyperparameters(lr=0.1, epochs=186, hidden=(26,), dropout_rate=0.1942226213222236, patience=30)\n",
            "Epoch 18/186 | train_loss=0.3202 train_acc=0.8828 train_f1=0.3469 val_loss=0.2959 val_acc=0.8840 val_f1=0.3482 \n",
            "Epoch 36/186 | train_loss=0.2514 train_acc=0.9208 train_f1=0.4096 val_loss=0.2403 val_acc=0.9214 val_f1=0.4104 \n",
            "Epoch 54/186 | train_loss=0.2268 train_acc=0.9241 train_f1=0.4313 val_loss=0.2183 val_acc=0.9235 val_f1=0.4291 \n",
            "Epoch 72/186 | train_loss=0.2109 train_acc=0.9297 train_f1=0.4454 val_loss=0.2024 val_acc=0.9296 val_f1=0.4431 \n",
            "Epoch 90/186 | train_loss=0.2036 train_acc=0.9362 train_f1=0.4562 val_loss=0.1923 val_acc=0.9350 val_f1=0.4531 \n",
            "Epoch 108/186 | train_loss=0.1936 train_acc=0.9410 train_f1=0.4665 val_loss=0.1831 val_acc=0.9398 val_f1=0.4621 \n",
            "Epoch 126/186 | train_loss=0.1862 train_acc=0.9437 train_f1=0.4731 val_loss=0.1761 val_acc=0.9430 val_f1=0.4711 \n",
            "Epoch 144/186 | train_loss=0.1834 train_acc=0.9442 train_f1=0.4759 val_loss=0.1725 val_acc=0.9437 val_f1=0.4740 \n",
            "Epoch 162/186 | train_loss=0.1781 train_acc=0.9484 train_f1=0.4819 val_loss=0.1673 val_acc=0.9476 val_f1=0.4793 \n",
            "Epoch 180/186 | train_loss=0.1752 train_acc=0.9512 train_f1=0.4912 val_loss=0.1628 val_acc=0.9508 val_f1=0.4883 \n",
            "Evaluating member 3: Hyperparameters(lr=0.046372265615574934, epochs=187, hidden=(32, 64, 243), dropout_rate=0.20172283699479723, patience=7)\n",
            "Epoch 18/187 | train_loss=0.4768 train_acc=0.6508 train_f1=0.2365 val_loss=0.4482 val_acc=0.6511 val_f1=0.2368 \n",
            "Epoch 36/187 | train_loss=0.3391 train_acc=0.8497 train_f1=0.3166 val_loss=0.3031 val_acc=0.8498 val_f1=0.3167 \n",
            "Epoch 54/187 | train_loss=0.2969 train_acc=0.8945 train_f1=0.3400 val_loss=0.2632 val_acc=0.8947 val_f1=0.3399 \n",
            "Epoch 72/187 | train_loss=0.2707 train_acc=0.9086 train_f1=0.3696 val_loss=0.2358 val_acc=0.9083 val_f1=0.3666 \n",
            "Epoch 90/187 | train_loss=0.2439 train_acc=0.9290 train_f1=0.4330 val_loss=0.2106 val_acc=0.9295 val_f1=0.4324 \n",
            "Epoch 108/187 | train_loss=0.2222 train_acc=0.9418 train_f1=0.4765 val_loss=0.1906 val_acc=0.9418 val_f1=0.4742 \n",
            "Epoch 126/187 | train_loss=0.2092 train_acc=0.9419 train_f1=0.4809 val_loss=0.1770 val_acc=0.9421 val_f1=0.4789 \n",
            "Epoch 144/187 | train_loss=0.1995 train_acc=0.9415 train_f1=0.4854 val_loss=0.1674 val_acc=0.9417 val_f1=0.4836 \n",
            "Epoch 162/187 | train_loss=0.1907 train_acc=0.9463 train_f1=0.4901 val_loss=0.1596 val_acc=0.9456 val_f1=0.4863 \n",
            "Epoch 180/187 | train_loss=0.1840 train_acc=0.9470 train_f1=0.4927 val_loss=0.1545 val_acc=0.9468 val_f1=0.4899 \n",
            "Evaluating member 4: Hyperparameters(lr=0.04524853533077976, epochs=165, hidden=(17, 32, 64, 229), dropout_rate=0.05030769657555457, patience=11)\n",
            "Epoch 16/165 | train_loss=0.4387 train_acc=0.8983 train_f1=0.3359 val_loss=0.4189 val_acc=0.8972 val_f1=0.3360 \n",
            "Epoch 32/165 | train_loss=0.2991 train_acc=0.8699 train_f1=0.3274 val_loss=0.2792 val_acc=0.8708 val_f1=0.3279 \n",
            "Epoch 48/165 | train_loss=0.2525 train_acc=0.9104 train_f1=0.3491 val_loss=0.2376 val_acc=0.9100 val_f1=0.3486 \n",
            "Epoch 64/165 | train_loss=0.2247 train_acc=0.9219 train_f1=0.4276 val_loss=0.2097 val_acc=0.9219 val_f1=0.4269 \n",
            "Epoch 80/165 | train_loss=0.2030 train_acc=0.9317 train_f1=0.4609 val_loss=0.1903 val_acc=0.9313 val_f1=0.4585 \n",
            "Epoch 96/165 | train_loss=0.1904 train_acc=0.9372 train_f1=0.4747 val_loss=0.1787 val_acc=0.9372 val_f1=0.4709 \n",
            "Epoch 112/165 | train_loss=0.1801 train_acc=0.9404 train_f1=0.4867 val_loss=0.1711 val_acc=0.9401 val_f1=0.4825 \n",
            "Epoch 128/165 | train_loss=0.1750 train_acc=0.9398 train_f1=0.4907 val_loss=0.1665 val_acc=0.9398 val_f1=0.4865 \n",
            "Epoch 144/165 | train_loss=0.1657 train_acc=0.9461 train_f1=0.4929 val_loss=0.1570 val_acc=0.9461 val_f1=0.4887 \n",
            "Epoch 160/165 | train_loss=0.1592 train_acc=0.9450 train_f1=0.4985 val_loss=0.1527 val_acc=0.9442 val_f1=0.4932 \n",
            "Evaluating member 5: Hyperparameters(lr=0.003587400087944944, epochs=208, hidden=(109, 112, 8, 64, 32), dropout_rate=0.3321562682917286, patience=30)\n",
            "Epoch 20/208 | train_loss=0.6671 train_acc=0.1108 train_f1=0.0669 val_loss=0.6171 val_acc=0.1108 val_f1=0.0669 \n",
            "Epoch 40/208 | train_loss=0.5078 train_acc=0.4098 train_f1=0.1592 val_loss=0.4720 val_acc=0.4116 val_f1=0.1598 \n",
            "Epoch 60/208 | train_loss=0.3993 train_acc=0.9216 train_f1=0.3549 val_loss=0.3643 val_acc=0.9224 val_f1=0.3552 \n",
            "Epoch 80/208 | train_loss=0.3221 train_acc=0.9418 train_f1=0.3676 val_loss=0.2896 val_acc=0.9419 val_f1=0.3674 \n",
            "Epoch 100/208 | train_loss=0.2842 train_acc=0.9490 train_f1=0.3731 val_loss=0.2616 val_acc=0.9489 val_f1=0.3728 \n",
            "Epoch 120/208 | train_loss=0.2598 train_acc=0.9503 train_f1=0.3747 val_loss=0.2378 val_acc=0.9500 val_f1=0.3742 \n",
            "Epoch 140/208 | train_loss=0.2437 train_acc=0.9505 train_f1=0.3744 val_loss=0.2140 val_acc=0.9505 val_f1=0.3742 \n",
            "Epoch 160/208 | train_loss=0.2241 train_acc=0.9520 train_f1=0.3753 val_loss=0.1961 val_acc=0.9521 val_f1=0.3752 \n",
            "Epoch 180/208 | train_loss=0.2124 train_acc=0.9524 train_f1=0.3908 val_loss=0.1805 val_acc=0.9529 val_f1=0.3920 \n",
            "Epoch 200/208 | train_loss=0.1970 train_acc=0.9601 train_f1=0.4539 val_loss=0.1711 val_acc=0.9603 val_f1=0.4533 \n",
            "Evaluating member 6: Hyperparameters(lr=0.003506727360643074, epochs=204, hidden=(109, 8, 32), dropout_rate=0.25651789656004775, patience=29)\n",
            "Epoch 20/204 | train_loss=0.6583 train_acc=0.0676 train_f1=0.0532 val_loss=0.5979 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/204 | train_loss=0.4905 train_acc=0.6373 train_f1=0.2320 val_loss=0.4386 val_acc=0.6366 val_f1=0.2319 \n",
            "Epoch 60/204 | train_loss=0.3676 train_acc=0.8884 train_f1=0.3360 val_loss=0.3045 val_acc=0.8887 val_f1=0.3361 \n",
            "Epoch 80/204 | train_loss=0.3162 train_acc=0.9142 train_f1=0.3508 val_loss=0.2653 val_acc=0.9148 val_f1=0.3510 \n",
            "Epoch 100/204 | train_loss=0.2908 train_acc=0.9241 train_f1=0.3570 val_loss=0.2437 val_acc=0.9243 val_f1=0.3570 \n",
            "Epoch 120/204 | train_loss=0.2694 train_acc=0.9297 train_f1=0.3608 val_loss=0.2252 val_acc=0.9291 val_f1=0.3601 \n",
            "Epoch 140/204 | train_loss=0.2484 train_acc=0.9370 train_f1=0.3656 val_loss=0.2100 val_acc=0.9364 val_f1=0.3647 \n",
            "Epoch 160/204 | train_loss=0.2340 train_acc=0.9404 train_f1=0.3682 val_loss=0.1975 val_acc=0.9402 val_f1=0.3674 \n",
            "Epoch 180/204 | train_loss=0.2242 train_acc=0.9418 train_f1=0.3832 val_loss=0.1845 val_acc=0.9416 val_f1=0.3817 \n",
            "Epoch 200/204 | train_loss=0.2150 train_acc=0.9456 train_f1=0.4133 val_loss=0.1770 val_acc=0.9459 val_f1=0.4152 \n",
            "Evaluating member 7: Hyperparameters(lr=0.003598157980097085, epochs=187, hidden=(114, 112, 32, 128), dropout_rate=0.03730848559799194, patience=29)\n",
            "Epoch 18/187 | train_loss=0.6062 train_acc=0.2166 train_f1=0.0999 val_loss=0.5680 val_acc=0.2163 val_f1=0.0998 \n",
            "Epoch 36/187 | train_loss=0.4088 train_acc=0.6971 train_f1=0.2535 val_loss=0.3950 val_acc=0.6979 val_f1=0.2538 \n",
            "Epoch 54/187 | train_loss=0.2801 train_acc=0.9006 train_f1=0.3431 val_loss=0.2742 val_acc=0.9008 val_f1=0.3430 \n",
            "Epoch 72/187 | train_loss=0.2371 train_acc=0.9261 train_f1=0.3584 val_loss=0.2365 val_acc=0.9258 val_f1=0.3580 \n",
            "Epoch 90/187 | train_loss=0.2001 train_acc=0.9377 train_f1=0.4128 val_loss=0.2009 val_acc=0.9369 val_f1=0.4115 \n",
            "Epoch 108/187 | train_loss=0.1647 train_acc=0.9410 train_f1=0.4700 val_loss=0.1651 val_acc=0.9403 val_f1=0.4678 \n",
            "Epoch 126/187 | train_loss=0.1388 train_acc=0.9491 train_f1=0.4948 val_loss=0.1430 val_acc=0.9485 val_f1=0.4906 \n",
            "Epoch 144/187 | train_loss=0.1213 train_acc=0.9542 train_f1=0.5136 val_loss=0.1278 val_acc=0.9528 val_f1=0.5080 \n",
            "Epoch 162/187 | train_loss=0.1061 train_acc=0.9553 train_f1=0.5200 val_loss=0.1159 val_acc=0.9543 val_f1=0.5161 \n",
            "Epoch 180/187 | train_loss=0.0949 train_acc=0.9601 train_f1=0.5291 val_loss=0.1084 val_acc=0.9590 val_f1=0.5242 \n",
            "Evaluating member 8: Hyperparameters(lr=0.0485220700998793, epochs=189, hidden=(32, 55, 245), dropout_rate=0.09823877343261134, patience=5)\n",
            "Epoch 18/189 | train_loss=0.3470 train_acc=0.9175 train_f1=0.3513 val_loss=0.3217 val_acc=0.9179 val_f1=0.3518 \n",
            "Epoch 36/189 | train_loss=0.2748 train_acc=0.9024 train_f1=0.3674 val_loss=0.2517 val_acc=0.9032 val_f1=0.3669 \n",
            "Epoch 54/189 | train_loss=0.2367 train_acc=0.9202 train_f1=0.4308 val_loss=0.2152 val_acc=0.9205 val_f1=0.4309 \n",
            "Epoch 72/189 | train_loss=0.2094 train_acc=0.9342 train_f1=0.4613 val_loss=0.1895 val_acc=0.9343 val_f1=0.4614 \n",
            "Epoch 90/189 | train_loss=0.1927 train_acc=0.9487 train_f1=0.4842 val_loss=0.1738 val_acc=0.9486 val_f1=0.4828 \n",
            "Epoch 108/189 | train_loss=0.1806 train_acc=0.9497 train_f1=0.4902 val_loss=0.1600 val_acc=0.9499 val_f1=0.4893 \n",
            "Epoch 126/189 | train_loss=0.1683 train_acc=0.9500 train_f1=0.4943 val_loss=0.1524 val_acc=0.9498 val_f1=0.4937 \n",
            "Epoch 144/189 | train_loss=0.1599 train_acc=0.9529 train_f1=0.4982 val_loss=0.1461 val_acc=0.9527 val_f1=0.4961 \n",
            "Epoch 162/189 | train_loss=0.1549 train_acc=0.9543 train_f1=0.5023 val_loss=0.1396 val_acc=0.9539 val_f1=0.5003 \n",
            "Epoch 180/189 | train_loss=0.1470 train_acc=0.9534 train_f1=0.5019 val_loss=0.1339 val_acc=0.9530 val_f1=0.4992 \n",
            "Evaluating member 9: Hyperparameters(lr=0.003708686696692238, epochs=196, hidden=(109, 110, 8, 32), dropout_rate=0.24705218040334334, patience=29)\n",
            "Epoch 19/196 | train_loss=0.6558 train_acc=0.2058 train_f1=0.0965 val_loss=0.5850 val_acc=0.2047 val_f1=0.0962 \n",
            "Epoch 38/196 | train_loss=0.5335 train_acc=0.4666 train_f1=0.1770 val_loss=0.4901 val_acc=0.4684 val_f1=0.1776 \n",
            "Epoch 57/196 | train_loss=0.4441 train_acc=0.7564 train_f1=0.2766 val_loss=0.4097 val_acc=0.7567 val_f1=0.2767 \n",
            "Epoch 76/196 | train_loss=0.3632 train_acc=0.9071 train_f1=0.3472 val_loss=0.3311 val_acc=0.9078 val_f1=0.3473 \n",
            "Epoch 95/196 | train_loss=0.3112 train_acc=0.9343 train_f1=0.3628 val_loss=0.2768 val_acc=0.9345 val_f1=0.3627 \n",
            "Epoch 114/196 | train_loss=0.2778 train_acc=0.9413 train_f1=0.3673 val_loss=0.2449 val_acc=0.9419 val_f1=0.3676 \n",
            "Epoch 133/196 | train_loss=0.2489 train_acc=0.9466 train_f1=0.3710 val_loss=0.2157 val_acc=0.9469 val_f1=0.3709 \n",
            "Epoch 152/196 | train_loss=0.2248 train_acc=0.9475 train_f1=0.3716 val_loss=0.1918 val_acc=0.9479 val_f1=0.3715 \n",
            "Epoch 171/196 | train_loss=0.2055 train_acc=0.9507 train_f1=0.3990 val_loss=0.1752 val_acc=0.9510 val_f1=0.3999 \n",
            "Epoch 190/196 | train_loss=0.1953 train_acc=0.9558 train_f1=0.4693 val_loss=0.1605 val_acc=0.9564 val_f1=0.4703 \n",
            "Epoch 20/205 | train_loss=0.7034 train_acc=0.0676 train_f1=0.0532 val_loss=0.6168 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/205 | train_loss=0.5553 train_acc=0.0676 train_f1=0.0532 val_loss=0.5035 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 60/205 | train_loss=0.4351 train_acc=0.9007 train_f1=0.3435 val_loss=0.3805 val_acc=0.9008 val_f1=0.3433 \n",
            "Epoch 80/205 | train_loss=0.3523 train_acc=0.9412 train_f1=0.3674 val_loss=0.2986 val_acc=0.9417 val_f1=0.3673 \n",
            "Epoch 100/205 | train_loss=0.3028 train_acc=0.9432 train_f1=0.3690 val_loss=0.2661 val_acc=0.9434 val_f1=0.3692 \n",
            "Epoch 120/205 | train_loss=0.2776 train_acc=0.9473 train_f1=0.3725 val_loss=0.2496 val_acc=0.9471 val_f1=0.3722 \n",
            "Epoch 140/205 | train_loss=0.2635 train_acc=0.9477 train_f1=0.3728 val_loss=0.2274 val_acc=0.9477 val_f1=0.3726 \n",
            "Epoch 160/205 | train_loss=0.2498 train_acc=0.9503 train_f1=0.3750 val_loss=0.2162 val_acc=0.9502 val_f1=0.3747 \n",
            "Epoch 180/205 | train_loss=0.2356 train_acc=0.9506 train_f1=0.3748 val_loss=0.2049 val_acc=0.9505 val_f1=0.3744 \n",
            "Epoch 200/205 | train_loss=0.2256 train_acc=0.9512 train_f1=0.3753 val_loss=0.1950 val_acc=0.9512 val_f1=0.3749 \n",
            "Epoch 18/188 | train_loss=0.5745 train_acc=0.0676 train_f1=0.0532 val_loss=0.5610 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/188 | train_loss=0.4151 train_acc=0.8322 train_f1=0.3083 val_loss=0.3735 val_acc=0.8320 val_f1=0.3084 \n",
            "Epoch 54/188 | train_loss=0.3037 train_acc=0.9092 train_f1=0.3478 val_loss=0.2727 val_acc=0.9096 val_f1=0.3480 \n",
            "Epoch 72/188 | train_loss=0.2673 train_acc=0.9238 train_f1=0.3572 val_loss=0.2451 val_acc=0.9235 val_f1=0.3569 \n",
            "Epoch 90/188 | train_loss=0.2409 train_acc=0.9335 train_f1=0.3632 val_loss=0.2177 val_acc=0.9330 val_f1=0.3626 \n",
            "Epoch 108/188 | train_loss=0.2127 train_acc=0.9442 train_f1=0.4513 val_loss=0.1893 val_acc=0.9440 val_f1=0.4517 \n",
            "Epoch 126/188 | train_loss=0.1906 train_acc=0.9489 train_f1=0.4884 val_loss=0.1683 val_acc=0.9488 val_f1=0.4865 \n",
            "Epoch 144/188 | train_loss=0.1771 train_acc=0.9505 train_f1=0.5027 val_loss=0.1535 val_acc=0.9511 val_f1=0.5019 \n",
            "Epoch 162/188 | train_loss=0.1649 train_acc=0.9533 train_f1=0.5106 val_loss=0.1432 val_acc=0.9534 val_f1=0.5086 \n",
            "Epoch 180/188 | train_loss=0.1541 train_acc=0.9545 train_f1=0.5164 val_loss=0.1353 val_acc=0.9546 val_f1=0.5133 \n",
            "Epoch 18/183 | train_loss=0.5795 train_acc=0.0676 train_f1=0.0532 val_loss=0.5596 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/183 | train_loss=0.4653 train_acc=0.4717 train_f1=0.1785 val_loss=0.4516 val_acc=0.4726 val_f1=0.1788 \n",
            "Epoch 54/183 | train_loss=0.2970 train_acc=0.8956 train_f1=0.3402 val_loss=0.2861 val_acc=0.8955 val_f1=0.3403 \n",
            "Epoch 72/183 | train_loss=0.2467 train_acc=0.9226 train_f1=0.3561 val_loss=0.2394 val_acc=0.9224 val_f1=0.3559 \n",
            "Epoch 90/183 | train_loss=0.2087 train_acc=0.9352 train_f1=0.3711 val_loss=0.2043 val_acc=0.9350 val_f1=0.3699 \n",
            "Epoch 108/183 | train_loss=0.1689 train_acc=0.9442 train_f1=0.4767 val_loss=0.1636 val_acc=0.9437 val_f1=0.4731 \n",
            "Epoch 126/183 | train_loss=0.1390 train_acc=0.9493 train_f1=0.5069 val_loss=0.1368 val_acc=0.9488 val_f1=0.5036 \n",
            "Epoch 144/183 | train_loss=0.1185 train_acc=0.9537 train_f1=0.5192 val_loss=0.1203 val_acc=0.9533 val_f1=0.5154 \n",
            "Epoch 162/183 | train_loss=0.1060 train_acc=0.9573 train_f1=0.5268 val_loss=0.1121 val_acc=0.9563 val_f1=0.5211 \n",
            "Epoch 180/183 | train_loss=0.0985 train_acc=0.9598 train_f1=0.5315 val_loss=0.1072 val_acc=0.9591 val_f1=0.5260 \n",
            "Epoch 19/193 | train_loss=0.6565 train_acc=0.0942 train_f1=0.0616 val_loss=0.5738 val_acc=0.0938 val_f1=0.0615 \n",
            "Epoch 38/193 | train_loss=0.5399 train_acc=0.2989 train_f1=0.1252 val_loss=0.5145 val_acc=0.3011 val_f1=0.1259 \n",
            "Epoch 57/193 | train_loss=0.4577 train_acc=0.7172 train_f1=0.2614 val_loss=0.4246 val_acc=0.7165 val_f1=0.2611 \n",
            "Epoch 76/193 | train_loss=0.3632 train_acc=0.9092 train_f1=0.3471 val_loss=0.3136 val_acc=0.9094 val_f1=0.3469 \n",
            "Epoch 95/193 | train_loss=0.3039 train_acc=0.9188 train_f1=0.3531 val_loss=0.2613 val_acc=0.9192 val_f1=0.3531 \n",
            "Epoch 114/193 | train_loss=0.2703 train_acc=0.9302 train_f1=0.3607 val_loss=0.2379 val_acc=0.9300 val_f1=0.3604 \n",
            "Epoch 133/193 | train_loss=0.2512 train_acc=0.9356 train_f1=0.3646 val_loss=0.2214 val_acc=0.9351 val_f1=0.3639 \n",
            "Epoch 152/193 | train_loss=0.2351 train_acc=0.9403 train_f1=0.3680 val_loss=0.2052 val_acc=0.9398 val_f1=0.3674 \n",
            "Epoch 171/193 | train_loss=0.2198 train_acc=0.9406 train_f1=0.3684 val_loss=0.1927 val_acc=0.9404 val_f1=0.3679 \n",
            "Epoch 190/193 | train_loss=0.2088 train_acc=0.9437 train_f1=0.3710 val_loss=0.1818 val_acc=0.9432 val_f1=0.3704 \n",
            "Epoch 17/172 | train_loss=0.3795 train_acc=0.8249 train_f1=0.3046 val_loss=0.3284 val_acc=0.8240 val_f1=0.3040 \n",
            "Epoch 34/172 | train_loss=0.3149 train_acc=0.8859 train_f1=0.3526 val_loss=0.2680 val_acc=0.8867 val_f1=0.3518 \n",
            "Epoch 51/172 | train_loss=0.2892 train_acc=0.9037 train_f1=0.3707 val_loss=0.2462 val_acc=0.9041 val_f1=0.3688 \n",
            "Epoch 68/172 | train_loss=0.2745 train_acc=0.9137 train_f1=0.3996 val_loss=0.2352 val_acc=0.9146 val_f1=0.3989 \n",
            "Epoch 85/172 | train_loss=0.2650 train_acc=0.9140 train_f1=0.4091 val_loss=0.2262 val_acc=0.9153 val_f1=0.4084 \n",
            "Epoch 102/172 | train_loss=0.2616 train_acc=0.9151 train_f1=0.4097 val_loss=0.2227 val_acc=0.9152 val_f1=0.4078 \n",
            "Epoch 119/172 | train_loss=0.2553 train_acc=0.9220 train_f1=0.4215 val_loss=0.2201 val_acc=0.9220 val_f1=0.4214 \n",
            "Epoch 136/172 | train_loss=0.2546 train_acc=0.9175 train_f1=0.4135 val_loss=0.2181 val_acc=0.9180 val_f1=0.4134 \n",
            "Epoch 153/172 | train_loss=0.2508 train_acc=0.9191 train_f1=0.4213 val_loss=0.2170 val_acc=0.9185 val_f1=0.4180 \n",
            "Epoch 170/172 | train_loss=0.2531 train_acc=0.9184 train_f1=0.4141 val_loss=0.2186 val_acc=0.9190 val_f1=0.4143 \n",
            "Epoch 20/201 | train_loss=0.3359 train_acc=0.8797 train_f1=0.3309 val_loss=0.3144 val_acc=0.8804 val_f1=0.3313 \n",
            "Epoch 40/201 | train_loss=0.2757 train_acc=0.9204 train_f1=0.3810 val_loss=0.2588 val_acc=0.9214 val_f1=0.3840 \n",
            "Epoch 60/201 | train_loss=0.2363 train_acc=0.9213 train_f1=0.4198 val_loss=0.2172 val_acc=0.9212 val_f1=0.4191 \n",
            "Epoch 80/201 | train_loss=0.2125 train_acc=0.9399 train_f1=0.4610 val_loss=0.1933 val_acc=0.9399 val_f1=0.4597 \n",
            "Epoch 100/201 | train_loss=0.1922 train_acc=0.9456 train_f1=0.4820 val_loss=0.1796 val_acc=0.9453 val_f1=0.4789 \n",
            "Epoch 120/201 | train_loss=0.1818 train_acc=0.9470 train_f1=0.4798 val_loss=0.1671 val_acc=0.9471 val_f1=0.4786 \n",
            "Early stopping at epoch 127\n",
            "Epoch 16/166 | train_loss=0.7385 train_acc=0.0676 train_f1=0.0532 val_loss=0.7264 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 21\n",
            "Epoch 19/190 | train_loss=0.3927 train_acc=0.8406 train_f1=0.3116 val_loss=0.3731 val_acc=0.8403 val_f1=0.3118 \n",
            "Epoch 38/190 | train_loss=0.2932 train_acc=0.8986 train_f1=0.3416 val_loss=0.2846 val_acc=0.8985 val_f1=0.3416 \n",
            "Epoch 57/190 | train_loss=0.2553 train_acc=0.9111 train_f1=0.3491 val_loss=0.2475 val_acc=0.9112 val_f1=0.3492 \n",
            "Epoch 76/190 | train_loss=0.2254 train_acc=0.9165 train_f1=0.4264 val_loss=0.2108 val_acc=0.9159 val_f1=0.4262 \n",
            "Epoch 95/190 | train_loss=0.2054 train_acc=0.9286 train_f1=0.4500 val_loss=0.1915 val_acc=0.9285 val_f1=0.4506 \n",
            "Epoch 114/190 | train_loss=0.1935 train_acc=0.9399 train_f1=0.4735 val_loss=0.1806 val_acc=0.9392 val_f1=0.4719 \n",
            "Epoch 133/190 | train_loss=0.1796 train_acc=0.9466 train_f1=0.4833 val_loss=0.1684 val_acc=0.9464 val_f1=0.4821 \n",
            "Epoch 152/190 | train_loss=0.1625 train_acc=0.9448 train_f1=0.4867 val_loss=0.1546 val_acc=0.9443 val_f1=0.4836 \n",
            "Epoch 171/190 | train_loss=0.1539 train_acc=0.9450 train_f1=0.4898 val_loss=0.1420 val_acc=0.9444 val_f1=0.4865 \n",
            "Epoch 190/190 | train_loss=0.1451 train_acc=0.9504 train_f1=0.4965 val_loss=0.1359 val_acc=0.9497 val_f1=0.4932 \n",
            "Epoch 17/173 | train_loss=0.6293 train_acc=0.0676 train_f1=0.0532 val_loss=0.6035 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/173 | train_loss=0.4726 train_acc=0.7108 train_f1=0.2585 val_loss=0.4413 val_acc=0.7111 val_f1=0.2588 \n",
            "Epoch 51/173 | train_loss=0.3465 train_acc=0.9224 train_f1=0.3551 val_loss=0.3138 val_acc=0.9214 val_f1=0.3542 \n",
            "Epoch 68/173 | train_loss=0.2954 train_acc=0.9404 train_f1=0.3662 val_loss=0.2914 val_acc=0.9402 val_f1=0.3660 \n",
            "Epoch 85/173 | train_loss=0.2718 train_acc=0.9433 train_f1=0.3688 val_loss=0.2671 val_acc=0.9429 val_f1=0.3684 \n",
            "Epoch 102/173 | train_loss=0.2504 train_acc=0.9497 train_f1=0.4043 val_loss=0.2372 val_acc=0.9491 val_f1=0.4034 \n",
            "Epoch 119/173 | train_loss=0.2328 train_acc=0.9561 train_f1=0.4598 val_loss=0.2207 val_acc=0.9566 val_f1=0.4614 \n",
            "Epoch 136/173 | train_loss=0.2195 train_acc=0.9568 train_f1=0.4822 val_loss=0.2003 val_acc=0.9570 val_f1=0.4820 \n",
            "Epoch 153/173 | train_loss=0.2041 train_acc=0.9579 train_f1=0.4968 val_loss=0.1842 val_acc=0.9580 val_f1=0.4948 \n",
            "Epoch 170/173 | train_loss=0.1906 train_acc=0.9558 train_f1=0.5018 val_loss=0.1707 val_acc=0.9563 val_f1=0.5013 \n",
            "Epoch 19/192 | train_loss=0.5737 train_acc=0.0676 train_f1=0.0532 val_loss=0.5718 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/192 | train_loss=0.4532 train_acc=0.6011 train_f1=0.2204 val_loss=0.4394 val_acc=0.6008 val_f1=0.2203 \n",
            "Epoch 57/192 | train_loss=0.2907 train_acc=0.9182 train_f1=0.3531 val_loss=0.2795 val_acc=0.9187 val_f1=0.3534 \n",
            "Epoch 76/192 | train_loss=0.2245 train_acc=0.9337 train_f1=0.3631 val_loss=0.2143 val_acc=0.9332 val_f1=0.3627 \n",
            "Epoch 95/192 | train_loss=0.1702 train_acc=0.9507 train_f1=0.4465 val_loss=0.1661 val_acc=0.9509 val_f1=0.4470 \n",
            "Epoch 114/192 | train_loss=0.1360 train_acc=0.9575 train_f1=0.5143 val_loss=0.1362 val_acc=0.9567 val_f1=0.5109 \n",
            "Epoch 133/192 | train_loss=0.1100 train_acc=0.9622 train_f1=0.5314 val_loss=0.1186 val_acc=0.9612 val_f1=0.5272 \n",
            "Epoch 152/192 | train_loss=0.0944 train_acc=0.9651 train_f1=0.5384 val_loss=0.1065 val_acc=0.9639 val_f1=0.5330 \n",
            "Epoch 171/192 | train_loss=0.0833 train_acc=0.9724 train_f1=0.5476 val_loss=0.1069 val_acc=0.9709 val_f1=0.5406 \n",
            "Epoch 190/192 | train_loss=0.0740 train_acc=0.9728 train_f1=0.5508 val_loss=0.0996 val_acc=0.9712 val_f1=0.5426 \n",
            "Gen 7 Best: {'epochs_run': 166, 'val_loss': 0.13855786621570587, 'val_accuracy': 0.9524843044928389, 'val_f1': 0.5088854932126476, 'flops': 25073}\n",
            "Generation 8 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.003593805037533329, epochs=205, hidden=(109, 97, 8, 64, 32), dropout_rate=0.36153247397392185, patience=27)\n",
            "Epoch 20/205 | train_loss=0.6826 train_acc=0.0676 train_f1=0.0532 val_loss=0.6075 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/205 | train_loss=0.5437 train_acc=0.0676 train_f1=0.0532 val_loss=0.5137 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 60/205 | train_loss=0.4251 train_acc=0.9005 train_f1=0.3436 val_loss=0.3802 val_acc=0.9020 val_f1=0.3444 \n",
            "Epoch 80/205 | train_loss=0.3392 train_acc=0.9450 train_f1=0.3704 val_loss=0.2880 val_acc=0.9452 val_f1=0.3704 \n",
            "Epoch 100/205 | train_loss=0.2870 train_acc=0.9490 train_f1=0.3733 val_loss=0.2514 val_acc=0.9493 val_f1=0.3736 \n",
            "Epoch 120/205 | train_loss=0.2602 train_acc=0.9502 train_f1=0.3750 val_loss=0.2272 val_acc=0.9503 val_f1=0.3748 \n",
            "Epoch 140/205 | train_loss=0.2415 train_acc=0.9494 train_f1=0.3740 val_loss=0.2026 val_acc=0.9494 val_f1=0.3736 \n",
            "Epoch 160/205 | train_loss=0.2231 train_acc=0.9503 train_f1=0.3740 val_loss=0.1857 val_acc=0.9504 val_f1=0.3738 \n",
            "Epoch 180/205 | train_loss=0.2127 train_acc=0.9483 train_f1=0.3767 val_loss=0.1725 val_acc=0.9487 val_f1=0.3769 \n",
            "Epoch 200/205 | train_loss=0.1975 train_acc=0.9581 train_f1=0.4798 val_loss=0.1616 val_acc=0.9585 val_f1=0.4798 \n",
            "Evaluating member 1: Hyperparameters(lr=0.0035187107764782046, epochs=188, hidden=(64, 19, 143, 129, 128), dropout_rate=0.1897938983318748, patience=20)\n",
            "Epoch 18/188 | train_loss=0.5686 train_acc=0.0676 train_f1=0.0532 val_loss=0.5496 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/188 | train_loss=0.4547 train_acc=0.7307 train_f1=0.2661 val_loss=0.4181 val_acc=0.7299 val_f1=0.2659 \n",
            "Epoch 54/188 | train_loss=0.3180 train_acc=0.9232 train_f1=0.3558 val_loss=0.2803 val_acc=0.9239 val_f1=0.3562 \n",
            "Epoch 72/188 | train_loss=0.2691 train_acc=0.9352 train_f1=0.3641 val_loss=0.2494 val_acc=0.9351 val_f1=0.3638 \n",
            "Epoch 90/188 | train_loss=0.2453 train_acc=0.9417 train_f1=0.3683 val_loss=0.2269 val_acc=0.9417 val_f1=0.3681 \n",
            "Epoch 108/188 | train_loss=0.2209 train_acc=0.9458 train_f1=0.3881 val_loss=0.1992 val_acc=0.9461 val_f1=0.3888 \n",
            "Epoch 126/188 | train_loss=0.1995 train_acc=0.9524 train_f1=0.4815 val_loss=0.1750 val_acc=0.9522 val_f1=0.4780 \n",
            "Epoch 144/188 | train_loss=0.1850 train_acc=0.9566 train_f1=0.5075 val_loss=0.1594 val_acc=0.9569 val_f1=0.5067 \n",
            "Epoch 162/188 | train_loss=0.1740 train_acc=0.9575 train_f1=0.5140 val_loss=0.1487 val_acc=0.9578 val_f1=0.5129 \n",
            "Epoch 180/188 | train_loss=0.1622 train_acc=0.9600 train_f1=0.5200 val_loss=0.1433 val_acc=0.9603 val_f1=0.5184 \n",
            "Evaluating member 2: Hyperparameters(lr=0.0036737141836521024, epochs=183, hidden=(114, 112, 32, 117), dropout_rate=0.062412638613676595, patience=27)\n",
            "Epoch 18/183 | train_loss=0.5708 train_acc=0.0831 train_f1=0.0581 val_loss=0.5479 val_acc=0.0831 val_f1=0.0581 \n",
            "Epoch 36/183 | train_loss=0.4450 train_acc=0.5568 train_f1=0.2057 val_loss=0.4283 val_acc=0.5557 val_f1=0.2053 \n",
            "Epoch 54/183 | train_loss=0.2872 train_acc=0.9133 train_f1=0.3501 val_loss=0.2754 val_acc=0.9140 val_f1=0.3504 \n",
            "Epoch 72/183 | train_loss=0.2333 train_acc=0.9313 train_f1=0.3777 val_loss=0.2259 val_acc=0.9310 val_f1=0.3751 \n",
            "Epoch 90/183 | train_loss=0.1844 train_acc=0.9411 train_f1=0.4632 val_loss=0.1781 val_acc=0.9409 val_f1=0.4608 \n",
            "Epoch 108/183 | train_loss=0.1475 train_acc=0.9468 train_f1=0.4913 val_loss=0.1457 val_acc=0.9468 val_f1=0.4872 \n",
            "Epoch 126/183 | train_loss=0.1254 train_acc=0.9551 train_f1=0.5144 val_loss=0.1274 val_acc=0.9555 val_f1=0.5117 \n",
            "Epoch 144/183 | train_loss=0.1118 train_acc=0.9583 train_f1=0.5248 val_loss=0.1155 val_acc=0.9575 val_f1=0.5192 \n",
            "Epoch 162/183 | train_loss=0.1014 train_acc=0.9589 train_f1=0.5286 val_loss=0.1097 val_acc=0.9576 val_f1=0.5229 \n",
            "Epoch 180/183 | train_loss=0.0940 train_acc=0.9619 train_f1=0.5335 val_loss=0.1036 val_acc=0.9610 val_f1=0.5272 \n",
            "Evaluating member 3: Hyperparameters(lr=0.003953312444796359, epochs=193, hidden=(109, 112, 8, 32), dropout_rate=0.2647471931606283, patience=30)\n",
            "Epoch 19/193 | train_loss=0.6757 train_acc=0.1026 train_f1=0.0643 val_loss=0.5903 val_acc=0.1024 val_f1=0.0643 \n",
            "Epoch 38/193 | train_loss=0.5240 train_acc=0.5331 train_f1=0.1980 val_loss=0.4622 val_acc=0.5329 val_f1=0.1980 \n",
            "Epoch 57/193 | train_loss=0.4169 train_acc=0.8905 train_f1=0.3377 val_loss=0.3591 val_acc=0.8917 val_f1=0.3381 \n",
            "Epoch 76/193 | train_loss=0.3374 train_acc=0.9279 train_f1=0.3586 val_loss=0.2794 val_acc=0.9282 val_f1=0.3585 \n",
            "Epoch 95/193 | train_loss=0.2998 train_acc=0.9314 train_f1=0.3612 val_loss=0.2449 val_acc=0.9315 val_f1=0.3610 \n",
            "Epoch 114/193 | train_loss=0.2782 train_acc=0.9382 train_f1=0.3662 val_loss=0.2248 val_acc=0.9378 val_f1=0.3657 \n",
            "Epoch 133/193 | train_loss=0.2534 train_acc=0.9424 train_f1=0.3692 val_loss=0.2076 val_acc=0.9422 val_f1=0.3689 \n",
            "Epoch 152/193 | train_loss=0.2352 train_acc=0.9454 train_f1=0.3719 val_loss=0.1919 val_acc=0.9452 val_f1=0.3715 \n",
            "Epoch 171/193 | train_loss=0.2179 train_acc=0.9461 train_f1=0.3727 val_loss=0.1795 val_acc=0.9456 val_f1=0.3721 \n",
            "Epoch 190/193 | train_loss=0.2028 train_acc=0.9464 train_f1=0.3730 val_loss=0.1670 val_acc=0.9457 val_f1=0.3722 \n",
            "Evaluating member 4: Hyperparameters(lr=0.09238126633771677, epochs=172, hidden=(8, 26), dropout_rate=0.19559181174813917, patience=30)\n",
            "Epoch 17/172 | train_loss=0.4253 train_acc=0.8648 train_f1=0.3232 val_loss=0.3676 val_acc=0.8648 val_f1=0.3229 \n",
            "Epoch 34/172 | train_loss=0.3272 train_acc=0.8982 train_f1=0.3407 val_loss=0.2893 val_acc=0.8984 val_f1=0.3404 \n",
            "Epoch 51/172 | train_loss=0.3024 train_acc=0.9112 train_f1=0.3486 val_loss=0.2660 val_acc=0.9119 val_f1=0.3486 \n",
            "Epoch 68/172 | train_loss=0.2855 train_acc=0.9127 train_f1=0.3528 val_loss=0.2539 val_acc=0.9139 val_f1=0.3523 \n",
            "Epoch 85/172 | train_loss=0.2758 train_acc=0.9230 train_f1=0.3889 val_loss=0.2411 val_acc=0.9238 val_f1=0.3894 \n",
            "Epoch 102/172 | train_loss=0.2620 train_acc=0.9272 train_f1=0.4193 val_loss=0.2302 val_acc=0.9282 val_f1=0.4174 \n",
            "Epoch 119/172 | train_loss=0.2576 train_acc=0.9255 train_f1=0.4290 val_loss=0.2209 val_acc=0.9256 val_f1=0.4247 \n",
            "Epoch 136/172 | train_loss=0.2504 train_acc=0.9279 train_f1=0.4292 val_loss=0.2207 val_acc=0.9282 val_f1=0.4264 \n",
            "Epoch 153/172 | train_loss=0.2524 train_acc=0.9249 train_f1=0.4250 val_loss=0.2199 val_acc=0.9244 val_f1=0.4203 \n",
            "Epoch 170/172 | train_loss=0.2505 train_acc=0.9256 train_f1=0.4313 val_loss=0.2164 val_acc=0.9254 val_f1=0.4292 \n",
            "Evaluating member 5: Hyperparameters(lr=0.04996661044754136, epochs=201, hidden=(26, 55, 245), dropout_rate=0.08640386389483254, patience=6)\n",
            "Epoch 20/201 | train_loss=0.3361 train_acc=0.8790 train_f1=0.3307 val_loss=0.3137 val_acc=0.8793 val_f1=0.3309 \n",
            "Epoch 40/201 | train_loss=0.2574 train_acc=0.9057 train_f1=0.4074 val_loss=0.2392 val_acc=0.9068 val_f1=0.4079 \n",
            "Epoch 60/201 | train_loss=0.2180 train_acc=0.9238 train_f1=0.4492 val_loss=0.2023 val_acc=0.9242 val_f1=0.4474 \n",
            "Epoch 80/201 | train_loss=0.1934 train_acc=0.9381 train_f1=0.4791 val_loss=0.1812 val_acc=0.9386 val_f1=0.4781 \n",
            "Epoch 100/201 | train_loss=0.1799 train_acc=0.9419 train_f1=0.4883 val_loss=0.1657 val_acc=0.9418 val_f1=0.4857 \n",
            "Epoch 120/201 | train_loss=0.1678 train_acc=0.9483 train_f1=0.4939 val_loss=0.1593 val_acc=0.9491 val_f1=0.4926 \n",
            "Epoch 140/201 | train_loss=0.1574 train_acc=0.9514 train_f1=0.4976 val_loss=0.1498 val_acc=0.9517 val_f1=0.4945 \n",
            "Epoch 160/201 | train_loss=0.1486 train_acc=0.9538 train_f1=0.5056 val_loss=0.1426 val_acc=0.9537 val_f1=0.5038 \n",
            "Epoch 180/201 | train_loss=0.1424 train_acc=0.9558 train_f1=0.5133 val_loss=0.1355 val_acc=0.9559 val_f1=0.5109 \n",
            "Epoch 200/201 | train_loss=0.1357 train_acc=0.9537 train_f1=0.5166 val_loss=0.1317 val_acc=0.9531 val_f1=0.5133 \n",
            "Early stopping at epoch 200\n",
            "Evaluating member 6: Hyperparameters(lr=0.046376556358873514, epochs=166, hidden=(17, 32, 32, 64, 229), dropout_rate=0.06231807816986021, patience=5)\n",
            "Epoch 16/166 | train_loss=0.7647 train_acc=0.0676 train_f1=0.0532 val_loss=0.7454 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 16\n",
            "Evaluating member 7: Hyperparameters(lr=0.051395188881528775, epochs=190, hidden=(32, 55, 248), dropout_rate=0.04197561470133164, patience=5)\n",
            "Epoch 19/190 | train_loss=0.3535 train_acc=0.8998 train_f1=0.3414 val_loss=0.3363 val_acc=0.9003 val_f1=0.3418 \n",
            "Epoch 38/190 | train_loss=0.2681 train_acc=0.8982 train_f1=0.3424 val_loss=0.2592 val_acc=0.8985 val_f1=0.3424 \n",
            "Epoch 57/190 | train_loss=0.2266 train_acc=0.9159 train_f1=0.4051 val_loss=0.2215 val_acc=0.9160 val_f1=0.4053 \n",
            "Epoch 76/190 | train_loss=0.2003 train_acc=0.9279 train_f1=0.4453 val_loss=0.1965 val_acc=0.9282 val_f1=0.4463 \n",
            "Epoch 95/190 | train_loss=0.1810 train_acc=0.9371 train_f1=0.4703 val_loss=0.1767 val_acc=0.9376 val_f1=0.4700 \n",
            "Epoch 114/190 | train_loss=0.1657 train_acc=0.9454 train_f1=0.4867 val_loss=0.1638 val_acc=0.9444 val_f1=0.4822 \n",
            "Epoch 133/190 | train_loss=0.1568 train_acc=0.9444 train_f1=0.4881 val_loss=0.1535 val_acc=0.9435 val_f1=0.4835 \n",
            "Epoch 152/190 | train_loss=0.1458 train_acc=0.9498 train_f1=0.4967 val_loss=0.1450 val_acc=0.9494 val_f1=0.4920 \n",
            "Epoch 171/190 | train_loss=0.1367 train_acc=0.9541 train_f1=0.5034 val_loss=0.1375 val_acc=0.9538 val_f1=0.4988 \n",
            "Epoch 190/190 | train_loss=0.1373 train_acc=0.9332 train_f1=0.4859 val_loss=0.1368 val_acc=0.9324 val_f1=0.4814 \n",
            "Evaluating member 8: Hyperparameters(lr=0.0042111204481214215, epochs=173, hidden=(64, 8, 143, 129), dropout_rate=0.2555598695743311, patience=25)\n",
            "Epoch 17/173 | train_loss=0.5751 train_acc=0.0676 train_f1=0.0532 val_loss=0.5469 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/173 | train_loss=0.4212 train_acc=0.8654 train_f1=0.3234 val_loss=0.3742 val_acc=0.8653 val_f1=0.3236 \n",
            "Epoch 51/173 | train_loss=0.3281 train_acc=0.9214 train_f1=0.3546 val_loss=0.2908 val_acc=0.9212 val_f1=0.3543 \n",
            "Epoch 68/173 | train_loss=0.2900 train_acc=0.9300 train_f1=0.3602 val_loss=0.2826 val_acc=0.9300 val_f1=0.3598 \n",
            "Epoch 85/173 | train_loss=0.2666 train_acc=0.9376 train_f1=0.3653 val_loss=0.2618 val_acc=0.9373 val_f1=0.3649 \n",
            "Epoch 102/173 | train_loss=0.2479 train_acc=0.9444 train_f1=0.3709 val_loss=0.2412 val_acc=0.9443 val_f1=0.3706 \n",
            "Epoch 119/173 | train_loss=0.2292 train_acc=0.9514 train_f1=0.4273 val_loss=0.2173 val_acc=0.9512 val_f1=0.4266 \n",
            "Epoch 136/173 | train_loss=0.2142 train_acc=0.9556 train_f1=0.4774 val_loss=0.1972 val_acc=0.9557 val_f1=0.4769 \n",
            "Epoch 153/173 | train_loss=0.2035 train_acc=0.9560 train_f1=0.4954 val_loss=0.1857 val_acc=0.9557 val_f1=0.4926 \n",
            "Epoch 170/173 | train_loss=0.1900 train_acc=0.9569 train_f1=0.5003 val_loss=0.1774 val_acc=0.9565 val_f1=0.4976 \n",
            "Evaluating member 9: Hyperparameters(lr=0.0035791472480047317, epochs=192, hidden=(114, 256, 112, 32, 128), dropout_rate=0.07703182687082404, patience=30)\n",
            "Epoch 19/192 | train_loss=0.5754 train_acc=0.0676 train_f1=0.0532 val_loss=0.5744 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/192 | train_loss=0.4664 train_acc=0.4818 train_f1=0.1818 val_loss=0.4538 val_acc=0.4822 val_f1=0.1820 \n",
            "Epoch 57/192 | train_loss=0.2952 train_acc=0.9132 train_f1=0.3502 val_loss=0.2786 val_acc=0.9141 val_f1=0.3505 \n",
            "Epoch 76/192 | train_loss=0.2314 train_acc=0.9280 train_f1=0.3596 val_loss=0.2258 val_acc=0.9278 val_f1=0.3592 \n",
            "Epoch 95/192 | train_loss=0.1917 train_acc=0.9407 train_f1=0.3690 val_loss=0.1876 val_acc=0.9404 val_f1=0.3683 \n",
            "Epoch 114/192 | train_loss=0.1559 train_acc=0.9519 train_f1=0.4517 val_loss=0.1549 val_acc=0.9515 val_f1=0.4526 \n",
            "Epoch 133/192 | train_loss=0.1257 train_acc=0.9582 train_f1=0.5180 val_loss=0.1298 val_acc=0.9579 val_f1=0.5156 \n",
            "Epoch 152/192 | train_loss=0.1054 train_acc=0.9647 train_f1=0.5338 val_loss=0.1145 val_acc=0.9643 val_f1=0.5297 \n",
            "Epoch 171/192 | train_loss=0.0908 train_acc=0.9652 train_f1=0.5398 val_loss=0.1021 val_acc=0.9638 val_f1=0.5339 \n",
            "Epoch 190/192 | train_loss=0.0799 train_acc=0.9678 train_f1=0.5462 val_loss=0.1018 val_acc=0.9661 val_f1=0.5396 \n",
            "Epoch 18/186 | train_loss=0.5313 train_acc=0.1070 train_f1=0.0657 val_loss=0.5289 val_acc=0.1067 val_f1=0.0656 \n",
            "Epoch 36/186 | train_loss=0.3190 train_acc=0.9005 train_f1=0.3422 val_loss=0.3110 val_acc=0.9002 val_f1=0.3424 \n",
            "Epoch 54/186 | train_loss=0.2499 train_acc=0.9139 train_f1=0.3546 val_loss=0.2479 val_acc=0.9142 val_f1=0.3549 \n",
            "Epoch 72/186 | train_loss=0.2038 train_acc=0.9295 train_f1=0.4282 val_loss=0.2066 val_acc=0.9293 val_f1=0.4275 \n",
            "Epoch 90/186 | train_loss=0.1678 train_acc=0.9383 train_f1=0.4709 val_loss=0.1718 val_acc=0.9377 val_f1=0.4665 \n",
            "Epoch 108/186 | train_loss=0.1424 train_acc=0.9462 train_f1=0.4971 val_loss=0.1479 val_acc=0.9452 val_f1=0.4913 \n",
            "Epoch 126/186 | train_loss=0.1237 train_acc=0.9533 train_f1=0.5107 val_loss=0.1323 val_acc=0.9530 val_f1=0.5077 \n",
            "Epoch 144/186 | train_loss=0.1109 train_acc=0.9573 train_f1=0.5199 val_loss=0.1215 val_acc=0.9566 val_f1=0.5156 \n",
            "Epoch 162/186 | train_loss=0.1013 train_acc=0.9604 train_f1=0.5273 val_loss=0.1131 val_acc=0.9596 val_f1=0.5219 \n",
            "Epoch 180/186 | train_loss=0.0939 train_acc=0.9620 train_f1=0.5330 val_loss=0.1079 val_acc=0.9608 val_f1=0.5270 \n",
            "Epoch 16/169 | train_loss=0.3888 train_acc=0.7944 train_f1=0.2915 val_loss=0.3444 val_acc=0.7947 val_f1=0.2917 \n",
            "Epoch 32/169 | train_loss=0.3168 train_acc=0.8971 train_f1=0.3402 val_loss=0.2755 val_acc=0.8970 val_f1=0.3400 \n",
            "Epoch 48/169 | train_loss=0.2848 train_acc=0.9204 train_f1=0.3754 val_loss=0.2425 val_acc=0.9206 val_f1=0.3767 \n",
            "Epoch 64/169 | train_loss=0.2678 train_acc=0.9292 train_f1=0.4017 val_loss=0.2272 val_acc=0.9293 val_f1=0.4031 \n",
            "Epoch 80/169 | train_loss=0.2588 train_acc=0.9331 train_f1=0.4256 val_loss=0.2140 val_acc=0.9337 val_f1=0.4260 \n",
            "Epoch 96/169 | train_loss=0.2501 train_acc=0.9338 train_f1=0.4262 val_loss=0.2065 val_acc=0.9343 val_f1=0.4263 \n",
            "Epoch 112/169 | train_loss=0.2416 train_acc=0.9357 train_f1=0.4289 val_loss=0.2010 val_acc=0.9359 val_f1=0.4299 \n",
            "Epoch 128/169 | train_loss=0.2394 train_acc=0.9342 train_f1=0.4382 val_loss=0.1938 val_acc=0.9346 val_f1=0.4396 \n",
            "Epoch 144/169 | train_loss=0.2315 train_acc=0.9341 train_f1=0.4548 val_loss=0.1900 val_acc=0.9340 val_f1=0.4536 \n",
            "Epoch 160/169 | train_loss=0.2295 train_acc=0.9318 train_f1=0.4575 val_loss=0.1853 val_acc=0.9316 val_f1=0.4568 \n",
            "Epoch 19/191 | train_loss=0.5807 train_acc=0.0676 train_f1=0.0532 val_loss=0.5582 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/191 | train_loss=0.4168 train_acc=0.8662 train_f1=0.3241 val_loss=0.3749 val_acc=0.8670 val_f1=0.3251 \n",
            "Epoch 57/191 | train_loss=0.3038 train_acc=0.9162 train_f1=0.3523 val_loss=0.2668 val_acc=0.9171 val_f1=0.3528 \n",
            "Epoch 76/191 | train_loss=0.2671 train_acc=0.9280 train_f1=0.3596 val_loss=0.2414 val_acc=0.9281 val_f1=0.3595 \n",
            "Epoch 95/191 | train_loss=0.2437 train_acc=0.9376 train_f1=0.3654 val_loss=0.2212 val_acc=0.9373 val_f1=0.3648 \n",
            "Epoch 114/191 | train_loss=0.2183 train_acc=0.9441 train_f1=0.4200 val_loss=0.1997 val_acc=0.9442 val_f1=0.4187 \n",
            "Epoch 133/191 | train_loss=0.2019 train_acc=0.9523 train_f1=0.4901 val_loss=0.1779 val_acc=0.9525 val_f1=0.4885 \n",
            "Epoch 152/191 | train_loss=0.1848 train_acc=0.9550 train_f1=0.5050 val_loss=0.1615 val_acc=0.9545 val_f1=0.5021 \n",
            "Epoch 171/191 | train_loss=0.1730 train_acc=0.9560 train_f1=0.5152 val_loss=0.1512 val_acc=0.9554 val_f1=0.5117 \n",
            "Epoch 190/191 | train_loss=0.1629 train_acc=0.9576 train_f1=0.5210 val_loss=0.1450 val_acc=0.9569 val_f1=0.5166 \n",
            "Epoch 19/199 | train_loss=0.6939 train_acc=0.0676 train_f1=0.0532 val_loss=0.6033 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.5635 train_acc=0.2362 train_f1=0.1060 val_loss=0.5110 val_acc=0.2352 val_f1=0.1056 \n",
            "Epoch 57/199 | train_loss=0.4604 train_acc=0.8221 train_f1=0.3040 val_loss=0.4004 val_acc=0.8224 val_f1=0.3041 \n",
            "Epoch 76/199 | train_loss=0.3736 train_acc=0.9121 train_f1=0.3494 val_loss=0.2951 val_acc=0.9128 val_f1=0.3497 \n",
            "Epoch 95/199 | train_loss=0.3244 train_acc=0.9256 train_f1=0.3575 val_loss=0.2610 val_acc=0.9257 val_f1=0.3574 \n",
            "Epoch 114/199 | train_loss=0.2859 train_acc=0.9317 train_f1=0.3621 val_loss=0.2370 val_acc=0.9320 val_f1=0.3619 \n",
            "Epoch 133/199 | train_loss=0.2604 train_acc=0.9367 train_f1=0.3656 val_loss=0.2173 val_acc=0.9365 val_f1=0.3652 \n",
            "Epoch 152/199 | train_loss=0.2424 train_acc=0.9420 train_f1=0.3695 val_loss=0.2044 val_acc=0.9420 val_f1=0.3692 \n",
            "Epoch 171/199 | train_loss=0.2279 train_acc=0.9441 train_f1=0.3710 val_loss=0.1940 val_acc=0.9438 val_f1=0.3706 \n",
            "Epoch 190/199 | train_loss=0.2227 train_acc=0.9432 train_f1=0.3702 val_loss=0.1836 val_acc=0.9428 val_f1=0.3698 \n",
            "Epoch 15/153 | train_loss=0.4393 train_acc=0.8183 train_f1=0.3006 val_loss=0.3756 val_acc=0.8171 val_f1=0.3005 \n",
            "Epoch 30/153 | train_loss=0.3179 train_acc=0.9100 train_f1=0.3480 val_loss=0.2871 val_acc=0.9103 val_f1=0.3481 \n",
            "Epoch 45/153 | train_loss=0.2793 train_acc=0.9179 train_f1=0.3537 val_loss=0.2547 val_acc=0.9181 val_f1=0.3536 \n",
            "Epoch 60/153 | train_loss=0.2537 train_acc=0.9295 train_f1=0.3800 val_loss=0.2287 val_acc=0.9289 val_f1=0.3805 \n",
            "Epoch 75/153 | train_loss=0.2349 train_acc=0.9359 train_f1=0.4261 val_loss=0.2101 val_acc=0.9356 val_f1=0.4218 \n",
            "Epoch 90/153 | train_loss=0.2204 train_acc=0.9391 train_f1=0.4466 val_loss=0.1930 val_acc=0.9401 val_f1=0.4463 \n",
            "Epoch 105/153 | train_loss=0.2089 train_acc=0.9419 train_f1=0.4648 val_loss=0.1800 val_acc=0.9427 val_f1=0.4643 \n",
            "Epoch 120/153 | train_loss=0.1942 train_acc=0.9454 train_f1=0.4791 val_loss=0.1703 val_acc=0.9460 val_f1=0.4780 \n",
            "Epoch 135/153 | train_loss=0.1857 train_acc=0.9446 train_f1=0.4894 val_loss=0.1662 val_acc=0.9442 val_f1=0.4865 \n",
            "Early stopping at epoch 137\n",
            "Epoch 19/196 | train_loss=0.5500 train_acc=0.0676 train_f1=0.0532 val_loss=0.5267 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/196 | train_loss=0.4102 train_acc=0.7726 train_f1=0.2830 val_loss=0.3837 val_acc=0.7735 val_f1=0.2834 \n",
            "Epoch 57/196 | train_loss=0.2703 train_acc=0.9358 train_f1=0.3639 val_loss=0.2549 val_acc=0.9359 val_f1=0.3638 \n",
            "Epoch 76/196 | train_loss=0.2120 train_acc=0.9385 train_f1=0.3671 val_loss=0.2054 val_acc=0.9384 val_f1=0.3670 \n",
            "Epoch 95/196 | train_loss=0.1715 train_acc=0.9528 train_f1=0.4561 val_loss=0.1653 val_acc=0.9530 val_f1=0.4562 \n",
            "Epoch 114/196 | train_loss=0.1389 train_acc=0.9557 train_f1=0.5191 val_loss=0.1362 val_acc=0.9555 val_f1=0.5144 \n",
            "Epoch 133/196 | train_loss=0.1153 train_acc=0.9626 train_f1=0.5321 val_loss=0.1178 val_acc=0.9622 val_f1=0.5275 \n",
            "Epoch 152/196 | train_loss=0.1017 train_acc=0.9635 train_f1=0.5369 val_loss=0.1087 val_acc=0.9626 val_f1=0.5309 \n",
            "Epoch 171/196 | train_loss=0.0911 train_acc=0.9677 train_f1=0.5431 val_loss=0.1042 val_acc=0.9665 val_f1=0.5371 \n",
            "Epoch 190/196 | train_loss=0.0800 train_acc=0.9700 train_f1=0.5481 val_loss=0.1008 val_acc=0.9687 val_f1=0.5412 \n",
            "Epoch 18/184 | train_loss=0.3708 train_acc=0.8547 train_f1=0.3180 val_loss=0.3466 val_acc=0.8553 val_f1=0.3183 \n",
            "Epoch 36/184 | train_loss=0.2965 train_acc=0.8899 train_f1=0.3371 val_loss=0.2772 val_acc=0.8912 val_f1=0.3377 \n",
            "Epoch 54/184 | train_loss=0.2567 train_acc=0.9188 train_f1=0.4069 val_loss=0.2405 val_acc=0.9202 val_f1=0.4062 \n",
            "Epoch 72/184 | train_loss=0.2279 train_acc=0.9219 train_f1=0.4391 val_loss=0.2124 val_acc=0.9229 val_f1=0.4409 \n",
            "Epoch 90/184 | train_loss=0.2069 train_acc=0.9352 train_f1=0.4668 val_loss=0.1918 val_acc=0.9358 val_f1=0.4649 \n",
            "Epoch 108/184 | train_loss=0.1899 train_acc=0.9407 train_f1=0.4798 val_loss=0.1794 val_acc=0.9410 val_f1=0.4781 \n",
            "Epoch 126/184 | train_loss=0.1798 train_acc=0.9473 train_f1=0.4937 val_loss=0.1736 val_acc=0.9471 val_f1=0.4907 \n",
            "Epoch 144/184 | train_loss=0.1751 train_acc=0.9467 train_f1=0.4929 val_loss=0.1660 val_acc=0.9470 val_f1=0.4916 \n",
            "Epoch 162/184 | train_loss=0.1675 train_acc=0.9450 train_f1=0.4951 val_loss=0.1564 val_acc=0.9450 val_f1=0.4931 \n",
            "Epoch 180/184 | train_loss=0.1580 train_acc=0.9471 train_f1=0.4996 val_loss=0.1500 val_acc=0.9469 val_f1=0.4966 \n",
            "Epoch 15/159 | train_loss=0.5911 train_acc=0.0676 train_f1=0.0532 val_loss=0.5523 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/159 | train_loss=0.4401 train_acc=0.8377 train_f1=0.3101 val_loss=0.3925 val_acc=0.8380 val_f1=0.3109 \n",
            "Epoch 45/159 | train_loss=0.3293 train_acc=0.9247 train_f1=0.3564 val_loss=0.2947 val_acc=0.9244 val_f1=0.3558 \n",
            "Epoch 60/159 | train_loss=0.2911 train_acc=0.9389 train_f1=0.3653 val_loss=0.2853 val_acc=0.9385 val_f1=0.3650 \n",
            "Epoch 75/159 | train_loss=0.2691 train_acc=0.9426 train_f1=0.3683 val_loss=0.2647 val_acc=0.9419 val_f1=0.3675 \n",
            "Epoch 90/159 | train_loss=0.2508 train_acc=0.9457 train_f1=0.3862 val_loss=0.2410 val_acc=0.9451 val_f1=0.3848 \n",
            "Epoch 105/159 | train_loss=0.2363 train_acc=0.9526 train_f1=0.4285 val_loss=0.2200 val_acc=0.9518 val_f1=0.4272 \n",
            "Epoch 120/159 | train_loss=0.2238 train_acc=0.9554 train_f1=0.4542 val_loss=0.2026 val_acc=0.9553 val_f1=0.4532 \n",
            "Epoch 135/159 | train_loss=0.2118 train_acc=0.9562 train_f1=0.4657 val_loss=0.1869 val_acc=0.9563 val_f1=0.4652 \n",
            "Epoch 150/159 | train_loss=0.1977 train_acc=0.9576 train_f1=0.4802 val_loss=0.1745 val_acc=0.9576 val_f1=0.4800 \n",
            "Epoch 19/199 | train_loss=0.6798 train_acc=0.0676 train_f1=0.0532 val_loss=0.6412 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.5474 train_acc=0.0676 train_f1=0.0532 val_loss=0.5317 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 57/199 | train_loss=0.4711 train_acc=0.6124 train_f1=0.2243 val_loss=0.4435 val_acc=0.6126 val_f1=0.2244 \n",
            "Epoch 76/199 | train_loss=0.3971 train_acc=0.9004 train_f1=0.3437 val_loss=0.3584 val_acc=0.9022 val_f1=0.3447 \n",
            "Epoch 95/199 | train_loss=0.3192 train_acc=0.9378 train_f1=0.3658 val_loss=0.2790 val_acc=0.9374 val_f1=0.3654 \n",
            "Epoch 114/199 | train_loss=0.2750 train_acc=0.9428 train_f1=0.3692 val_loss=0.2452 val_acc=0.9425 val_f1=0.3688 \n",
            "Epoch 133/199 | train_loss=0.2513 train_acc=0.9479 train_f1=0.3734 val_loss=0.2297 val_acc=0.9478 val_f1=0.3732 \n",
            "Epoch 152/199 | train_loss=0.2371 train_acc=0.9487 train_f1=0.3744 val_loss=0.2144 val_acc=0.9485 val_f1=0.3739 \n",
            "Epoch 171/199 | train_loss=0.2203 train_acc=0.9494 train_f1=0.3746 val_loss=0.1990 val_acc=0.9491 val_f1=0.3742 \n",
            "Epoch 190/199 | train_loss=0.2077 train_acc=0.9520 train_f1=0.3767 val_loss=0.1921 val_acc=0.9515 val_f1=0.3758 \n",
            "Epoch 19/199 | train_loss=0.5961 train_acc=0.0676 train_f1=0.0532 val_loss=0.5606 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.4458 train_acc=0.8597 train_f1=0.3208 val_loss=0.4133 val_acc=0.8591 val_f1=0.3208 \n",
            "Epoch 57/199 | train_loss=0.3221 train_acc=0.9279 train_f1=0.3589 val_loss=0.2772 val_acc=0.9278 val_f1=0.3586 \n",
            "Epoch 76/199 | train_loss=0.2794 train_acc=0.9391 train_f1=0.3668 val_loss=0.2582 val_acc=0.9389 val_f1=0.3663 \n",
            "Epoch 95/199 | train_loss=0.2556 train_acc=0.9456 train_f1=0.3713 val_loss=0.2434 val_acc=0.9457 val_f1=0.3710 \n",
            "Epoch 114/199 | train_loss=0.2371 train_acc=0.9512 train_f1=0.3752 val_loss=0.2307 val_acc=0.9508 val_f1=0.3745 \n",
            "Epoch 133/199 | train_loss=0.2213 train_acc=0.9558 train_f1=0.4164 val_loss=0.2084 val_acc=0.9557 val_f1=0.4174 \n",
            "Epoch 152/199 | train_loss=0.2050 train_acc=0.9599 train_f1=0.4941 val_loss=0.1881 val_acc=0.9600 val_f1=0.4919 \n",
            "Epoch 171/199 | train_loss=0.1934 train_acc=0.9604 train_f1=0.5096 val_loss=0.1771 val_acc=0.9609 val_f1=0.5081 \n",
            "Epoch 190/199 | train_loss=0.1854 train_acc=0.9610 train_f1=0.5152 val_loss=0.1648 val_acc=0.9612 val_f1=0.5122 \n",
            "Gen 8 Best: {'epochs_run': 205, 'val_loss': 0.16000425815582275, 'val_accuracy': 0.9582107121836374, 'val_f1': 0.4934265674561332, 'flops': 17557}\n",
            "Generation 9 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.0037121384321819276, epochs=186, hidden=(114, 112, 117), dropout_rate=0.03850868957115378, patience=28)\n",
            "Epoch 18/186 | train_loss=0.5438 train_acc=0.0893 train_f1=0.0601 val_loss=0.5428 val_acc=0.0895 val_f1=0.0602 \n",
            "Epoch 36/186 | train_loss=0.3338 train_acc=0.8708 train_f1=0.3262 val_loss=0.3211 val_acc=0.8708 val_f1=0.3261 \n",
            "Epoch 54/186 | train_loss=0.2472 train_acc=0.9104 train_f1=0.3802 val_loss=0.2435 val_acc=0.9111 val_f1=0.3826 \n",
            "Epoch 72/186 | train_loss=0.2005 train_acc=0.9282 train_f1=0.4327 val_loss=0.2010 val_acc=0.9286 val_f1=0.4351 \n",
            "Epoch 90/186 | train_loss=0.1651 train_acc=0.9418 train_f1=0.4747 val_loss=0.1694 val_acc=0.9413 val_f1=0.4730 \n",
            "Epoch 108/186 | train_loss=0.1409 train_acc=0.9460 train_f1=0.4952 val_loss=0.1476 val_acc=0.9454 val_f1=0.4908 \n",
            "Epoch 126/186 | train_loss=0.1220 train_acc=0.9525 train_f1=0.5115 val_loss=0.1315 val_acc=0.9518 val_f1=0.5059 \n",
            "Epoch 144/186 | train_loss=0.1093 train_acc=0.9556 train_f1=0.5207 val_loss=0.1198 val_acc=0.9552 val_f1=0.5165 \n",
            "Epoch 162/186 | train_loss=0.0994 train_acc=0.9602 train_f1=0.5275 val_loss=0.1130 val_acc=0.9598 val_f1=0.5218 \n",
            "Epoch 180/186 | train_loss=0.0919 train_acc=0.9588 train_f1=0.5283 val_loss=0.1074 val_acc=0.9583 val_f1=0.5238 \n",
            "Evaluating member 1: Hyperparameters(lr=0.09461247931863066, epochs=169, hidden=(13, 26), dropout_rate=0.23342018077669796, patience=30)\n",
            "Epoch 16/169 | train_loss=0.4395 train_acc=0.8389 train_f1=0.3110 val_loss=0.3776 val_acc=0.8390 val_f1=0.3109 \n",
            "Epoch 32/169 | train_loss=0.3499 train_acc=0.8866 train_f1=0.3346 val_loss=0.3006 val_acc=0.8867 val_f1=0.3342 \n",
            "Epoch 48/169 | train_loss=0.3139 train_acc=0.9213 train_f1=0.3540 val_loss=0.2674 val_acc=0.9219 val_f1=0.3538 \n",
            "Epoch 64/169 | train_loss=0.2895 train_acc=0.9294 train_f1=0.3588 val_loss=0.2486 val_acc=0.9291 val_f1=0.3582 \n",
            "Epoch 80/169 | train_loss=0.2750 train_acc=0.9295 train_f1=0.3591 val_loss=0.2315 val_acc=0.9298 val_f1=0.3588 \n",
            "Epoch 96/169 | train_loss=0.2626 train_acc=0.9329 train_f1=0.3666 val_loss=0.2177 val_acc=0.9331 val_f1=0.3671 \n",
            "Epoch 112/169 | train_loss=0.2580 train_acc=0.9349 train_f1=0.3920 val_loss=0.2100 val_acc=0.9349 val_f1=0.3920 \n",
            "Epoch 128/169 | train_loss=0.2502 train_acc=0.9372 train_f1=0.4096 val_loss=0.2046 val_acc=0.9372 val_f1=0.4096 \n",
            "Epoch 144/169 | train_loss=0.2483 train_acc=0.9426 train_f1=0.4311 val_loss=0.2026 val_acc=0.9422 val_f1=0.4293 \n",
            "Epoch 160/169 | train_loss=0.2417 train_acc=0.9407 train_f1=0.4317 val_loss=0.2001 val_acc=0.9403 val_f1=0.4295 \n",
            "Evaluating member 2: Hyperparameters(lr=0.0034489133441874048, epochs=191, hidden=(64, 19, 127, 129, 128), dropout_rate=0.22899923822263116, patience=19)\n",
            "Epoch 19/191 | train_loss=0.5902 train_acc=0.0676 train_f1=0.0532 val_loss=0.5667 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/191 | train_loss=0.4165 train_acc=0.8896 train_f1=0.3358 val_loss=0.3745 val_acc=0.8899 val_f1=0.3365 \n",
            "Epoch 57/191 | train_loss=0.2986 train_acc=0.9105 train_f1=0.3492 val_loss=0.2590 val_acc=0.9101 val_f1=0.3489 \n",
            "Epoch 76/191 | train_loss=0.2611 train_acc=0.9387 train_f1=0.3669 val_loss=0.2351 val_acc=0.9379 val_f1=0.3662 \n",
            "Epoch 95/191 | train_loss=0.2369 train_acc=0.9460 train_f1=0.3716 val_loss=0.2137 val_acc=0.9453 val_f1=0.3708 \n",
            "Epoch 114/191 | train_loss=0.2142 train_acc=0.9526 train_f1=0.4365 val_loss=0.1910 val_acc=0.9525 val_f1=0.4352 \n",
            "Epoch 133/191 | train_loss=0.1954 train_acc=0.9555 train_f1=0.4925 val_loss=0.1680 val_acc=0.9552 val_f1=0.4899 \n",
            "Epoch 152/191 | train_loss=0.1809 train_acc=0.9575 train_f1=0.5083 val_loss=0.1535 val_acc=0.9577 val_f1=0.5066 \n",
            "Epoch 171/191 | train_loss=0.1692 train_acc=0.9580 train_f1=0.5143 val_loss=0.1441 val_acc=0.9581 val_f1=0.5115 \n",
            "Epoch 190/191 | train_loss=0.1634 train_acc=0.9594 train_f1=0.5211 val_loss=0.1387 val_acc=0.9594 val_f1=0.5190 \n",
            "Evaluating member 3: Hyperparameters(lr=0.0034805998217720237, epochs=199, hidden=(109, 97, 8, 32), dropout_rate=0.33902441897623503, patience=27)\n",
            "Epoch 19/199 | train_loss=0.7187 train_acc=0.0676 train_f1=0.0532 val_loss=0.6024 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.5904 train_acc=0.0900 train_f1=0.0603 val_loss=0.5260 val_acc=0.0909 val_f1=0.0606 \n",
            "Epoch 57/199 | train_loss=0.5020 train_acc=0.5218 train_f1=0.1946 val_loss=0.4694 val_acc=0.5220 val_f1=0.1947 \n",
            "Epoch 76/199 | train_loss=0.4230 train_acc=0.8919 train_f1=0.3386 val_loss=0.3647 val_acc=0.8931 val_f1=0.3392 \n",
            "Epoch 95/199 | train_loss=0.3561 train_acc=0.9253 train_f1=0.3576 val_loss=0.2865 val_acc=0.9262 val_f1=0.3580 \n",
            "Epoch 114/199 | train_loss=0.3151 train_acc=0.9317 train_f1=0.3616 val_loss=0.2505 val_acc=0.9319 val_f1=0.3615 \n",
            "Epoch 133/199 | train_loss=0.2880 train_acc=0.9384 train_f1=0.3658 val_loss=0.2284 val_acc=0.9385 val_f1=0.3656 \n",
            "Epoch 152/199 | train_loss=0.2690 train_acc=0.9429 train_f1=0.3691 val_loss=0.2109 val_acc=0.9427 val_f1=0.3687 \n",
            "Epoch 171/199 | train_loss=0.2523 train_acc=0.9457 train_f1=0.3709 val_loss=0.1971 val_acc=0.9454 val_f1=0.3705 \n",
            "Epoch 190/199 | train_loss=0.2341 train_acc=0.9462 train_f1=0.3716 val_loss=0.1861 val_acc=0.9461 val_f1=0.3712 \n",
            "Evaluating member 4: Hyperparameters(lr=0.05175575278742651, epochs=153, hidden=(17, 32, 64, 229), dropout_rate=0.0956683002901218, patience=6)\n",
            "Epoch 15/153 | train_loss=0.6571 train_acc=0.6192 train_f1=0.2204 val_loss=0.5550 val_acc=0.6209 val_f1=0.2216 \n",
            "Epoch 30/153 | train_loss=0.3989 train_acc=0.8418 train_f1=0.3125 val_loss=0.3485 val_acc=0.8430 val_f1=0.3133 \n",
            "Epoch 45/153 | train_loss=0.3277 train_acc=0.9066 train_f1=0.3456 val_loss=0.2961 val_acc=0.9074 val_f1=0.3460 \n",
            "Epoch 60/153 | train_loss=0.3020 train_acc=0.9257 train_f1=0.3572 val_loss=0.2781 val_acc=0.9252 val_f1=0.3569 \n",
            "Epoch 75/153 | train_loss=0.2871 train_acc=0.9265 train_f1=0.3582 val_loss=0.2626 val_acc=0.9265 val_f1=0.3579 \n",
            "Epoch 90/153 | train_loss=0.2755 train_acc=0.9311 train_f1=0.3631 val_loss=0.2507 val_acc=0.9305 val_f1=0.3629 \n",
            "Epoch 105/153 | train_loss=0.2662 train_acc=0.9343 train_f1=0.3936 val_loss=0.2426 val_acc=0.9345 val_f1=0.3952 \n",
            "Epoch 120/153 | train_loss=0.2585 train_acc=0.9346 train_f1=0.4110 val_loss=0.2322 val_acc=0.9352 val_f1=0.4152 \n",
            "Epoch 135/153 | train_loss=0.2490 train_acc=0.9365 train_f1=0.4265 val_loss=0.2217 val_acc=0.9369 val_f1=0.4293 \n",
            "Epoch 150/153 | train_loss=0.2393 train_acc=0.9413 train_f1=0.4397 val_loss=0.2148 val_acc=0.9424 val_f1=0.4427 \n",
            "Evaluating member 5: Hyperparameters(lr=0.00403538854178842, epochs=196, hidden=(114, 256, 112, 23, 128), dropout_rate=0.11812099753435862, patience=21)\n",
            "Epoch 19/196 | train_loss=0.5709 train_acc=0.0676 train_f1=0.0532 val_loss=0.5458 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/196 | train_loss=0.3610 train_acc=0.9178 train_f1=0.3526 val_loss=0.3316 val_acc=0.9176 val_f1=0.3521 \n",
            "Epoch 57/196 | train_loss=0.2612 train_acc=0.9231 train_f1=0.3569 val_loss=0.2494 val_acc=0.9236 val_f1=0.3569 \n",
            "Epoch 76/196 | train_loss=0.2144 train_acc=0.9369 train_f1=0.3661 val_loss=0.2079 val_acc=0.9372 val_f1=0.3661 \n",
            "Epoch 95/196 | train_loss=0.1689 train_acc=0.9511 train_f1=0.4303 val_loss=0.1617 val_acc=0.9510 val_f1=0.4295 \n",
            "Epoch 114/196 | train_loss=0.1344 train_acc=0.9606 train_f1=0.5258 val_loss=0.1328 val_acc=0.9599 val_f1=0.5223 \n",
            "Epoch 133/196 | train_loss=0.1108 train_acc=0.9636 train_f1=0.5355 val_loss=0.1168 val_acc=0.9627 val_f1=0.5302 \n",
            "Epoch 152/196 | train_loss=0.0961 train_acc=0.9668 train_f1=0.5435 val_loss=0.1125 val_acc=0.9659 val_f1=0.5378 \n",
            "Epoch 171/196 | train_loss=0.0853 train_acc=0.9706 train_f1=0.5486 val_loss=0.1100 val_acc=0.9694 val_f1=0.5422 \n",
            "Epoch 190/196 | train_loss=0.0747 train_acc=0.9702 train_f1=0.5510 val_loss=0.1101 val_acc=0.9685 val_f1=0.5429 \n",
            "Early stopping at epoch 194\n",
            "Evaluating member 6: Hyperparameters(lr=0.05233225560196745, epochs=184, hidden=(32, 49, 248), dropout_rate=0.07680467436098656, patience=8)\n",
            "Epoch 18/184 | train_loss=0.4026 train_acc=0.8551 train_f1=0.3180 val_loss=0.3850 val_acc=0.8561 val_f1=0.3187 \n",
            "Epoch 36/184 | train_loss=0.3028 train_acc=0.9030 train_f1=0.3438 val_loss=0.2816 val_acc=0.9038 val_f1=0.3442 \n",
            "Epoch 54/184 | train_loss=0.2630 train_acc=0.9142 train_f1=0.3605 val_loss=0.2476 val_acc=0.9149 val_f1=0.3601 \n",
            "Epoch 72/184 | train_loss=0.2360 train_acc=0.9228 train_f1=0.4353 val_loss=0.2175 val_acc=0.9236 val_f1=0.4341 \n",
            "Epoch 90/184 | train_loss=0.2101 train_acc=0.9344 train_f1=0.4617 val_loss=0.1964 val_acc=0.9343 val_f1=0.4591 \n",
            "Epoch 108/184 | train_loss=0.1936 train_acc=0.9405 train_f1=0.4772 val_loss=0.1814 val_acc=0.9406 val_f1=0.4751 \n",
            "Epoch 126/184 | train_loss=0.1824 train_acc=0.9420 train_f1=0.4803 val_loss=0.1705 val_acc=0.9411 val_f1=0.4768 \n",
            "Epoch 144/184 | train_loss=0.1752 train_acc=0.9426 train_f1=0.4865 val_loss=0.1582 val_acc=0.9418 val_f1=0.4829 \n",
            "Epoch 162/184 | train_loss=0.1596 train_acc=0.9423 train_f1=0.4911 val_loss=0.1488 val_acc=0.9419 val_f1=0.4865 \n",
            "Epoch 180/184 | train_loss=0.1557 train_acc=0.9443 train_f1=0.5018 val_loss=0.1419 val_acc=0.9439 val_f1=0.4962 \n",
            "Evaluating member 7: Hyperparameters(lr=0.0047203730619620665, epochs=159, hidden=(54, 8, 143, 129), dropout_rate=0.25547431111005386, patience=23)\n",
            "Epoch 15/159 | train_loss=0.6074 train_acc=0.0676 train_f1=0.0532 val_loss=0.5612 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/159 | train_loss=0.4431 train_acc=0.8180 train_f1=0.3008 val_loss=0.4079 val_acc=0.8166 val_f1=0.3006 \n",
            "Epoch 45/159 | train_loss=0.3324 train_acc=0.9150 train_f1=0.3508 val_loss=0.2907 val_acc=0.9138 val_f1=0.3499 \n",
            "Epoch 60/159 | train_loss=0.2913 train_acc=0.9285 train_f1=0.3591 val_loss=0.2680 val_acc=0.9287 val_f1=0.3589 \n",
            "Epoch 75/159 | train_loss=0.2710 train_acc=0.9380 train_f1=0.3655 val_loss=0.2555 val_acc=0.9384 val_f1=0.3657 \n",
            "Epoch 90/159 | train_loss=0.2539 train_acc=0.9431 train_f1=0.3847 val_loss=0.2365 val_acc=0.9434 val_f1=0.3869 \n",
            "Epoch 105/159 | train_loss=0.2406 train_acc=0.9490 train_f1=0.4398 val_loss=0.2176 val_acc=0.9498 val_f1=0.4408 \n",
            "Epoch 120/159 | train_loss=0.2263 train_acc=0.9539 train_f1=0.4727 val_loss=0.2060 val_acc=0.9540 val_f1=0.4699 \n",
            "Epoch 135/159 | train_loss=0.2146 train_acc=0.9542 train_f1=0.4879 val_loss=0.1903 val_acc=0.9548 val_f1=0.4867 \n",
            "Epoch 150/159 | train_loss=0.2046 train_acc=0.9557 train_f1=0.4952 val_loss=0.1793 val_acc=0.9558 val_f1=0.4934 \n",
            "Evaluating member 8: Hyperparameters(lr=0.003589651575918487, epochs=199, hidden=(127, 97, 8, 64, 32), dropout_rate=0.2779720752071959, patience=27)\n",
            "Epoch 19/199 | train_loss=0.6396 train_acc=0.0676 train_f1=0.0532 val_loss=0.5932 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.5337 train_acc=0.0676 train_f1=0.0532 val_loss=0.5125 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 57/199 | train_loss=0.4278 train_acc=0.8456 train_f1=0.3152 val_loss=0.3815 val_acc=0.8465 val_f1=0.3155 \n",
            "Epoch 76/199 | train_loss=0.3522 train_acc=0.9275 train_f1=0.3591 val_loss=0.2956 val_acc=0.9282 val_f1=0.3595 \n",
            "Epoch 95/199 | train_loss=0.3217 train_acc=0.9348 train_f1=0.3634 val_loss=0.2706 val_acc=0.9350 val_f1=0.3633 \n",
            "Epoch 114/199 | train_loss=0.3075 train_acc=0.9377 train_f1=0.3653 val_loss=0.2564 val_acc=0.9377 val_f1=0.3648 \n",
            "Epoch 133/199 | train_loss=0.2958 train_acc=0.9378 train_f1=0.3651 val_loss=0.2422 val_acc=0.9374 val_f1=0.3645 \n",
            "Epoch 152/199 | train_loss=0.2835 train_acc=0.9396 train_f1=0.3667 val_loss=0.2290 val_acc=0.9390 val_f1=0.3659 \n",
            "Epoch 171/199 | train_loss=0.2710 train_acc=0.9441 train_f1=0.3699 val_loss=0.2218 val_acc=0.9441 val_f1=0.3694 \n",
            "Epoch 190/199 | train_loss=0.2667 train_acc=0.9460 train_f1=0.3717 val_loss=0.2127 val_acc=0.9460 val_f1=0.3713 \n",
            "Evaluating member 9: Hyperparameters(lr=0.0035462886119500565, epochs=199, hidden=(64, 19, 143, 129, 132), dropout_rate=0.2820349659896595, patience=20)\n",
            "Epoch 19/199 | train_loss=0.5986 train_acc=0.0676 train_f1=0.0532 val_loss=0.5692 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 38/199 | train_loss=0.4379 train_acc=0.8667 train_f1=0.3246 val_loss=0.4053 val_acc=0.8657 val_f1=0.3244 \n",
            "Epoch 57/199 | train_loss=0.3190 train_acc=0.9303 train_f1=0.3605 val_loss=0.2750 val_acc=0.9304 val_f1=0.3605 \n",
            "Epoch 76/199 | train_loss=0.2761 train_acc=0.9325 train_f1=0.3628 val_loss=0.2466 val_acc=0.9321 val_f1=0.3625 \n",
            "Epoch 95/199 | train_loss=0.2547 train_acc=0.9441 train_f1=0.3707 val_loss=0.2333 val_acc=0.9444 val_f1=0.3708 \n",
            "Epoch 114/199 | train_loss=0.2335 train_acc=0.9502 train_f1=0.3752 val_loss=0.2179 val_acc=0.9500 val_f1=0.3747 \n",
            "Epoch 133/199 | train_loss=0.2235 train_acc=0.9540 train_f1=0.4119 val_loss=0.2009 val_acc=0.9540 val_f1=0.4107 \n",
            "Epoch 152/199 | train_loss=0.2077 train_acc=0.9586 train_f1=0.4736 val_loss=0.1829 val_acc=0.9581 val_f1=0.4716 \n",
            "Epoch 171/199 | train_loss=0.1953 train_acc=0.9592 train_f1=0.5006 val_loss=0.1678 val_acc=0.9588 val_f1=0.4973 \n",
            "Epoch 190/199 | train_loss=0.1843 train_acc=0.9591 train_f1=0.5121 val_loss=0.1582 val_acc=0.9593 val_f1=0.5106 \n",
            "Epoch 15/156 | train_loss=0.7344 train_acc=0.0676 train_f1=0.0532 val_loss=0.7534 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/156 | train_loss=0.7309 train_acc=0.0676 train_f1=0.0532 val_loss=0.7334 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 45/156 | train_loss=0.7276 train_acc=0.0676 train_f1=0.0532 val_loss=0.7281 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 60/156 | train_loss=0.7264 train_acc=0.0676 train_f1=0.0532 val_loss=0.7267 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 75/156 | train_loss=0.7260 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 90/156 | train_loss=0.7260 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 105/156 | train_loss=0.7261 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 120/156 | train_loss=0.7260 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 135/156 | train_loss=0.7260 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 150/156 | train_loss=0.7260 train_acc=0.0676 train_f1=0.0532 val_loss=0.7265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 20/204 | train_loss=0.3863 train_acc=0.8635 train_f1=0.3225 val_loss=0.3428 val_acc=0.8637 val_f1=0.3223 \n",
            "Epoch 40/204 | train_loss=0.2844 train_acc=0.8945 train_f1=0.3398 val_loss=0.2598 val_acc=0.8942 val_f1=0.3397 \n",
            "Epoch 60/204 | train_loss=0.2419 train_acc=0.9147 train_f1=0.3914 val_loss=0.2217 val_acc=0.9145 val_f1=0.3886 \n",
            "Epoch 80/204 | train_loss=0.2080 train_acc=0.9347 train_f1=0.4652 val_loss=0.1904 val_acc=0.9348 val_f1=0.4643 \n",
            "Epoch 100/204 | train_loss=0.1852 train_acc=0.9413 train_f1=0.4864 val_loss=0.1682 val_acc=0.9411 val_f1=0.4825 \n",
            "Epoch 120/204 | train_loss=0.1704 train_acc=0.9422 train_f1=0.4960 val_loss=0.1556 val_acc=0.9423 val_f1=0.4925 \n",
            "Epoch 140/204 | train_loss=0.1592 train_acc=0.9450 train_f1=0.5006 val_loss=0.1463 val_acc=0.9448 val_f1=0.4974 \n",
            "Epoch 160/204 | train_loss=0.1508 train_acc=0.9426 train_f1=0.4983 val_loss=0.1397 val_acc=0.9424 val_f1=0.4954 \n",
            "Epoch 180/204 | train_loss=0.1435 train_acc=0.9504 train_f1=0.5114 val_loss=0.1356 val_acc=0.9498 val_f1=0.5069 \n",
            "Early stopping at epoch 199\n",
            "Epoch 17/176 | train_loss=0.5257 train_acc=0.3290 train_f1=0.1342 val_loss=0.5022 val_acc=0.3303 val_f1=0.1347 \n",
            "Epoch 34/176 | train_loss=0.3103 train_acc=0.8482 train_f1=0.3158 val_loss=0.3015 val_acc=0.8480 val_f1=0.3156 \n",
            "Epoch 51/176 | train_loss=0.2478 train_acc=0.9169 train_f1=0.3669 val_loss=0.2449 val_acc=0.9169 val_f1=0.3662 \n",
            "Epoch 68/176 | train_loss=0.2013 train_acc=0.9329 train_f1=0.4305 val_loss=0.1992 val_acc=0.9327 val_f1=0.4281 \n",
            "Epoch 85/176 | train_loss=0.1638 train_acc=0.9453 train_f1=0.4777 val_loss=0.1636 val_acc=0.9448 val_f1=0.4747 \n",
            "Epoch 102/176 | train_loss=0.1370 train_acc=0.9517 train_f1=0.5008 val_loss=0.1394 val_acc=0.9508 val_f1=0.4964 \n",
            "Epoch 119/176 | train_loss=0.1200 train_acc=0.9558 train_f1=0.5134 val_loss=0.1259 val_acc=0.9550 val_f1=0.5082 \n",
            "Epoch 136/176 | train_loss=0.1087 train_acc=0.9590 train_f1=0.5222 val_loss=0.1171 val_acc=0.9585 val_f1=0.5181 \n",
            "Epoch 153/176 | train_loss=0.0992 train_acc=0.9593 train_f1=0.5255 val_loss=0.1107 val_acc=0.9588 val_f1=0.5214 \n",
            "Epoch 170/176 | train_loss=0.0918 train_acc=0.9616 train_f1=0.5300 val_loss=0.1066 val_acc=0.9608 val_f1=0.5252 \n",
            "Epoch 16/163 | train_loss=0.7293 train_acc=0.0676 train_f1=0.0532 val_loss=0.7266 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/163 | train_loss=0.6134 train_acc=0.0676 train_f1=0.0532 val_loss=0.5881 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 48/163 | train_loss=0.3547 train_acc=0.8492 train_f1=0.3153 val_loss=0.3506 val_acc=0.8506 val_f1=0.3160 \n",
            "Epoch 64/163 | train_loss=0.3047 train_acc=0.9024 train_f1=0.3430 val_loss=0.2994 val_acc=0.9027 val_f1=0.3430 \n",
            "Epoch 80/163 | train_loss=0.2626 train_acc=0.9067 train_f1=0.3466 val_loss=0.2560 val_acc=0.9070 val_f1=0.3467 \n",
            "Epoch 96/163 | train_loss=0.2268 train_acc=0.9074 train_f1=0.4257 val_loss=0.2258 val_acc=0.9080 val_f1=0.4249 \n",
            "Epoch 112/163 | train_loss=0.2126 train_acc=0.9283 train_f1=0.4346 val_loss=0.2129 val_acc=0.9281 val_f1=0.4320 \n",
            "Epoch 128/163 | train_loss=0.1952 train_acc=0.9251 train_f1=0.4624 val_loss=0.1971 val_acc=0.9259 val_f1=0.4614 \n",
            "Epoch 144/163 | train_loss=0.1810 train_acc=0.9277 train_f1=0.4758 val_loss=0.1851 val_acc=0.9273 val_f1=0.4718 \n",
            "Epoch 160/163 | train_loss=0.1741 train_acc=0.9278 train_f1=0.4578 val_loss=0.1829 val_acc=0.9278 val_f1=0.4656 \n",
            "Epoch 16/163 | train_loss=0.4002 train_acc=0.7661 train_f1=0.2792 val_loss=0.3806 val_acc=0.7670 val_f1=0.2796 \n",
            "Epoch 32/163 | train_loss=0.2758 train_acc=0.9126 train_f1=0.3501 val_loss=0.2816 val_acc=0.9124 val_f1=0.3499 \n",
            "Epoch 48/163 | train_loss=0.2215 train_acc=0.9210 train_f1=0.3897 val_loss=0.2302 val_acc=0.9217 val_f1=0.3905 \n",
            "Epoch 64/163 | train_loss=0.1872 train_acc=0.9323 train_f1=0.4473 val_loss=0.2004 val_acc=0.9315 val_f1=0.4436 \n",
            "Epoch 80/163 | train_loss=0.1616 train_acc=0.9360 train_f1=0.4668 val_loss=0.1777 val_acc=0.9363 val_f1=0.4650 \n",
            "Epoch 96/163 | train_loss=0.1425 train_acc=0.9461 train_f1=0.4871 val_loss=0.1598 val_acc=0.9460 val_f1=0.4847 \n",
            "Epoch 112/163 | train_loss=0.1396 train_acc=0.9362 train_f1=0.4786 val_loss=0.1581 val_acc=0.9364 val_f1=0.4761 \n",
            "Early stopping at epoch 112\n",
            "Epoch 20/201 | train_loss=0.6985 train_acc=0.0676 train_f1=0.0532 val_loss=0.6460 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 40/201 | train_loss=0.5436 train_acc=0.0676 train_f1=0.0532 val_loss=0.5034 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 60/201 | train_loss=0.4228 train_acc=0.9073 train_f1=0.3474 val_loss=0.3738 val_acc=0.9077 val_f1=0.3472 \n",
            "Epoch 80/201 | train_loss=0.3531 train_acc=0.9376 train_f1=0.3659 val_loss=0.2936 val_acc=0.9377 val_f1=0.3658 \n",
            "Epoch 100/201 | train_loss=0.3223 train_acc=0.9468 train_f1=0.3719 val_loss=0.2724 val_acc=0.9468 val_f1=0.3718 \n",
            "Epoch 120/201 | train_loss=0.3037 train_acc=0.9479 train_f1=0.3729 val_loss=0.2507 val_acc=0.9476 val_f1=0.3726 \n",
            "Epoch 140/201 | train_loss=0.2835 train_acc=0.9509 train_f1=0.3754 val_loss=0.2331 val_acc=0.9512 val_f1=0.3754 \n",
            "Epoch 160/201 | train_loss=0.2706 train_acc=0.9513 train_f1=0.3758 val_loss=0.2148 val_acc=0.9513 val_f1=0.3755 \n",
            "Epoch 180/201 | train_loss=0.2309 train_acc=0.9495 train_f1=0.3738 val_loss=0.1869 val_acc=0.9494 val_f1=0.3735 \n",
            "Epoch 200/201 | train_loss=0.2028 train_acc=0.9478 train_f1=0.3716 val_loss=0.1760 val_acc=0.9475 val_f1=0.3712 \n",
            "Epoch 18/183 | train_loss=0.7492 train_acc=0.1398 train_f1=0.0760 val_loss=0.6015 val_acc=0.1391 val_f1=0.0758 \n",
            "Epoch 36/183 | train_loss=0.5871 train_acc=0.4430 train_f1=0.1695 val_loss=0.5086 val_acc=0.4443 val_f1=0.1699 \n",
            "Epoch 54/183 | train_loss=0.4908 train_acc=0.7552 train_f1=0.2763 val_loss=0.4375 val_acc=0.7562 val_f1=0.2765 \n",
            "Epoch 72/183 | train_loss=0.4186 train_acc=0.8901 train_f1=0.3384 val_loss=0.3721 val_acc=0.8911 val_f1=0.3388 \n",
            "Epoch 90/183 | train_loss=0.3679 train_acc=0.9325 train_f1=0.3631 val_loss=0.3218 val_acc=0.9327 val_f1=0.3629 \n",
            "Epoch 108/183 | train_loss=0.3344 train_acc=0.9406 train_f1=0.3681 val_loss=0.2902 val_acc=0.9408 val_f1=0.3680 \n",
            "Epoch 126/183 | train_loss=0.3108 train_acc=0.9441 train_f1=0.3702 val_loss=0.2707 val_acc=0.9441 val_f1=0.3700 \n",
            "Epoch 144/183 | train_loss=0.2878 train_acc=0.9452 train_f1=0.3710 val_loss=0.2528 val_acc=0.9448 val_f1=0.3707 \n",
            "Epoch 162/183 | train_loss=0.2790 train_acc=0.9464 train_f1=0.3716 val_loss=0.2377 val_acc=0.9465 val_f1=0.3717 \n",
            "Epoch 180/183 | train_loss=0.2698 train_acc=0.9478 train_f1=0.3727 val_loss=0.2266 val_acc=0.9476 val_f1=0.3726 \n",
            "Epoch 14/145 | train_loss=0.6240 train_acc=0.0676 train_f1=0.0532 val_loss=0.5935 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/145 | train_loss=0.4643 train_acc=0.8217 train_f1=0.3026 val_loss=0.4375 val_acc=0.8212 val_f1=0.3027 \n",
            "Epoch 42/145 | train_loss=0.3435 train_acc=0.8974 train_f1=0.3405 val_loss=0.3023 val_acc=0.8965 val_f1=0.3399 \n",
            "Epoch 56/145 | train_loss=0.3043 train_acc=0.9184 train_f1=0.3524 val_loss=0.2802 val_acc=0.9195 val_f1=0.3532 \n",
            "Epoch 70/145 | train_loss=0.2824 train_acc=0.9262 train_f1=0.3581 val_loss=0.2678 val_acc=0.9264 val_f1=0.3581 \n",
            "Epoch 84/145 | train_loss=0.2679 train_acc=0.9344 train_f1=0.3633 val_loss=0.2571 val_acc=0.9340 val_f1=0.3631 \n",
            "Epoch 98/145 | train_loss=0.2549 train_acc=0.9398 train_f1=0.3667 val_loss=0.2441 val_acc=0.9395 val_f1=0.3664 \n",
            "Epoch 112/145 | train_loss=0.2426 train_acc=0.9473 train_f1=0.4149 val_loss=0.2293 val_acc=0.9473 val_f1=0.4143 \n",
            "Epoch 126/145 | train_loss=0.2291 train_acc=0.9519 train_f1=0.4707 val_loss=0.2136 val_acc=0.9515 val_f1=0.4661 \n",
            "Epoch 140/145 | train_loss=0.2225 train_acc=0.9536 train_f1=0.4854 val_loss=0.2008 val_acc=0.9533 val_f1=0.4830 \n",
            "Epoch 16/168 | train_loss=0.5447 train_acc=0.0676 train_f1=0.0532 val_loss=0.5416 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/168 | train_loss=0.3616 train_acc=0.8593 train_f1=0.3206 val_loss=0.3386 val_acc=0.8590 val_f1=0.3204 \n",
            "Epoch 48/168 | train_loss=0.2731 train_acc=0.9060 train_f1=0.3482 val_loss=0.2629 val_acc=0.9068 val_f1=0.3488 \n",
            "Epoch 64/168 | train_loss=0.2280 train_acc=0.9321 train_f1=0.4142 val_loss=0.2198 val_acc=0.9318 val_f1=0.4133 \n",
            "Epoch 80/168 | train_loss=0.1940 train_acc=0.9382 train_f1=0.4488 val_loss=0.1879 val_acc=0.9385 val_f1=0.4489 \n",
            "Epoch 96/168 | train_loss=0.1692 train_acc=0.9450 train_f1=0.4715 val_loss=0.1654 val_acc=0.9448 val_f1=0.4699 \n",
            "Epoch 112/168 | train_loss=0.1512 train_acc=0.9483 train_f1=0.4909 val_loss=0.1483 val_acc=0.9485 val_f1=0.4881 \n",
            "Epoch 128/168 | train_loss=0.1367 train_acc=0.9519 train_f1=0.5060 val_loss=0.1360 val_acc=0.9521 val_f1=0.5017 \n",
            "Epoch 144/168 | train_loss=0.1263 train_acc=0.9532 train_f1=0.5134 val_loss=0.1260 val_acc=0.9530 val_f1=0.5089 \n",
            "Epoch 160/168 | train_loss=0.1161 train_acc=0.9563 train_f1=0.5197 val_loss=0.1178 val_acc=0.9559 val_f1=0.5149 \n",
            "Epoch 17/170 | train_loss=0.5741 train_acc=0.0676 train_f1=0.0532 val_loss=0.5361 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/170 | train_loss=0.4132 train_acc=0.8952 train_f1=0.3383 val_loss=0.3639 val_acc=0.8951 val_f1=0.3382 \n",
            "Epoch 51/170 | train_loss=0.3264 train_acc=0.9238 train_f1=0.3556 val_loss=0.2903 val_acc=0.9249 val_f1=0.3563 \n",
            "Epoch 68/170 | train_loss=0.2883 train_acc=0.9331 train_f1=0.3618 val_loss=0.2783 val_acc=0.9339 val_f1=0.3622 \n",
            "Epoch 85/170 | train_loss=0.2653 train_acc=0.9420 train_f1=0.3677 val_loss=0.2558 val_acc=0.9420 val_f1=0.3676 \n",
            "Epoch 102/170 | train_loss=0.2467 train_acc=0.9462 train_f1=0.3851 val_loss=0.2323 val_acc=0.9467 val_f1=0.3872 \n",
            "Epoch 119/170 | train_loss=0.2363 train_acc=0.9521 train_f1=0.4401 val_loss=0.2126 val_acc=0.9523 val_f1=0.4387 \n",
            "Epoch 136/170 | train_loss=0.2210 train_acc=0.9552 train_f1=0.4690 val_loss=0.1949 val_acc=0.9552 val_f1=0.4679 \n",
            "Epoch 153/170 | train_loss=0.2093 train_acc=0.9569 train_f1=0.4851 val_loss=0.1843 val_acc=0.9571 val_f1=0.4836 \n",
            "Epoch 170/170 | train_loss=0.1986 train_acc=0.9578 train_f1=0.4924 val_loss=0.1741 val_acc=0.9580 val_f1=0.4894 \n",
            "Gen 9 Best: {'epochs_run': 186, 'val_loss': 0.10744628310203552, 'val_accuracy': 0.9613620757308221, 'val_f1': 0.5263112577930762, 'flops': 30105}\n",
            "Generation 10 starting initial evaluation\n",
            "Evaluating member 0: Hyperparameters(lr=0.05246990154705591, epochs=156, hidden=(27, 32, 64, 229), dropout_rate=0.015538731434747363, patience=10)\n",
            "Epoch 15/156 | train_loss=0.7279 train_acc=0.0676 train_f1=0.0532 val_loss=0.7374 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 30/156 | train_loss=0.6644 train_acc=0.2884 train_f1=0.1212 val_loss=0.6555 val_acc=0.2861 val_f1=0.1205 \n",
            "Epoch 45/156 | train_loss=0.5025 train_acc=0.6290 train_f1=0.2292 val_loss=0.4814 val_acc=0.6300 val_f1=0.2296 \n",
            "Epoch 60/156 | train_loss=0.3661 train_acc=0.8788 train_f1=0.3308 val_loss=0.3362 val_acc=0.8802 val_f1=0.3318 \n",
            "Epoch 75/156 | train_loss=0.3299 train_acc=0.9049 train_f1=0.3460 val_loss=0.3043 val_acc=0.9050 val_f1=0.3460 \n",
            "Epoch 90/156 | train_loss=0.3045 train_acc=0.9074 train_f1=0.3482 val_loss=0.2807 val_acc=0.9073 val_f1=0.3480 \n",
            "Epoch 105/156 | train_loss=0.2850 train_acc=0.9185 train_f1=0.3551 val_loss=0.2623 val_acc=0.9180 val_f1=0.3547 \n",
            "Epoch 120/156 | train_loss=0.2689 train_acc=0.9270 train_f1=0.3622 val_loss=0.2456 val_acc=0.9264 val_f1=0.3616 \n",
            "Epoch 135/156 | train_loss=0.2528 train_acc=0.9179 train_f1=0.4097 val_loss=0.2299 val_acc=0.9176 val_f1=0.4081 \n",
            "Epoch 150/156 | train_loss=0.2461 train_acc=0.8846 train_f1=0.4124 val_loss=0.2217 val_acc=0.8851 val_f1=0.4129 \n",
            "Evaluating member 1: Hyperparameters(lr=0.05441150720808875, epochs=204, hidden=(32, 49, 256), dropout_rate=0.08977444299100365, patience=11)\n",
            "Epoch 20/204 | train_loss=0.4062 train_acc=0.8857 train_f1=0.3331 val_loss=0.3438 val_acc=0.8861 val_f1=0.3336 \n",
            "Epoch 40/204 | train_loss=0.2886 train_acc=0.9068 train_f1=0.3630 val_loss=0.2621 val_acc=0.9076 val_f1=0.3634 \n",
            "Epoch 60/204 | train_loss=0.2426 train_acc=0.9241 train_f1=0.4030 val_loss=0.2209 val_acc=0.9250 val_f1=0.4053 \n",
            "Epoch 80/204 | train_loss=0.2148 train_acc=0.9369 train_f1=0.4538 val_loss=0.1933 val_acc=0.9376 val_f1=0.4570 \n",
            "Epoch 100/204 | train_loss=0.1921 train_acc=0.9429 train_f1=0.4739 val_loss=0.1689 val_acc=0.9437 val_f1=0.4734 \n",
            "Epoch 120/204 | train_loss=0.1759 train_acc=0.9450 train_f1=0.4858 val_loss=0.1547 val_acc=0.9456 val_f1=0.4839 \n",
            "Epoch 140/204 | train_loss=0.1656 train_acc=0.9484 train_f1=0.4973 val_loss=0.1488 val_acc=0.9487 val_f1=0.4954 \n",
            "Epoch 160/204 | train_loss=0.1567 train_acc=0.9499 train_f1=0.5033 val_loss=0.1425 val_acc=0.9505 val_f1=0.5018 \n",
            "Epoch 180/204 | train_loss=0.1514 train_acc=0.9542 train_f1=0.5101 val_loss=0.1385 val_acc=0.9550 val_f1=0.5092 \n",
            "Epoch 200/204 | train_loss=0.1464 train_acc=0.9532 train_f1=0.5121 val_loss=0.1320 val_acc=0.9541 val_f1=0.5104 \n",
            "Evaluating member 2: Hyperparameters(lr=0.003893303295399896, epochs=176, hidden=(114, 99, 117), dropout_rate=0.045037127200796, patience=30)\n",
            "Epoch 17/176 | train_loss=0.5041 train_acc=0.3222 train_f1=0.1323 val_loss=0.5014 val_acc=0.3231 val_f1=0.1325 \n",
            "Epoch 34/176 | train_loss=0.3139 train_acc=0.9003 train_f1=0.3420 val_loss=0.3049 val_acc=0.9005 val_f1=0.3417 \n",
            "Epoch 51/176 | train_loss=0.2467 train_acc=0.9214 train_f1=0.3924 val_loss=0.2430 val_acc=0.9219 val_f1=0.3923 \n",
            "Epoch 68/176 | train_loss=0.2035 train_acc=0.9286 train_f1=0.4381 val_loss=0.2022 val_acc=0.9287 val_f1=0.4363 \n",
            "Epoch 85/176 | train_loss=0.1724 train_acc=0.9423 train_f1=0.4682 val_loss=0.1741 val_acc=0.9420 val_f1=0.4665 \n",
            "Epoch 102/176 | train_loss=0.1478 train_acc=0.9502 train_f1=0.4882 val_loss=0.1523 val_acc=0.9491 val_f1=0.4826 \n",
            "Epoch 119/176 | train_loss=0.1292 train_acc=0.9527 train_f1=0.5060 val_loss=0.1350 val_acc=0.9520 val_f1=0.5002 \n",
            "Epoch 136/176 | train_loss=0.1157 train_acc=0.9569 train_f1=0.5175 val_loss=0.1242 val_acc=0.9565 val_f1=0.5119 \n",
            "Epoch 153/176 | train_loss=0.1076 train_acc=0.9585 train_f1=0.5226 val_loss=0.1177 val_acc=0.9584 val_f1=0.5178 \n",
            "Epoch 170/176 | train_loss=0.0990 train_acc=0.9617 train_f1=0.5281 val_loss=0.1141 val_acc=0.9608 val_f1=0.5230 \n",
            "Evaluating member 3: Hyperparameters(lr=0.04667083441389324, epochs=163, hidden=(8, 17, 32, 64, 229), dropout_rate=0.008619009906961572, patience=8)\n",
            "Epoch 16/163 | train_loss=0.5947 train_acc=0.3216 train_f1=0.1301 val_loss=0.5667 val_acc=0.3215 val_f1=0.1302 \n",
            "Epoch 32/163 | train_loss=0.3449 train_acc=0.8669 train_f1=0.3249 val_loss=0.3259 val_acc=0.8685 val_f1=0.3259 \n",
            "Epoch 48/163 | train_loss=0.2862 train_acc=0.8872 train_f1=0.3360 val_loss=0.2757 val_acc=0.8879 val_f1=0.3367 \n",
            "Epoch 64/163 | train_loss=0.2569 train_acc=0.9002 train_f1=0.3668 val_loss=0.2466 val_acc=0.9006 val_f1=0.3684 \n",
            "Epoch 80/163 | train_loss=0.2303 train_acc=0.9155 train_f1=0.4328 val_loss=0.2238 val_acc=0.9163 val_f1=0.4328 \n",
            "Epoch 96/163 | train_loss=0.2139 train_acc=0.9187 train_f1=0.4420 val_loss=0.2046 val_acc=0.9196 val_f1=0.4428 \n",
            "Epoch 112/163 | train_loss=0.2025 train_acc=0.9310 train_f1=0.4641 val_loss=0.1921 val_acc=0.9329 val_f1=0.4660 \n",
            "Epoch 128/163 | train_loss=0.1907 train_acc=0.9336 train_f1=0.4768 val_loss=0.1867 val_acc=0.9348 val_f1=0.4767 \n",
            "Epoch 144/163 | train_loss=0.1841 train_acc=0.9293 train_f1=0.4613 val_loss=0.1791 val_acc=0.9301 val_f1=0.4609 \n",
            "Epoch 160/163 | train_loss=0.1765 train_acc=0.9372 train_f1=0.4632 val_loss=0.1770 val_acc=0.9379 val_f1=0.4632 \n",
            "Evaluating member 4: Hyperparameters(lr=0.05074101504874744, epochs=163, hidden=(32, 49, 256), dropout_rate=0.0, patience=6)\n",
            "Epoch 16/163 | train_loss=0.3671 train_acc=0.8770 train_f1=0.3291 val_loss=0.3351 val_acc=0.8774 val_f1=0.3296 \n",
            "Epoch 32/163 | train_loss=0.2657 train_acc=0.8982 train_f1=0.3416 val_loss=0.2688 val_acc=0.8993 val_f1=0.3422 \n",
            "Epoch 48/163 | train_loss=0.2254 train_acc=0.9064 train_f1=0.4167 val_loss=0.2306 val_acc=0.9072 val_f1=0.4164 \n",
            "Epoch 64/163 | train_loss=0.1855 train_acc=0.9243 train_f1=0.4599 val_loss=0.1952 val_acc=0.9242 val_f1=0.4558 \n",
            "Epoch 80/163 | train_loss=0.1564 train_acc=0.9238 train_f1=0.4654 val_loss=0.1716 val_acc=0.9240 val_f1=0.4633 \n",
            "Epoch 96/163 | train_loss=0.1359 train_acc=0.9387 train_f1=0.4800 val_loss=0.1520 val_acc=0.9390 val_f1=0.4782 \n",
            "Epoch 112/163 | train_loss=0.1171 train_acc=0.9457 train_f1=0.4977 val_loss=0.1379 val_acc=0.9459 val_f1=0.4950 \n",
            "Early stopping at epoch 121\n",
            "Evaluating member 5: Hyperparameters(lr=0.0035517635835199818, epochs=201, hidden=(127, 111, 8, 64, 32), dropout_rate=0.2930988721444421, patience=26)\n",
            "Epoch 20/201 | train_loss=0.6355 train_acc=0.0698 train_f1=0.0539 val_loss=0.5836 val_acc=0.0701 val_f1=0.0540 \n",
            "Epoch 40/201 | train_loss=0.5149 train_acc=0.3766 train_f1=0.1490 val_loss=0.4766 val_acc=0.3782 val_f1=0.1496 \n",
            "Epoch 60/201 | train_loss=0.4105 train_acc=0.9071 train_f1=0.3466 val_loss=0.3633 val_acc=0.9071 val_f1=0.3462 \n",
            "Epoch 80/201 | train_loss=0.3472 train_acc=0.9443 train_f1=0.3693 val_loss=0.3026 val_acc=0.9438 val_f1=0.3683 \n",
            "Epoch 100/201 | train_loss=0.3182 train_acc=0.9510 train_f1=0.3746 val_loss=0.2791 val_acc=0.9505 val_f1=0.3736 \n",
            "Epoch 120/201 | train_loss=0.3040 train_acc=0.9529 train_f1=0.3768 val_loss=0.2575 val_acc=0.9525 val_f1=0.3762 \n",
            "Epoch 140/201 | train_loss=0.2853 train_acc=0.9539 train_f1=0.3778 val_loss=0.2408 val_acc=0.9533 val_f1=0.3770 \n",
            "Epoch 160/201 | train_loss=0.2576 train_acc=0.9555 train_f1=0.3796 val_loss=0.2170 val_acc=0.9549 val_f1=0.3785 \n",
            "Epoch 180/201 | train_loss=0.2233 train_acc=0.9552 train_f1=0.3800 val_loss=0.1958 val_acc=0.9543 val_f1=0.3787 \n",
            "Epoch 200/201 | train_loss=0.2068 train_acc=0.9550 train_f1=0.3803 val_loss=0.1826 val_acc=0.9540 val_f1=0.3788 \n",
            "Evaluating member 6: Hyperparameters(lr=0.0036298503783421428, epochs=183, hidden=(109, 97, 8, 38), dropout_rate=0.4046518654385146, patience=26)\n",
            "Epoch 18/183 | train_loss=0.6942 train_acc=0.0676 train_f1=0.0532 val_loss=0.6135 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 36/183 | train_loss=0.5703 train_acc=0.0676 train_f1=0.0532 val_loss=0.5426 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 54/183 | train_loss=0.4913 train_acc=0.6016 train_f1=0.2206 val_loss=0.4638 val_acc=0.6011 val_f1=0.2204 \n",
            "Epoch 72/183 | train_loss=0.4092 train_acc=0.9139 train_f1=0.3506 val_loss=0.3504 val_acc=0.9141 val_f1=0.3502 \n",
            "Epoch 90/183 | train_loss=0.3566 train_acc=0.9241 train_f1=0.3568 val_loss=0.2994 val_acc=0.9244 val_f1=0.3567 \n",
            "Epoch 108/183 | train_loss=0.3221 train_acc=0.9374 train_f1=0.3654 val_loss=0.2655 val_acc=0.9375 val_f1=0.3653 \n",
            "Epoch 126/183 | train_loss=0.2945 train_acc=0.9405 train_f1=0.3679 val_loss=0.2468 val_acc=0.9402 val_f1=0.3676 \n",
            "Epoch 144/183 | train_loss=0.2811 train_acc=0.9425 train_f1=0.3693 val_loss=0.2321 val_acc=0.9424 val_f1=0.3691 \n",
            "Epoch 162/183 | train_loss=0.2649 train_acc=0.9462 train_f1=0.3719 val_loss=0.2216 val_acc=0.9460 val_f1=0.3714 \n",
            "Epoch 180/183 | train_loss=0.2541 train_acc=0.9473 train_f1=0.3727 val_loss=0.2102 val_acc=0.9473 val_f1=0.3723 \n",
            "Evaluating member 7: Hyperparameters(lr=0.004938653152582292, epochs=145, hidden=(54, 8, 143, 103), dropout_rate=0.24141267541485403, patience=16)\n",
            "Epoch 14/145 | train_loss=0.5997 train_acc=0.4166 train_f1=0.1608 val_loss=0.5398 val_acc=0.4140 val_f1=0.1602 \n",
            "Epoch 28/145 | train_loss=0.4274 train_acc=0.8019 train_f1=0.2943 val_loss=0.3799 val_acc=0.8010 val_f1=0.2944 \n",
            "Epoch 42/145 | train_loss=0.3333 train_acc=0.9081 train_f1=0.3468 val_loss=0.3051 val_acc=0.9066 val_f1=0.3456 \n",
            "Epoch 56/145 | train_loss=0.2931 train_acc=0.9329 train_f1=0.3625 val_loss=0.2809 val_acc=0.9321 val_f1=0.3618 \n",
            "Epoch 70/145 | train_loss=0.2710 train_acc=0.9391 train_f1=0.3669 val_loss=0.2541 val_acc=0.9388 val_f1=0.3664 \n",
            "Epoch 84/145 | train_loss=0.2516 train_acc=0.9437 train_f1=0.3698 val_loss=0.2360 val_acc=0.9428 val_f1=0.3689 \n",
            "Epoch 98/145 | train_loss=0.2352 train_acc=0.9468 train_f1=0.4250 val_loss=0.2100 val_acc=0.9468 val_f1=0.4262 \n",
            "Epoch 112/145 | train_loss=0.2232 train_acc=0.9515 train_f1=0.4608 val_loss=0.1949 val_acc=0.9514 val_f1=0.4603 \n",
            "Epoch 126/145 | train_loss=0.2115 train_acc=0.9524 train_f1=0.4751 val_loss=0.1815 val_acc=0.9527 val_f1=0.4761 \n",
            "Epoch 140/145 | train_loss=0.2008 train_acc=0.9553 train_f1=0.4850 val_loss=0.1749 val_acc=0.9556 val_f1=0.4857 \n",
            "Evaluating member 8: Hyperparameters(lr=0.004019555375059654, epochs=168, hidden=(108, 112, 117), dropout_rate=0.08861606277746012, patience=30)\n",
            "Epoch 16/168 | train_loss=0.5225 train_acc=0.2234 train_f1=0.1020 val_loss=0.5164 val_acc=0.2242 val_f1=0.1022 \n",
            "Epoch 32/168 | train_loss=0.3346 train_acc=0.8724 train_f1=0.3269 val_loss=0.3159 val_acc=0.8713 val_f1=0.3261 \n",
            "Epoch 48/168 | train_loss=0.2606 train_acc=0.9112 train_f1=0.3739 val_loss=0.2492 val_acc=0.9127 val_f1=0.3743 \n",
            "Epoch 64/168 | train_loss=0.2141 train_acc=0.9329 train_f1=0.4289 val_loss=0.2062 val_acc=0.9331 val_f1=0.4291 \n",
            "Epoch 80/168 | train_loss=0.1820 train_acc=0.9406 train_f1=0.4643 val_loss=0.1754 val_acc=0.9404 val_f1=0.4632 \n",
            "Epoch 96/168 | train_loss=0.1564 train_acc=0.9464 train_f1=0.4889 val_loss=0.1520 val_acc=0.9458 val_f1=0.4861 \n",
            "Epoch 112/168 | train_loss=0.1367 train_acc=0.9496 train_f1=0.5048 val_loss=0.1352 val_acc=0.9489 val_f1=0.5015 \n",
            "Epoch 128/168 | train_loss=0.1253 train_acc=0.9567 train_f1=0.5165 val_loss=0.1240 val_acc=0.9566 val_f1=0.5126 \n",
            "Epoch 144/168 | train_loss=0.1117 train_acc=0.9582 train_f1=0.5241 val_loss=0.1166 val_acc=0.9577 val_f1=0.5206 \n",
            "Epoch 160/168 | train_loss=0.1033 train_acc=0.9599 train_f1=0.5294 val_loss=0.1104 val_acc=0.9594 val_f1=0.5251 \n",
            "Evaluating member 9: Hyperparameters(lr=0.0044599352290379275, epochs=170, hidden=(54, 8, 137, 129), dropout_rate=0.2832969058953688, patience=26)\n",
            "Epoch 17/170 | train_loss=0.5696 train_acc=0.0676 train_f1=0.0532 val_loss=0.5270 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/170 | train_loss=0.3864 train_acc=0.8939 train_f1=0.3384 val_loss=0.3291 val_acc=0.8940 val_f1=0.3387 \n",
            "Epoch 51/170 | train_loss=0.3150 train_acc=0.9201 train_f1=0.3539 val_loss=0.2825 val_acc=0.9206 val_f1=0.3542 \n",
            "Epoch 68/170 | train_loss=0.2810 train_acc=0.9327 train_f1=0.3626 val_loss=0.2746 val_acc=0.9322 val_f1=0.3621 \n",
            "Epoch 85/170 | train_loss=0.2653 train_acc=0.9380 train_f1=0.3661 val_loss=0.2583 val_acc=0.9376 val_f1=0.3660 \n",
            "Epoch 102/170 | train_loss=0.2514 train_acc=0.9414 train_f1=0.3687 val_loss=0.2373 val_acc=0.9409 val_f1=0.3682 \n",
            "Epoch 119/170 | train_loss=0.2337 train_acc=0.9467 train_f1=0.3908 val_loss=0.2195 val_acc=0.9461 val_f1=0.3908 \n",
            "Epoch 136/170 | train_loss=0.2185 train_acc=0.9491 train_f1=0.4484 val_loss=0.1964 val_acc=0.9489 val_f1=0.4481 \n",
            "Epoch 153/170 | train_loss=0.2068 train_acc=0.9517 train_f1=0.4833 val_loss=0.1819 val_acc=0.9513 val_f1=0.4827 \n",
            "Epoch 170/170 | train_loss=0.1995 train_acc=0.9518 train_f1=0.4981 val_loss=0.1713 val_acc=0.9513 val_f1=0.4952 \n",
            "Epoch 16/168 | train_loss=0.7620 train_acc=0.0676 train_f1=0.0532 val_loss=0.7617 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 32/168 | train_loss=0.6525 train_acc=0.0676 train_f1=0.0532 val_loss=0.6265 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 48/168 | train_loss=0.3601 train_acc=0.9031 train_f1=0.3429 val_loss=0.3438 val_acc=0.9044 val_f1=0.3436 \n",
            "Epoch 64/168 | train_loss=0.3063 train_acc=0.9056 train_f1=0.3455 val_loss=0.2896 val_acc=0.9066 val_f1=0.3458 \n",
            "Epoch 80/168 | train_loss=0.2818 train_acc=0.9233 train_f1=0.3569 val_loss=0.2684 val_acc=0.9236 val_f1=0.3566 \n",
            "Epoch 96/168 | train_loss=0.2615 train_acc=0.9177 train_f1=0.3842 val_loss=0.2532 val_acc=0.9170 val_f1=0.3826 \n",
            "Epoch 112/168 | train_loss=0.2396 train_acc=0.9186 train_f1=0.4157 val_loss=0.2391 val_acc=0.9181 val_f1=0.4152 \n",
            "Epoch 128/168 | train_loss=0.2226 train_acc=0.9255 train_f1=0.4345 val_loss=0.2246 val_acc=0.9257 val_f1=0.4343 \n",
            "Epoch 144/168 | train_loss=0.2114 train_acc=0.9269 train_f1=0.4472 val_loss=0.2150 val_acc=0.9267 val_f1=0.4436 \n",
            "Epoch 160/168 | train_loss=0.2032 train_acc=0.9274 train_f1=0.4464 val_loss=0.2097 val_acc=0.9273 val_f1=0.4435 \n",
            "Epoch 21/210 | train_loss=0.7483 train_acc=0.0676 train_f1=0.0532 val_loss=0.7283 val_acc=0.0676 val_f1=0.0532 \n",
            "Early stopping at epoch 31\n",
            "Epoch 14/140 | train_loss=0.6118 train_acc=0.0676 train_f1=0.0532 val_loss=0.5734 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 28/140 | train_loss=0.4836 train_acc=0.5462 train_f1=0.2021 val_loss=0.4549 val_acc=0.5446 val_f1=0.2015 \n",
            "Epoch 42/140 | train_loss=0.3419 train_acc=0.9029 train_f1=0.3439 val_loss=0.3069 val_acc=0.9027 val_f1=0.3435 \n",
            "Epoch 56/140 | train_loss=0.2932 train_acc=0.9170 train_f1=0.3519 val_loss=0.2642 val_acc=0.9172 val_f1=0.3521 \n",
            "Epoch 70/140 | train_loss=0.2651 train_acc=0.9252 train_f1=0.3574 val_loss=0.2376 val_acc=0.9261 val_f1=0.3578 \n",
            "Epoch 84/140 | train_loss=0.2415 train_acc=0.9347 train_f1=0.3733 val_loss=0.2128 val_acc=0.9345 val_f1=0.3727 \n",
            "Epoch 98/140 | train_loss=0.2198 train_acc=0.9413 train_f1=0.4321 val_loss=0.1931 val_acc=0.9418 val_f1=0.4334 \n",
            "Epoch 112/140 | train_loss=0.2027 train_acc=0.9476 train_f1=0.4755 val_loss=0.1758 val_acc=0.9482 val_f1=0.4752 \n",
            "Epoch 126/140 | train_loss=0.1859 train_acc=0.9503 train_f1=0.4889 val_loss=0.1618 val_acc=0.9501 val_f1=0.4856 \n",
            "Epoch 140/140 | train_loss=0.1785 train_acc=0.9503 train_f1=0.4982 val_loss=0.1516 val_acc=0.9501 val_f1=0.4959 \n",
            "Epoch 17/173 | train_loss=0.5726 train_acc=0.0676 train_f1=0.0532 val_loss=0.5385 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 34/173 | train_loss=0.4166 train_acc=0.8471 train_f1=0.3147 val_loss=0.3684 val_acc=0.8469 val_f1=0.3147 \n",
            "Epoch 51/173 | train_loss=0.3175 train_acc=0.9233 train_f1=0.3556 val_loss=0.2803 val_acc=0.9234 val_f1=0.3555 \n",
            "Epoch 68/173 | train_loss=0.2784 train_acc=0.9308 train_f1=0.3606 val_loss=0.2641 val_acc=0.9310 val_f1=0.3606 \n",
            "Epoch 85/173 | train_loss=0.2577 train_acc=0.9369 train_f1=0.3857 val_loss=0.2370 val_acc=0.9370 val_f1=0.3862 \n",
            "Epoch 102/173 | train_loss=0.2395 train_acc=0.9445 train_f1=0.4415 val_loss=0.2152 val_acc=0.9450 val_f1=0.4420 \n",
            "Epoch 119/173 | train_loss=0.2240 train_acc=0.9506 train_f1=0.4705 val_loss=0.2002 val_acc=0.9511 val_f1=0.4699 \n",
            "Epoch 136/173 | train_loss=0.2135 train_acc=0.9531 train_f1=0.4825 val_loss=0.1904 val_acc=0.9536 val_f1=0.4807 \n",
            "Epoch 153/173 | train_loss=0.2033 train_acc=0.9542 train_f1=0.4928 val_loss=0.1810 val_acc=0.9543 val_f1=0.4903 \n",
            "Epoch 170/173 | train_loss=0.1949 train_acc=0.9556 train_f1=0.4980 val_loss=0.1756 val_acc=0.9558 val_f1=0.4959 \n",
            "Epoch 18/185 | train_loss=0.4727 train_acc=0.5463 train_f1=0.2019 val_loss=0.4496 val_acc=0.5457 val_f1=0.2018 \n",
            "Epoch 36/185 | train_loss=0.3134 train_acc=0.8639 train_f1=0.3233 val_loss=0.2987 val_acc=0.8643 val_f1=0.3233 \n",
            "Epoch 54/185 | train_loss=0.2579 train_acc=0.9048 train_f1=0.3690 val_loss=0.2469 val_acc=0.9056 val_f1=0.3701 \n",
            "Epoch 72/185 | train_loss=0.2242 train_acc=0.9234 train_f1=0.4182 val_loss=0.2134 val_acc=0.9240 val_f1=0.4189 \n",
            "Epoch 90/185 | train_loss=0.1970 train_acc=0.9366 train_f1=0.4483 val_loss=0.1897 val_acc=0.9372 val_f1=0.4479 \n",
            "Epoch 108/185 | train_loss=0.1782 train_acc=0.9429 train_f1=0.4633 val_loss=0.1721 val_acc=0.9429 val_f1=0.4609 \n",
            "Epoch 126/185 | train_loss=0.1624 train_acc=0.9467 train_f1=0.4785 val_loss=0.1580 val_acc=0.9469 val_f1=0.4753 \n",
            "Epoch 144/185 | train_loss=0.1497 train_acc=0.9501 train_f1=0.4918 val_loss=0.1469 val_acc=0.9499 val_f1=0.4878 \n",
            "Epoch 162/185 | train_loss=0.1413 train_acc=0.9527 train_f1=0.5008 val_loss=0.1382 val_acc=0.9525 val_f1=0.4964 \n",
            "Epoch 180/185 | train_loss=0.1340 train_acc=0.9541 train_f1=0.5087 val_loss=0.1315 val_acc=0.9542 val_f1=0.5045 \n",
            "Epoch 16/169 | train_loss=0.5586 train_acc=0.2747 train_f1=0.1178 val_loss=0.5162 val_acc=0.2757 val_f1=0.1180 \n",
            "Epoch 32/169 | train_loss=0.4028 train_acc=0.8278 train_f1=0.3056 val_loss=0.3502 val_acc=0.8272 val_f1=0.3056 \n",
            "Epoch 48/169 | train_loss=0.3127 train_acc=0.9248 train_f1=0.3569 val_loss=0.2849 val_acc=0.9251 val_f1=0.3569 \n",
            "Epoch 64/169 | train_loss=0.2851 train_acc=0.9334 train_f1=0.3627 val_loss=0.2677 val_acc=0.9334 val_f1=0.3625 \n",
            "Epoch 80/169 | train_loss=0.2649 train_acc=0.9390 train_f1=0.3666 val_loss=0.2525 val_acc=0.9391 val_f1=0.3666 \n",
            "Epoch 96/169 | train_loss=0.2457 train_acc=0.9423 train_f1=0.3688 val_loss=0.2334 val_acc=0.9425 val_f1=0.3689 \n",
            "Epoch 112/169 | train_loss=0.2305 train_acc=0.9480 train_f1=0.4281 val_loss=0.2110 val_acc=0.9479 val_f1=0.4282 \n",
            "Epoch 128/169 | train_loss=0.2173 train_acc=0.9516 train_f1=0.4713 val_loss=0.1951 val_acc=0.9515 val_f1=0.4690 \n",
            "Epoch 144/169 | train_loss=0.2059 train_acc=0.9536 train_f1=0.4865 val_loss=0.1842 val_acc=0.9533 val_f1=0.4846 \n",
            "Epoch 160/169 | train_loss=0.1965 train_acc=0.9538 train_f1=0.4935 val_loss=0.1739 val_acc=0.9533 val_f1=0.4908 \n",
            "Epoch 16/161 | train_loss=0.3382 train_acc=0.8184 train_f1=0.3017 val_loss=0.3370 val_acc=0.8202 val_f1=0.3026 \n",
            "Epoch 32/161 | train_loss=0.2414 train_acc=0.9032 train_f1=0.4105 val_loss=0.2479 val_acc=0.9042 val_f1=0.4105 \n",
            "Epoch 48/161 | train_loss=0.1945 train_acc=0.9138 train_f1=0.4350 val_loss=0.2064 val_acc=0.9147 val_f1=0.4356 \n",
            "Epoch 64/161 | train_loss=0.1616 train_acc=0.9162 train_f1=0.4498 val_loss=0.1824 val_acc=0.9167 val_f1=0.4482 \n",
            "Epoch 80/161 | train_loss=0.1376 train_acc=0.9389 train_f1=0.4849 val_loss=0.1609 val_acc=0.9383 val_f1=0.4807 \n",
            "Early stopping at epoch 95\n",
            "Epoch 21/212 | train_loss=0.6178 train_acc=0.0676 train_f1=0.0532 val_loss=0.5707 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 42/212 | train_loss=0.5192 train_acc=0.0676 train_f1=0.0532 val_loss=0.4871 val_acc=0.0676 val_f1=0.0532 \n",
            "Epoch 63/212 | train_loss=0.4042 train_acc=0.9081 train_f1=0.3478 val_loss=0.3645 val_acc=0.9092 val_f1=0.3485 \n",
            "Epoch 84/212 | train_loss=0.3125 train_acc=0.9365 train_f1=0.3650 val_loss=0.2741 val_acc=0.9369 val_f1=0.3651 \n",
            "Epoch 105/212 | train_loss=0.2743 train_acc=0.9418 train_f1=0.3687 val_loss=0.2456 val_acc=0.9420 val_f1=0.3689 \n",
            "Epoch 126/212 | train_loss=0.2506 train_acc=0.9475 train_f1=0.3733 val_loss=0.2270 val_acc=0.9474 val_f1=0.3731 \n",
            "Epoch 147/212 | train_loss=0.2308 train_acc=0.9503 train_f1=0.3756 val_loss=0.2074 val_acc=0.9502 val_f1=0.3754 \n",
            "Epoch 168/212 | train_loss=0.2164 train_acc=0.9532 train_f1=0.3783 val_loss=0.1979 val_acc=0.9529 val_f1=0.3777 \n",
            "Epoch 189/212 | train_loss=0.2006 train_acc=0.9556 train_f1=0.3909 val_loss=0.1862 val_acc=0.9554 val_f1=0.3897 \n",
            "Epoch 210/212 | train_loss=0.1922 train_acc=0.9587 train_f1=0.4457 val_loss=0.1785 val_acc=0.9581 val_f1=0.4443 \n",
            "Epoch 17/174 | train_loss=0.3428 train_acc=0.8763 train_f1=0.3293 val_loss=0.3200 val_acc=0.8752 val_f1=0.3291 \n",
            "Epoch 34/174 | train_loss=0.2659 train_acc=0.9057 train_f1=0.3464 val_loss=0.2567 val_acc=0.9057 val_f1=0.3461 \n",
            "Epoch 51/174 | train_loss=0.2258 train_acc=0.9203 train_f1=0.4163 val_loss=0.2150 val_acc=0.9200 val_f1=0.4147 \n",
            "Epoch 68/174 | train_loss=0.1961 train_acc=0.9336 train_f1=0.4646 val_loss=0.1854 val_acc=0.9331 val_f1=0.4606 \n",
            "Epoch 85/174 | train_loss=0.1749 train_acc=0.9394 train_f1=0.4845 val_loss=0.1670 val_acc=0.9393 val_f1=0.4811 \n",
            "Epoch 102/174 | train_loss=0.1629 train_acc=0.9497 train_f1=0.4975 val_loss=0.1572 val_acc=0.9491 val_f1=0.4941 \n",
            "Epoch 119/174 | train_loss=0.1514 train_acc=0.9491 train_f1=0.4957 val_loss=0.1448 val_acc=0.9495 val_f1=0.4939 \n",
            "Epoch 136/174 | train_loss=0.1432 train_acc=0.9556 train_f1=0.5128 val_loss=0.1364 val_acc=0.9561 val_f1=0.5109 \n",
            "Epoch 153/174 | train_loss=0.1356 train_acc=0.9508 train_f1=0.5110 val_loss=0.1316 val_acc=0.9501 val_f1=0.5083 \n",
            "Epoch 170/174 | train_loss=0.1329 train_acc=0.9558 train_f1=0.5185 val_loss=0.1282 val_acc=0.9553 val_f1=0.5153 \n",
            "Epoch 16/166 | train_loss=0.5853 train_acc=0.5176 train_f1=0.1916 val_loss=0.5288 val_acc=0.5169 val_f1=0.1914 \n",
            "Epoch 32/166 | train_loss=0.4553 train_acc=0.8205 train_f1=0.3016 val_loss=0.4046 val_acc=0.8224 val_f1=0.3029 \n",
            "Epoch 48/166 | train_loss=0.3713 train_acc=0.8651 train_f1=0.3234 val_loss=0.3167 val_acc=0.8662 val_f1=0.3240 \n",
            "Epoch 64/166 | train_loss=0.3246 train_acc=0.9165 train_f1=0.3509 val_loss=0.2895 val_acc=0.9161 val_f1=0.3506 \n",
            "Epoch 80/166 | train_loss=0.2975 train_acc=0.9286 train_f1=0.3579 val_loss=0.2730 val_acc=0.9289 val_f1=0.3576 \n",
            "Epoch 96/166 | train_loss=0.2851 train_acc=0.9373 train_f1=0.3756 val_loss=0.2676 val_acc=0.9377 val_f1=0.3744 \n",
            "Epoch 112/166 | train_loss=0.2762 train_acc=0.9440 train_f1=0.4090 val_loss=0.2627 val_acc=0.9436 val_f1=0.4059 \n",
            "Epoch 128/166 | train_loss=0.2641 train_acc=0.9495 train_f1=0.4362 val_loss=0.2597 val_acc=0.9489 val_f1=0.4337 \n",
            "Epoch 144/166 | train_loss=0.2598 train_acc=0.9527 train_f1=0.4566 val_loss=0.2567 val_acc=0.9523 val_f1=0.4538 \n",
            "Epoch 160/166 | train_loss=0.2595 train_acc=0.9535 train_f1=0.4617 val_loss=0.2513 val_acc=0.9527 val_f1=0.4577 \n",
            "Gen 10 Best: {'epochs_run': 156, 'val_loss': 0.21382452547550201, 'val_accuracy': 0.8968633509907789, 'val_f1': 0.42037525596302644, 'flops': 19577}\n",
            "Hyperparameters(lr=0.04890030179245074, epochs=168, hidden=(8, 17, 32, 64, 252), dropout_rate=0.00907531786619363, patience=16)\n",
            "Hyperparameters(lr=0.05802355715495617, epochs=210, hidden=(32, 49, 256, 128), dropout_rate=0.1559651385724351, patience=9)\n",
            "Hyperparameters(lr=0.005399856147910205, epochs=140, hidden=(54, 22, 143, 103), dropout_rate=0.21892655673171862, patience=13)\n",
            "Hyperparameters(lr=0.004471308593308451, epochs=173, hidden=(51, 8, 137, 129), dropout_rate=0.2444290121352656, patience=30)\n",
            "Hyperparameters(lr=0.003954745660464097, epochs=185, hidden=(112, 117), dropout_rate=0.12734753622596945, patience=30)\n",
            "Hyperparameters(lr=0.004616030262129771, epochs=169, hidden=(54, 8, 137, 136), dropout_rate=0.2674171142606171, patience=23)\n",
            "Hyperparameters(lr=0.05065988888989215, epochs=161, hidden=(32, 8, 49, 256), dropout_rate=0.0, patience=5)\n",
            "Hyperparameters(lr=0.0034002250724818133, epochs=212, hidden=(127, 111, 8, 64, 32), dropout_rate=0.3383078693316247, patience=25)\n",
            "Hyperparameters(lr=0.04763833348409762, epochs=174, hidden=(32, 49, 238), dropout_rate=0.05888043739869199, patience=11)\n",
            "Hyperparameters(lr=0.005189798212898392, epochs=166, hidden=(8, 143, 103), dropout_rate=0.24914245097358395, patience=17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test output"
      ],
      "metadata": {
        "id": "WGchc6511C2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = prepare_data(test_dataset, device)\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    predictions = probs.argmax(axis=1)\n",
        "\n",
        "y_true = y_test.cpu().numpy()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, predictions)\n",
        "f_beta = fbeta_score(y_true, predictions, average=\"macro\", beta=2)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F-Beta (macro): {f_beta:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "wcGqY8qX1C2R"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}